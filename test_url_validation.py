#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ URL
"""

import asyncio
import aiohttp
import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from normalize_urls import get_url_status_and_final_location_async
from src.external_apis.scrapingbee_client import CustomScrapingBeeClient

async def test_url_validation():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é URL –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±–ª–æ–∫–∏—Ä—É—é—Ç –±–æ—Ç–æ–≤"""
    
    test_urls = [
        "https://www.adidas.com",
        "https://www.chewy.com", 
        "https://www.google.com",  # –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π —Ç–µ—Å—Ç
        "https://nonexistent-domain-12345.com"  # –ù–µ–∂–∏–≤–æ–π –¥–æ–º–µ–Ω
    ]
    
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ URL...")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ScrapingBee –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å API –∫–ª—é—á –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è)
    scrapingbee_client = None
    sb_api_key = os.getenv("SCRAPINGBEE_API_KEY")
    if sb_api_key:
        try:
            scrapingbee_client = CustomScrapingBeeClient(api_key=sb_api_key)
            print("‚úÖ ScrapingBee –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ScrapingBee: {e}")
    else:
        print("‚ö†Ô∏è  SCRAPINGBEE_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    
    print()
    
    async with aiohttp.ClientSession() as session:
        for url in test_urls:
            print(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º URL: {url}")
            
            try:
                is_live, final_url, error_message = await get_url_status_and_final_location_async(
                    url, 
                    session, 
                    timeout=15.0,
                    scrapingbee_client=scrapingbee_client
                )
                
                if is_live:
                    print(f"‚úÖ –ñ–ò–í–û–ô: {url}")
                    if final_url != url:
                        print(f"   ‚Ü≥ –§–∏–Ω–∞–ª—å–Ω—ã–π URL: {final_url}")
                    if error_message:
                        print(f"   ‚ÑπÔ∏è  –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: {error_message}")
                else:
                    print(f"‚ùå –ú–ï–†–¢–í–´–ô: {url}")
                    if error_message:
                        print(f"   ‚Ü≥ –ü—Ä–∏—á–∏–Ω–∞: {error_message}")
                        
            except Exception as e:
                print(f"üí• –û–®–ò–ë–ö–ê –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {url}: {e}")
            
            print("-" * 40)
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(1)
    
    if scrapingbee_client:
        await scrapingbee_client.close_async()
    
    print("\nüèÅ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("\nüìù –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print("   ‚Ä¢ Adidas –∏ Chewy –¥–æ–ª–∂–Ω—ã –ø–æ–∫–∞–∑–∞—Ç—å '–ñ–ò–í–û–ô' (–¥–∞–∂–µ –µ—Å–ª–∏ –±–ª–æ–∫–∏—Ä—É—é—Ç –±–æ—Ç–æ–≤)")
    print("   ‚Ä¢ Google –¥–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑–∞—Ç—å '–ñ–ò–í–û–ô'")
    print("   ‚Ä¢ –ù–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–æ–º–µ–Ω –¥–æ–ª–∂–µ–Ω –ø–æ–∫–∞–∑–∞—Ç—å '–ú–ï–†–¢–í–´–ô'")

if __name__ == "__main__":
    try:
        asyncio.run(test_url_validation())
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1) 