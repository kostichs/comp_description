# План изоляции алгоритма поиска веб-сайтов

## Цель
Выделить алгоритм поиска веб-сайтов компаний в отдельный автономный проект без фронтенда и интеграции с HubSpot.

## Структура нового проекта

### Основные компоненты для изоляции:
- [x] **Обработка входных файлов** - загрузка CSV/XLSX с одной колонкой компаний
- [x] **Алгоритм поиска веб-сайтов** - основная логика поиска homepage
- [x] **Конфигурация и настройки** - все необходимые конфиги
- [x] **Зависимости** - requirements.txt с минимальным набором
- [x] **Утилиты и хелперы** - вспомогательные функции

## Детальный план реализации

### 1. Создание структуры проекта
- [x] Создать папку `website-finder/`
- [x] Создать базовую структуру директорий:
  ```
  website-finder/
  ├── src/
  ├── config/
  ├── data/
  ├── output/
  ├── requirements.txt
  ├── main.py
  └── README.md
  ```

### 2. Копирование и адаптация файлов обработки данных
- [x] Скопировать `src/data_io.py` → `src/data_loader.py`
- [x] Адаптировать функции для работы только с одной колонкой
- [x] Убрать ненужные зависимости от других модулей

### 3. Изоляция алгоритма поиска веб-сайтов
- [x] Скопировать `src/finders/homepage_finder/` → `src/website_finder/`
- [x] Скопировать базовые классы из `src/finders/base_finder.py`
- [x] Адаптировать Google Search функциональность
- [x] Убрать зависимости от pipeline и других модулей

### 4. Конфигурация
- [x] Скопировать необходимые части из `src/config/`
- [x] Создать упрощенный конфиг только для поиска веб-сайтов
- [x] Адаптировать настройки логирования

### 5. Утилиты и хелперы
- [x] Скопировать `src/utils/` (частично)
- [x] Скопировать нужные функции валидации
- [x] Скопировать URL-утилиты и нормализацию доменов

### 6. Создание основного скрипта
- [x] Создать `main.py` как точку входа
- [x] Реализовать простой CLI интерфейс
- [x] Добавить обработку аргументов командной строки

### 7. Упаковка зависимостей
- [x] Создать минимальный `requirements.txt`
- [x] Убрать зависимости от FastAPI, HubSpot и других неиспользуемых библиотек

### 8. Тестирование и документация
- [x] Создать README.md с инструкциями по запуску
- [x] Протестировать на тестовых данных
- [x] Убедиться в автономности проекта

## Ключевые файлы для копирования и адаптации

### Обязательные модули:
- [ ] `src/data_io.py` → обработка входных файлов
- [ ] `src/finders/homepage_finder/` → основной алгоритм поиска
- [ ] `src/finders/base_finder.py` → базовые классы
- [ ] `src/utils/url_utils.py` → работа с URL
- [ ] `src/utils/domain_utils.py` → нормализация доменов
- [ ] `src/config/` → конфигурация и настройки

### Вспомогательные модули:
- [ ] Части `src/validation/` → валидация результатов
- [ ] Логирование и обработка ошибок
- [ ] Утилиты для работы с файлами

## Ожидаемый результат
Автономный проект, который:
- Принимает CSV/XLSX файл с одной колонкой названий компаний
- Запускается локально командой `python main.py input_file.csv`
- Ищет веб-сайты для каждой компании
- Выводит результаты в CSV файл
- Не требует веб-интерфейса или внешних интеграций

## Риски и предосторожности
- [ ] Проверить все зависимости между модулями
- [ ] Убедиться в отсутствии циклических импортов
- [ ] Тщательно протестировать изолированный код
- [ ] Сохранить оригинальную функциональность алгоритма 