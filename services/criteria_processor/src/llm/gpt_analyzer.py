import pandas as pd
import openai
from openai import OpenAI
import re
from src.utils.logging import log_info, log_debug, log_error
from src.external.serper import perform_google_search, save_serper_result, format_search_query
from src.external.scrapingbee_client import scrape_website_text, scrape_multiple_urls_with_signals
from src.external.async_scrapingbee import run_async_scrape_sync
from src.utils.signals_processor import extract_signals_keywords
from src.utils.config import ASYNC_SCRAPING_CONFIG
from typing import Tuple

class GPTAnalyzer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é GPT.
    """
    def __init__(self, session_id: str = None):
        self.client = OpenAI()
        self.session_id = session_id

    def analyze_criteria(self, context: str, criteria_df: pd.DataFrame, website: str = None) -> dict:
        """
        –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –ø–æ —Ä–∞–∑–Ω—ã–º –ø—Ä–æ–¥—É–∫—Ç–∞–º –∏ –∞—É–¥–∏—Ç–æ—Ä–∏—è–º.
        """
        self.website = website  # Store for use in _process_dynamic_criterion
        results = {}
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º
        for product, product_group in criteria_df.groupby('Product'):
            results[product] = self._evaluate_product(product, product_group, context)
        return self._format_results(results)

    def _evaluate_product(self, product_name: str, product_df: pd.DataFrame, context: str) -> list:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ –≤—Å–µ–º —Ü–µ–ª–µ–≤—ã–º –∞—É–¥–∏—Ç–æ—Ä–∏—è–º –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞."""
        log_info(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–¥—É–∫—Ç: {product_name}")
        audience_results = []
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ü–µ–ª–µ–≤—ã–º –∞—É–¥–∏—Ç–æ—Ä–∏—è–º
        for audience, audience_group in product_df.groupby('Target Audience'):
            audience_results.append(self._evaluate_audience(audience, audience_group, context, product_name))
        return audience_results

    def _evaluate_audience(self, audience_name: str, audience_df: pd.DataFrame, context: str, product_name: str) -> dict:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ –≤—Å–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –¥–ª—è –æ–¥–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏."""
        company_name_match = re.search(r"Company: (.*?)\\n", context)
        company_name = company_name_match.group(1) if company_name_match else "Unknown Company"
        
        log_info(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º {product_name} -> {audience_name}")

        # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        mandatory_criteria = audience_df[audience_df['Criteria Type'] == 'Mandatory']
        mandatory_passed = True
        for _, criterion in mandatory_criteria.iterrows():
            passed, _, _ = self._process_dynamic_criterion(company_name, criterion, context)
            if not passed:
                mandatory_passed = False
                break
        
        if not mandatory_passed:
            log_info(f"Mandatory –ù–ï –ø—Ä–æ–π–¥–µ–Ω—ã –¥–ª—è {product_name} -> {audience_name}")
            return {"Target Audience": audience_name, "Qualified": False, "Reason": "Mandatory criteria failed", "Score": 0}

        log_info(f"Mandatory –ø—Ä–æ–π–¥–µ–Ω—ã –¥–ª—è {product_name} -> {audience_name}")

        # –®–∞–≥ 2: –û—Ü–µ–Ω–∫–∞ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        qualification_criteria = audience_df[audience_df['Criteria Type'] == 'Qualification']
        if qualification_criteria.empty:
            return {"Target Audience": audience_name, "Qualified": True, "Reason": "No qualification criteria", "Score": 1}

        total_score = 0
        max_score = len(qualification_criteria)
        
        for _, criterion in qualification_criteria.iterrows():
            passed, reason, _ = self._process_dynamic_criterion(company_name, criterion, context)
            if passed:
                total_score += 1
        
        final_score = total_score / max_score if max_score > 0 else 0
        qualified = final_score > 0.5

        reason = f"{total_score}/{max_score} qualification criteria passed."
        log_info(f"{'–ö–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞' if qualified else '–ù–ï –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞'}: {product_name} -> {audience_name} (Score: {final_score:.3f})")

        return {"Target Audience": audience_name, "Qualified": qualified, "Reason": reason, "Score": final_score}

    def _process_dynamic_criterion(self, company_name: str, criterion: pd.Series, context: str) -> Tuple[bool, str, str]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∫—Ä–∏—Ç–µ—Ä–∏–π, –≤—ã–ø–æ–ª–Ω—è—è –ø–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ."""
        criterion_text = criterion['Criteria']
        full_context = context

        if pd.notna(criterion.get('Search Query')):
            search_query_template = str(criterion['Search Query'])
            
            # Handle both {website} and {company_name} placeholders
            search_query = search_query_template
            
            # First replace {website} if present and available
            if '{website}' in search_query:
                if self.website:
                    search_query = format_search_query(search_query, self.website)
                else:
                    log_debug(f"‚ö†Ô∏è Warning: Search query contains {{website}} but no website provided")
                    # Skip this criterion if website is required but not available
                    return False, "No website provided for site: search", ""
            
            # Then replace {company_name} if present
            if '{company_name}' in search_query:
                search_query = search_query.replace('{company_name}', company_name)
            
            log_debug(f"–í—ã–ø–æ–ª–Ω—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: {search_query}")
            search_response = perform_google_search(search_query, self.session_id, company_name)
            search_results = search_response.get('organic', []) if search_response else []
            
            if search_results:
                search_context = "\\n".join([f"Title: {r.get('title', '')}\\nSnippet: {r.get('snippet', '')}" for r in search_results])
                full_context += "\\n--- Dynamic Search Results ---\\n" + search_context

                if str(criterion.get('Deep Analysis')).lower() == 'true':
                    # Use async or sync scraping based on configuration
                    if ASYNC_SCRAPING_CONFIG['enable_async_scraping']:
                        log_info(f"üêù Starting async deep analysis with signals for '{search_query}'...")
                        try:
                            scraped_text = run_async_scrape_sync(
                                search_results, criterion, self.session_id, company_name, search_query, 
                                max_concurrent=ASYNC_SCRAPING_CONFIG['max_concurrent_scrapes']
                            )
                            if scraped_text:
                                full_context += "\\n--- Deep Analysis Content (Async) ---\\n" + scraped_text
                        except Exception as e:
                            log_error(f"‚ùå Async scraping failed: {e}")
                            if ASYNC_SCRAPING_CONFIG['fallback_to_sync']:
                                log_info("üîÑ Falling back to sync scraping...")
                                scraped_text = scrape_multiple_urls_with_signals(
                                    search_results, criterion, self.session_id, company_name, search_query
                                )
                                if scraped_text:
                                    full_context += "\\n--- Deep Analysis Content (Sync Fallback) ---\\n" + scraped_text
                    else:
                        log_info(f"üêù Starting sync deep analysis with signals for '{search_query}'...")
                        scraped_text = scrape_multiple_urls_with_signals(
                            search_results, criterion, self.session_id, company_name, search_query
                        )
                        if scraped_text:
                            full_context += "\\n--- Deep Analysis Content ---\\n" + scraped_text
        
        # Enhance prompt with signals context
        signals_keywords = extract_signals_keywords(criterion)
        signals_context = ""
        if signals_keywords:
            signals_context = f"\\nKey signals to look for: {', '.join(signals_keywords)}"
        
        prompt = f"""
        Context:
        {full_context}
        ---
        Criterion: {criterion_text}{signals_context}
        ---
        Based on the context, does the company meet the criterion? 
        {f"Pay special attention to mentions of: {', '.join(signals_keywords)}" if signals_keywords else ""}
        Respond with "Yes" or "No" and a brief, one-sentence explanation.
        Example: Yes, the company provides cloud services which aligns with the criterion.
        """
        
        response_text = self._get_gpt_response(prompt)
        parsed_response = GPTResponse(response_text)
        return parsed_response.is_yes(), parsed_response.get_reason(), response_text

    def _perform_deep_analysis_for_criterion(self, company_name: str, search_results: list, serper_query: str) -> str:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (—Å–∫—Ä–∞–ø–∏–Ω–≥) –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è."""
        if not search_results:
            return ""

        all_scraped_text = []
        for result in search_results[:3]: # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ 3 —Å—Å—ã–ª–∫–∞–º–∏
            link = result.get('link')
            if not link:
                continue
            
            scraped_text = scrape_website_text(link, self.session_id, company_name, serper_query)
            if scraped_text:
                all_scraped_text.append(scraped_text)
        
        return "\\n\\n".join(all_scraped_text)

    def _get_gpt_response(self, prompt: str) -> str:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ GPT –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç."""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
                max_tokens=100
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            log_error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ OpenAI API: {e}")
            return f"Error: {e}"

    def _format_results(self, results: dict) -> dict:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤—ã–≤–æ–¥–∞."""
        output = {}
        for product, audiences in results.items():
            qualified_audiences = [a for a in audiences if a['Qualified']]
            if qualified_audiences:
                output[f"Qualified_{product}"] = "Yes"
                output[f"Qualified_Audiences_{product}"] = ", ".join([a['Target Audience'] for a in qualified_audiences])
            else:
                output[f"Qualified_{product}"] = "No"
        return output


class GPTResponse:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT."""
    def __init__(self, response_text: str):
        self.text = response_text.lower()
        self.original_text = response_text

    def is_yes(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º."""
        return self.text.startswith('yes')

    def get_reason(self) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏—á–∏–Ω—É –∏–∑ –æ—Ç–≤–µ—Ç–∞."""
        match = re.search(r'yes,?(.*)', self.text, re.IGNORECASE)
        if not match:
            match = re.search(r'no,?(.*)', self.text, re.IGNORECASE)
        
        if match and match.group(1):
            return match.group(1).strip()
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        return self.original_text 