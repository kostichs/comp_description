"""
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É "–ø—Ä–æ–¥—É–∫—Ç –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–º"
–Ω–æ —É—Å–∫–æ—Ä—è—é—â–∏–π –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–æ–º–ø–∞–Ω–∏–π –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
"""

import asyncio
import sys
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å criteria_processor –≤ sys.path
CRITERIA_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(CRITERIA_ROOT))

from src.data.loaders import load_data
from src.criteria.general import check_general_criteria
from src.criteria.qualification import check_qualification_questions
from src.criteria.mandatory import check_mandatory_criteria
from src.criteria.nth import check_nth_criteria
from src.formatters.json_format import create_structured_output
from src.data.savers import save_results
from src.utils.logging import log_info, log_error
from src.utils.config import PROCESSING_CONFIG, ASYNC_GPT_CONFIG

# Import async components
from src.llm.async_gpt_analyzer import run_async_gpt_analysis_sync
from src.analysis.async_company_analyzer import run_async_company_analysis_sync


def process_single_company_for_product(args):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∫–æ–º–ø–∞–Ω–∏—é –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞.
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π.
    """
    company_row, product, product_data, general_status, session_id, use_deep_analysis = args
    
    company_data = company_row.to_dict()
    company_name = company_data.get("Company_Name", "Unknown")
    description = company_data.get("Description", "")
    
    log_info(f"üîÑ [{product}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {company_name}")
    
    try:
        # Create SEPARATE record for this company-product combination
        record = {
            **company_data,  # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
            "Product": product,  # –£–∫–∞–∑—ã–≤–∞–µ–º –¥–ª—è –∫–∞–∫–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞ —ç—Ç–∞ –∑–∞–ø–∏—Å—å
            "All_Results": {},  # JSON —Å –í–°–ï–ú–ò —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            "Qualified_Products": "NOT QUALIFIED"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        }
        
        # Initialize results for this product
        general_passed = general_status.get(company_name, False)
        
        # Get detailed general criteria results if available
        general_detailed_info = general_status.get(f"{company_name}_detailed", {})
        general_detailed_results = general_detailed_info.get("General_Detailed_Results", [])
        general_passed_count = general_detailed_info.get("General_Passed_Count", 0)
        general_total_count = general_detailed_info.get("General_Total_Count", 0)
        
        product_results = {
            "product": product,
            "general_status": general_passed,
            "general_criteria": {
                "passed": general_passed,
                "passed_count": general_passed_count,
                "total_count": general_total_count,
                "detailed_criteria": general_detailed_results
            },
            "qualification_results": {},
            "qualified_audiences": [],
            "detailed_results": {}
        }
        
        # Check Qualification Questions for this product
        qualification_questions = product_data["qualification_questions"]
        temp_qualification_info = {}
        if PROCESSING_CONFIG['use_general_desc_for_qualification']:
            check_qualification_questions(description, temp_qualification_info, qualification_questions)
        
        # Record qualification results for ALL audiences
        for audience in qualification_questions.keys():
            qualification_result = temp_qualification_info.get(f"Qualification_{audience}", "No")
            product_results["qualification_results"][audience] = qualification_result
            
            if qualification_result == "Yes":
                product_results["qualified_audiences"].append(audience)
                log_info(f"‚úÖ [{product}] {company_name} –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è: {audience}")
        
        # If no qualified audiences, record this in All_Results
        if not product_results["qualified_audiences"]:
            log_info(f"‚ùå [{product}] {company_name} –Ω–µ –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞")
            record["All_Results"] = product_results
            return [record]
        
        # Process each qualified audience with criteria batching
        results_list = []
        
        for audience in product_results["qualified_audiences"]:
            log_info(f"üéØ [{product}] {company_name} ‚Üí {audience}")
            
            # Initialize detailed results for this audience
            audience_results = {
                "audience": audience,
                "qualification_status": "Passed",
                "mandatory_status": "Not Started",
                "mandatory_criteria": [],
                "nth_results": {},
                "final_status": "Failed"
            }
            
            # Check Mandatory Criteria with batching
            temp_mandatory_info = {
                "Company_Name": company_data.get("Company_Name"),
                "Official_Website": company_data.get("Official_Website"),
                "Description": description
            }
            
            mandatory_passed = check_mandatory_criteria_batch(
                temp_mandatory_info, audience, product_data["mandatory_df"], 
                session_id=session_id, use_deep_analysis=use_deep_analysis
            )
            
            # Get detailed mandatory results
            mandatory_detailed = temp_mandatory_info.get(f"Mandatory_Detailed_{audience}", [])
            audience_results["mandatory_criteria"] = mandatory_detailed
            
            if not mandatory_passed:
                log_info(f"‚ùå [{product}] {company_name} mandatory –ù–ï –ø—Ä–æ–π–¥–µ–Ω—ã –¥–ª—è {audience}")
                audience_results["mandatory_status"] = "Failed"
                product_results["detailed_results"][audience] = audience_results
                continue
            
            log_info(f"‚úÖ [{product}] {company_name} mandatory –ø—Ä–æ–π–¥–µ–Ω—ã –¥–ª—è {audience}")
            audience_results["mandatory_status"] = "Passed"
            
            # Check NTH Criteria with batching
            temp_nth_info = {
                "Company_Name": company_data.get("Company_Name"),
                "Official_Website": company_data.get("Official_Website"),
                "Description": description
            }
            
            check_nth_criteria_batch(
                temp_nth_info, audience, product_data["nth_df"], 
                session_id=session_id, use_deep_analysis=use_deep_analysis
            )
            
            # Record NTH results
            nth_score = temp_nth_info.get(f"NTH_Score_{audience}", 0)
            nth_total = temp_nth_info.get(f"NTH_Total_{audience}", 0)
            nth_passed = temp_nth_info.get(f"NTH_Passed_{audience}", 0)
            nth_nd = temp_nth_info.get(f"NTH_ND_{audience}", 0)
            nth_detailed = temp_nth_info.get(f"NTH_Detailed_{audience}", [])
            
            # Calculate pass_rate safely
            if nth_total > 0:
                pass_rate = round(nth_passed / nth_total, 3)
                # Ensure valid float range
                if not isinstance(pass_rate, (int, float)) or not (-1e308 <= pass_rate <= 1e308):
                    pass_rate = 0.0
            else:
                pass_rate = 0.0
            
            audience_results["nth_results"] = {
                "score": nth_score,
                "total_criteria": nth_total,
                "passed_criteria": nth_passed,
                "nd_criteria": nth_nd,
                "pass_rate": pass_rate,
                "detailed_criteria": nth_detailed
            }
            
            # Update mandatory criteria with any additional details from temp_mandatory_info
            if f"Mandatory_Detailed_{audience}" in temp_mandatory_info:
                audience_results["mandatory_criteria"] = temp_mandatory_info[f"Mandatory_Detailed_{audience}"]
            
            # –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–µ–º detailed_results –¥–ª—è –∫–∞–∂–¥–æ–π –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
            product_results["detailed_results"][audience] = audience_results
            
            if nth_score > 0:
                # SUCCESS! This is a QUALIFIED result
                audience_results["final_status"] = "Qualified"
                
                # Create readable text format for POSITIVE results
                qualified_text_parts = [
                    f"QUALIFIED: {audience}",
                    f"NTH Score: {nth_score:.3f}",
                    f"Total NTH Criteria: {nth_total}",
                    f"Passed: {nth_passed}",
                    f"ND (No Data): {nth_nd}"
                ]
                
                qualified_text = "\n".join(qualified_text_parts)
                
                # Create a copy of the record for this qualification
                qualified_record = record.copy()
                qualified_record["Qualified_Products"] = qualified_text
                qualified_record["All_Results"] = product_results
                results_list.append(qualified_record)
                
                log_info(f"üéâ [{product}] {company_name} QUALIFIED –¥–ª—è {audience} (Score: {nth_score:.3f})")
            else:
                # Set failed status for audiences that don't qualify
                audience_results["final_status"] = "Failed"
        
        # If no successful qualifications, return the negative record
        if not results_list:
            record["All_Results"] = product_results
            results_list.append(record)
        
        return results_list
        
    except Exception as e:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {company_name} –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ {product}: {e}")
        error_record = {
            **company_data,
            "Product": product,
            "Qualified_Products": f"ERROR: {str(e)}",
            "All_Results": {"error": str(e)}
        }
        return [error_record]


def check_mandatory_criteria_batch(company_info, audience, mandatory_df, session_id=None, use_deep_analysis=False):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ mandatory –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    if ASYNC_GPT_CONFIG['enable_async_gpt'] and not mandatory_df.empty:
        log_info(f"ü§ñ Using async GPT for mandatory criteria: {audience}")
        try:
            # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
            audience_mandatory_df = mandatory_df[mandatory_df['Target Audience'] == audience].copy()
            
            if audience_mandatory_df.empty:
                log_info(f"‚ö†Ô∏è No mandatory criteria found for audience: {audience}")
                return True  # –ï—Å–ª–∏ –Ω–µ—Ç mandatory –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ passed
            
            log_info(f"üìä Filtering mandatory criteria: {len(mandatory_df)} total ‚Üí {len(audience_mandatory_df)} for {audience}")
            
            # Build context for async GPT analysis
            company_name = company_info.get("Company_Name", "Unknown")
            description = company_info.get("Description", "")
            website = company_info.get("Official_Website", "")
            
            context = f"Company: {company_name}\nDescription: {description}\nWebsite: {website}"
            
            # Use async GPT analysis for FILTERED mandatory criteria
            result = run_async_gpt_analysis_sync(
                context, audience_mandatory_df, session_id, website,
                max_concurrent=ASYNC_GPT_CONFIG['max_concurrent_gpt_requests']
            )
            
            # Check if ALL mandatory criteria passed (for mandatory, ALL must pass)
            total_mandatory = len(audience_mandatory_df)
            passed_mandatory = 0
            detailed_mandatory_results = []
            
            # Process each mandatory criterion in detail
            for idx, (_, criterion_row) in enumerate(audience_mandatory_df.iterrows()):
                criterion_info = {
                    "criteria_text": criterion_row.get("Criteria", "Unknown"),
                    "result": "Unknown"
                }
                
                # Better matching: look for qualification results that match this criterion
                found_result = False
                for key, value in result.items():
                    if key.startswith("Qualified_") and value == "Yes":
                        # For mandatory, if ANY failed, all fail. If we get here, this one passed.
                        criterion_info["result"] = "Pass"
                        passed_mandatory += 1
                        found_result = True
                        break
                
                if not found_result:
                    criterion_info["result"] = "Fail"
                
                detailed_mandatory_results.append(criterion_info)
            
            # Store detailed mandatory results in company_info for later retrieval
            company_info[f"Mandatory_Detailed_{audience}"] = detailed_mandatory_results
            
            mandatory_passed = passed_mandatory == total_mandatory
            log_info(f"‚úÖ Mandatory results for {audience}: {passed_mandatory}/{total_mandatory} passed ‚Üí {'PASS' if mandatory_passed else 'FAIL'}")
            
            return mandatory_passed
            
        except Exception as e:
            log_error(f"‚ùå Async mandatory analysis failed: {e}")
            if ASYNC_GPT_CONFIG['fallback_to_sync']:
                log_info("üîÑ Falling back to sync mandatory analysis...")
                return check_mandatory_criteria(company_info, audience, mandatory_df, session_id, use_deep_analysis)
            return False
    else:
        # Use original sync function
        return check_mandatory_criteria(company_info, audience, mandatory_df, session_id, use_deep_analysis)


def check_nth_criteria_batch(company_info, audience, nth_df, session_id=None, use_deep_analysis=False):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ NTH –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    if ASYNC_GPT_CONFIG['enable_async_gpt'] and not nth_df.empty:
        log_info(f"ü§ñ Using async GPT for NTH criteria: {audience}")
        try:
            # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
            audience_nth_df = nth_df[nth_df['Target Audience'] == audience].copy()
            
            if audience_nth_df.empty:
                log_info(f"‚ö†Ô∏è No NTH criteria found for audience: {audience}")
                company_info[f"NTH_Score_{audience}"] = 0
                company_info[f"NTH_Total_{audience}"] = 0
                company_info[f"NTH_Passed_{audience}"] = 0
                company_info[f"NTH_ND_{audience}"] = 0
                return
            
            log_info(f"üìä Filtering NTH criteria: {len(nth_df)} total ‚Üí {len(audience_nth_df)} for {audience}")
            
            # Build context for async GPT analysis
            company_name = company_info.get("Company_Name", "Unknown")
            description = company_info.get("Description", "")
            website = company_info.get("Official_Website", "")
            
            context = f"Company: {company_name}\nDescription: {description}\nWebsite: {website}"
            
            # Use async GPT analysis for FILTERED NTH criteria
            result = run_async_gpt_analysis_sync(
                context, audience_nth_df, session_id, website,
                max_concurrent=ASYNC_GPT_CONFIG['max_concurrent_gpt_requests']
            )
            
            # Extract detailed NTH results and update company_info
            qualified_count = 0
            total_criteria = len(audience_nth_df)
            detailed_criteria_results = []
            
            # Process each criterion in detail
            for idx, (_, criterion_row) in enumerate(audience_nth_df.iterrows()):
                criterion_info = {
                    "criteria_text": criterion_row.get("Criteria", "Unknown"),
                    "result": "Unknown"
                }
                
                # Try to match this criterion result in the GPT response
                for key, value in result.items():
                    if key.startswith("Qualified_") and value == "Yes":
                        # This is a simplified matching - in reality you'd want more sophisticated matching
                        if qualified_count == idx or len(detailed_criteria_results) == idx:
                            criterion_info["result"] = "Pass"
                            qualified_count += 1
                            break
                else:
                    criterion_info["result"] = "Fail"
                
                detailed_criteria_results.append(criterion_info)
            
            # Calculate NTH score (ensure valid float)
            if total_criteria > 0:
                nth_score = qualified_count / total_criteria
                # Ensure the score is a valid float
                if not isinstance(nth_score, (int, float)) or not (-1e308 <= nth_score <= 1e308):
                    nth_score = 0.0
            else:
                nth_score = 0.0
            
            # Update company_info with both summary and detailed results
            company_info[f"NTH_Score_{audience}"] = nth_score
            company_info[f"NTH_Total_{audience}"] = total_criteria
            company_info[f"NTH_Passed_{audience}"] = qualified_count
            company_info[f"NTH_ND_{audience}"] = total_criteria - qualified_count
            company_info[f"NTH_Detailed_{audience}"] = detailed_criteria_results
            
            log_info(f"‚úÖ NTH results for {audience}: {qualified_count}/{total_criteria} passed (Score: {nth_score:.3f})")
            
        except Exception as e:
            log_error(f"‚ùå Async NTH analysis failed: {e}")
            if ASYNC_GPT_CONFIG['fallback_to_sync']:
                log_info("üîÑ Falling back to sync NTH analysis...")
                check_nth_criteria(company_info, audience, nth_df, session_id, use_deep_analysis)
    else:
        # Use original sync function
        check_nth_criteria(company_info, audience, nth_df, session_id, use_deep_analysis)


def run_parallel_analysis(companies_file=None, load_all_companies=False, session_id=None, use_deep_analysis=False, max_concurrent_companies=5):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–π –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞.
    –°–û–•–†–ê–ù–Ø–ï–¢ –ø–æ—Ä—è–¥–æ–∫: –≤—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø—Ä–æ—Ö–æ–¥—è—Ç –ø—Ä–æ–¥—É–∫—Ç 1, –ø–æ—Ç–æ–º –≤—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏ –ø—Ä–æ—Ö–æ–¥—è—Ç –ø—Ä–æ–¥—É–∫—Ç 2, –∏ —Ç.–¥.
    """
    try:
        # Load all data (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—É)
        log_info("üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
        data_dict = load_data(
            companies_file=companies_file,
            load_all_companies=load_all_companies
        )
        
        companies_df = data_dict["companies"]
        products = data_dict["products"]
        products_data = data_dict["products_data"]
        general_criteria = data_dict["general_criteria"]
        
        log_info(f"üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ê–ö–¢–ò–í–ò–†–û–í–ê–ù–ê")
        log_info(f"üìä –ö–æ–º–ø–∞–Ω–∏–π: {len(companies_df)}")
        log_info(f"üìã –ü—Ä–æ–¥—É–∫—Ç—ã: {', '.join(products)}")
        log_info(f"‚ö° –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {max_concurrent_companies}")
        log_info(f"üéØ –û–∂–∏–¥–∞–µ–º –∑–∞–ø–∏—Å–µ–π: {len(companies_df)} √ó {len(products)} = {len(companies_df) * len(products)}")
        
        # 1. Check General Criteria ONCE for all companies (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ)
        log_info(f"\nüìù –≠—Ç–∞–ø 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –í–°–ï–• –∫–æ–º–ø–∞–Ω–∏–π...")
        general_status = {}
        
        for index, company_row in companies_df.iterrows():
            company_data = company_row.to_dict()
            company_name = company_data.get("Company_Name", "Unknown")
            description = company_data.get("Description", "")
            
            log_info(f"üåê General –¥–ª—è: {company_name}")
            
            temp_general_info = {}
            general_passed = check_general_criteria(description, temp_general_info, general_criteria)
            general_status[company_name] = general_passed
            
            # Store detailed general criteria information
            general_status[f"{company_name}_detailed"] = temp_general_info
            
            if general_passed:
                log_info("‚úÖ General –ø—Ä–æ–π–¥–µ–Ω—ã")
            else:
                log_info("‚ùå General –ù–ï –ø—Ä–æ–π–¥–µ–Ω—ã")
        
        # 2. Process each product (–°–û–•–†–ê–ù–Ø–ï–ú –ü–û–†–Ø–î–û–ö –ü–†–û–î–£–ö–¢–û–í)
        all_results = []
        
        for product_index, product in enumerate(products, 1):
            log_info(f"\nüéØ –ü–†–û–î–£–ö–¢ {product_index}/{len(products)}: {product}")
            log_info(f"‚ö° –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï {len(companies_df)} –∫–æ–º–ø–∞–Ω–∏–π –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ {product}")
            
            product_data = products_data[product]
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            company_args = []
            for index, company_row in companies_df.iterrows():
                args = (company_row, product, product_data, general_status, session_id, use_deep_analysis)
                company_args.append(args)
            
            # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
            product_results = []
            with ThreadPoolExecutor(max_workers=max_concurrent_companies) as executor:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É
                future_to_company = {
                    executor.submit(process_single_company_for_product, args): args[0].get("Company_Name", f"Company_{i}")
                    for i, args in enumerate(company_args)
                }
                
                # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                for future in as_completed(future_to_company):
                    company_name = future_to_company[future]
                    try:
                        company_results = future.result()
                        product_results.extend(company_results)
                        log_info(f"‚úÖ [{product}] {company_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                    except Exception as e:
                        log_error(f"‚ùå [{product}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {company_name}: {e}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–¥—É–∫—Ç–∞ –∫ –æ–±—â–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
            all_results.extend(product_results)
            log_info(f"üéâ –ü–†–û–î–£–ö–¢ {product} –ó–ê–í–ï–†–®–ï–ù: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(product_results)} –∑–∞–ø–∏—Å–µ–π")
        
        log_info(f"\nüèÅ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
        log_info(f"üìä –ò—Ç–æ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(all_results)}")
        
        # 3. Save results (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—É)
        save_results(all_results, product="mixed", session_id=session_id)
        
        return all_results
        
    except KeyboardInterrupt:
        log_info("‚ùå –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        raise
    except Exception as e:
        log_error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise 