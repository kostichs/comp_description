"""
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É "–ø—Ä–æ–¥—É–∫—Ç –∑–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–º"
–Ω–æ —É—Å–∫–æ—Ä—è—é—â–∏–π –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–æ–º–ø–∞–Ω–∏–π –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
"""

import asyncio
import sys
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å criteria_processor –≤ sys.path
CRITERIA_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(CRITERIA_ROOT))

from src.data.loaders import load_data
from src.criteria.general import check_general_criteria
from src.criteria.qualification import check_qualification_questions
from src.criteria.mandatory import check_mandatory_criteria
from src.criteria.nth import check_nth_criteria
from src.formatters.json_format import create_structured_output
from src.data.savers import save_results
from src.utils.logging import log_info, log_error
from src.utils.config import PROCESSING_CONFIG, ASYNC_GPT_CONFIG, CIRCUIT_BREAKER_CONFIG
from src.utils.state_manager import ProcessingStateManager

# Import async components
from src.llm.async_gpt_analyzer import run_async_gpt_analysis_sync
from src.analysis.async_company_analyzer import run_async_company_analysis_sync


def process_single_company_for_product(args):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∫–æ–º–ø–∞–Ω–∏—é –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞.
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π.
    """
    company_row, product, product_data, general_status, session_id, use_deep_analysis = args
    
    company_data = company_row.to_dict()
    company_name = company_data.get("Company_Name", "Unknown")
    description = company_data.get("Description", "")
    
    log_info(f"üîÑ [{product}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {company_name}")
    
    try:
        # Create SEPARATE record for this company-product combination
        record = {
            **company_data,  # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
            "Product": product,  # –£–∫–∞–∑—ã–≤–∞–µ–º –¥–ª—è –∫–∞–∫–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞ —ç—Ç–∞ –∑–∞–ø–∏—Å—å
            "All_Results": {},  # JSON —Å –í–°–ï–ú–ò —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            "Qualified_Products": "NOT QUALIFIED"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        }
        
        # Initialize results for this product
        general_passed = general_status.get(company_name, False)
        
        # Get detailed general criteria results if available
        general_detailed_info = general_status.get(f"{company_name}_detailed", {})
        general_detailed_results = general_detailed_info.get("General_Detailed_Results", [])
        general_passed_count = general_detailed_info.get("General_Passed_Count", 0)
        general_total_count = general_detailed_info.get("General_Total_Count", 0)
        
        product_results = {
            "product": product,
            "general_status": general_passed,
            "general_criteria": {
                "passed": general_passed,
                "passed_count": general_passed_count,
                "total_count": general_total_count,
                "detailed_criteria": general_detailed_results
            },
            "qualification_results": {},
            "qualified_audiences": [],
            "detailed_results": {}
        }
        
        # CRITICAL: If general criteria failed, stop processing immediately
        if not general_passed:
            log_info(f"‚ùå [{product}] {company_name} –ù–ï –ü–†–û–®–õ–ê general –∫—Ä–∏—Ç–µ—Ä–∏–∏ - –ü–†–ï–†–´–í–ê–ï–ú –∞–Ω–∞–ª–∏–∑")
            record["Qualified_Products"] = "NOT QUALIFIED - Failed General Criteria"
            record["All_Results"] = product_results
            return [record]
        
        # Check Qualification Questions for this product
        qualification_questions = product_data["qualification_questions"]
        temp_qualification_info = {}
        if PROCESSING_CONFIG['use_general_desc_for_qualification']:
            check_qualification_questions(description, temp_qualification_info, qualification_questions)
        
        # Record qualification results for ALL audiences
        for audience in qualification_questions.keys():
            qualification_result = temp_qualification_info.get(f"Qualification_{audience}", "No")
            product_results["qualification_results"][audience] = qualification_result
            
            if qualification_result == "Yes":
                product_results["qualified_audiences"].append(audience)
                log_info(f"‚úÖ [{product}] {company_name} –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è: {audience}")
        
        # If no qualified audiences, record this as NOT QUALIFIED (failed qualification)
        if not product_results["qualified_audiences"]:
            log_info(f"‚ùå [{product}] {company_name} –Ω–µ –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞ - –ù–ï –î–û–®–õ–ê –¥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤")
            record["Qualified_Products"] = "NOT QUALIFIED - Failed Qualification Questions"
            record["All_Results"] = product_results
            return [record]
        
        # Process each qualified audience with criteria batching
        results_list = []
        
        for audience in product_results["qualified_audiences"]:
            log_info(f"üéØ [{product}] {company_name} ‚Üí {audience}")
            
            # Initialize detailed results for this audience
            audience_results = {
                "audience": audience,
                "qualification_status": "Passed",
                "mandatory_status": "Not Started",
                "mandatory_criteria": [],
                "nth_results": {},
                "final_status": "Failed"
            }
            
            # Check Mandatory Criteria with batching
            temp_mandatory_info = {
                "Company_Name": company_data.get("Company_Name"),
                "Official_Website": company_data.get("Official_Website"),
                "Description": description
            }
            
            mandatory_passed = check_mandatory_criteria_batch(
                temp_mandatory_info, audience, product_data["mandatory_df"], 
                session_id=session_id, use_deep_analysis=use_deep_analysis
            )
            
            # Get detailed mandatory results - –æ–Ω–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ temp_mandatory_info —Ñ—É–Ω–∫—Ü–∏–µ–π check_mandatory_criteria_batch
            mandatory_detailed = temp_mandatory_info.get(f"Mandatory_Detailed_{audience}", [])
            audience_results["mandatory_criteria"] = mandatory_detailed
            
            if not mandatory_passed:
                log_info(f"‚ùå [{product}] {company_name} mandatory –ù–ï –ø—Ä–æ–π–¥–µ–Ω—ã –¥–ª—è {audience} - –ù–ï –î–û–®–õ–ê –¥–æ NTH")
                audience_results["mandatory_status"] = "Failed"
                audience_results["final_status"] = "Failed Mandatory"
                product_results["detailed_results"][audience] = audience_results
                
                # Create NOT QUALIFIED record for failed mandatory
                failed_mandatory_record = record.copy()
                failed_mandatory_record["Qualified_Products"] = f"NOT QUALIFIED - Failed Mandatory Criteria for {audience}"
                failed_mandatory_record["All_Results"] = product_results
                results_list.append(failed_mandatory_record)
                continue
            
            log_info(f"‚úÖ [{product}] {company_name} mandatory –ø—Ä–æ–π–¥–µ–Ω—ã –¥–ª—è {audience}")
            audience_results["mandatory_status"] = "Passed"
            
            # Check NTH Criteria with batching
            temp_nth_info = {
                "Company_Name": company_data.get("Company_Name"),
                "Official_Website": company_data.get("Official_Website"),
                "Description": description
            }
            
            check_nth_criteria_batch(
                temp_nth_info, audience, product_data["nth_df"], 
                session_id=session_id, use_deep_analysis=use_deep_analysis
            )
            
            # Record NTH results - –æ–Ω–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ temp_nth_info —Ñ—É–Ω–∫—Ü–∏–µ–π check_nth_criteria_batch
            nth_score = temp_nth_info.get(f"NTH_Score_{audience}", 0)
            nth_total = temp_nth_info.get(f"NTH_Total_{audience}", 0)
            nth_passed = temp_nth_info.get(f"NTH_Passed_{audience}", 0)
            nth_nd = temp_nth_info.get(f"NTH_ND_{audience}", 0)
            nth_detailed = temp_nth_info.get(f"NTH_Detailed_{audience}", [])
            
            # Calculate pass_rate safely
            if nth_total > 0:
                pass_rate = round(nth_passed / nth_total, 3)
                # Ensure valid float range
                if not isinstance(pass_rate, (int, float)) or not (-1e308 <= pass_rate <= 1e308):
                    pass_rate = 0.0
            else:
                pass_rate = 0.0
            
            audience_results["nth_results"] = {
                "score": nth_score,
                "total_criteria": nth_total,
                "passed_criteria": nth_passed,
                "nd_criteria": nth_nd,
                "pass_rate": pass_rate,
                "detailed_criteria": nth_detailed
            }
            
            # –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–µ–º detailed_results –¥–ª—è –∫–∞–∂–¥–æ–π –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
            product_results["detailed_results"][audience] = audience_results
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –õ–û–ì–ò–ö–ò: –µ—Å–ª–∏ –∫–æ–º–ø–∞–Ω–∏—è –¥–æ—à–ª–∞ –¥–æ NTH, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –í–°–ï–ì–î–ê
            # –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Å—á–µ—Ç–∞ (–¥–∞–∂–µ –µ—Å–ª–∏ nth_score = 0)
            
            if nth_score > 0:
                # SUCCESS! This is a QUALIFIED result with positive score
                audience_results["final_status"] = "Qualified"
                status_text = "QUALIFIED"
                log_message = f"üéâ [{product}] {company_name} QUALIFIED –¥–ª—è {audience} (Score: {nth_score:.3f})"
            else:
                # This is also QUALIFIED (passed qualification/mandatory) but with 0 NTH score
                audience_results["final_status"] = "Qualified" 
                status_text = "QUALIFIED"
                log_message = f"‚úÖ [{product}] {company_name} QUALIFIED –¥–ª—è {audience} (Score: {nth_score:.3f}) - –ø—Ä–æ—à–ª–∞ –≤—Å–µ —ç—Ç–∞–ø—ã"
            
            # Create readable text format for ALL completed NTH analyses
            result_text_parts = [
                f"{status_text}: {audience}",
                f"NTH Score: {nth_score:.3f}",
                f"Total NTH Criteria: {nth_total}",
                f"Passed: {nth_passed}",
                f"ND (No Data): {nth_nd}"
            ]
            
            result_text = "\n".join(result_text_parts)
            
            # Create a copy of the record for this completed analysis
            result_record = record.copy()
            result_record["Qualified_Products"] = result_text
            result_record["All_Results"] = product_results
            results_list.append(result_record)
            
            log_info(log_message)
        
        # If no results at all (no audiences analyzed), return the base record
        if not results_list:
            record["All_Results"] = product_results
            record["Qualified_Products"] = "NO AUDIENCES ANALYZED"
            results_list.append(record)
        
        return results_list
        
    except Exception as e:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {company_name} –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ {product}: {e}")
        error_record = {
            **company_data,
            "Product": product,
            "Qualified_Products": f"ERROR: {str(e)}",
            "All_Results": {"error": str(e)}
        }
        return [error_record]


def check_mandatory_criteria_batch(company_info, audience, mandatory_df, session_id=None, use_deep_analysis=False):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ mandatory –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    # Check for Circuit Breaker exceptions
    if CIRCUIT_BREAKER_CONFIG['enable_circuit_breaker']:
        from src.utils.circuit_breaker import CircuitOpenException
        try:
            pass  # Circuit breaker check happens in openai_client
        except CircuitOpenException as e:
            log_error(f"üî¥ Circuit Breaker –±–ª–æ–∫–∏—Ä—É–µ—Ç mandatory –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è {audience}: {e}")
            return False  # Fail mandatory when circuit is open
    
    # –ü–æ–ª—É—á–∞–µ–º state_manager –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    state_manager = None
    if session_id:
        try:
            from src.utils.state_manager import ProcessingStateManager
            state_manager = ProcessingStateManager(session_id)
        except Exception as e:
            log_error(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å StateManager: {e}")
    
    if ASYNC_GPT_CONFIG['enable_async_gpt'] and not mandatory_df.empty:
        log_info(f"ü§ñ Using async GPT for mandatory criteria: {audience}")
        try:
            # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
            audience_mandatory_df = mandatory_df[mandatory_df['Target Audience'] == audience].copy()
            
            if audience_mandatory_df.empty:
                log_info(f"‚ö†Ô∏è No mandatory criteria found for audience: {audience}")
                # Store empty detailed results when no criteria
                company_info[f"Mandatory_Detailed_{audience}"] = []
                return True  # –ï—Å–ª–∏ –Ω–µ—Ç mandatory –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ passed
            
            log_info(f"üìä Filtering mandatory criteria: {len(mandatory_df)} total ‚Üí {len(audience_mandatory_df)} for {audience}")
            
            # Build context for async GPT analysis
            company_name = company_info.get("Company_Name", "Unknown")
            description = company_info.get("Description", "")
            website = company_info.get("Official_Website", "")
            
            context = f"Company: {company_name}\nDescription: {description}\nWebsite: {website}"
            
            # Use async GPT analysis for FILTERED mandatory criteria
            result = run_async_gpt_analysis_sync(
                context, audience_mandatory_df, session_id, website,
                max_concurrent=ASYNC_GPT_CONFIG['max_concurrent_gpt_requests']
            )
            
            # Check if ALL mandatory criteria passed (for mandatory, ALL must pass)
            total_mandatory = len(audience_mandatory_df)
            passed_mandatory = 0
            detailed_mandatory_results = []
            
            # Process each mandatory criterion in detail
            for idx, (_, criterion_row) in enumerate(audience_mandatory_df.iterrows()):
                criterion_info = {
                    "criteria_text": criterion_row.get("Criteria", "Unknown"),
                    "result": "Unknown"
                }
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è mandatory –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
                found_result = False
                for key, value in result.items():
                    if key.startswith("Qualified_") and value == "Yes":
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∏–Ω–¥–µ–∫—Å—É –∫—Ä–∏—Ç–µ—Ä–∏—è
                        try:
                            key_index = int(key.split("_")[-1]) - 1  # GPT —Å—á–∏—Ç–∞–µ—Ç —Å 1, –º—ã —Å 0
                            if key_index == idx:
                                criterion_info["result"] = "Pass"
                                passed_mandatory += 1
                                found_result = True
                                # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                                if state_manager:
                                    state_manager.record_criterion_result("mandatory", "Pass")
                                break
                        except (ValueError, IndexError):
                            # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                            continue
                
                if not found_result:
                    criterion_info["result"] = "Fail"
                    # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    if state_manager:
                        state_manager.record_criterion_result("mandatory", "Fail")
                
                detailed_mandatory_results.append(criterion_info)
            
            # Store detailed mandatory results in company_info for later retrieval
            company_info[f"Mandatory_Detailed_{audience}"] = detailed_mandatory_results
            
            mandatory_passed = passed_mandatory == total_mandatory
            log_info(f"‚úÖ Mandatory results for {audience}: {passed_mandatory}/{total_mandatory} passed ‚Üí {'PASS' if mandatory_passed else 'FAIL'}")
            
            return mandatory_passed
            
        except Exception as e:
            # Handle Circuit Breaker exceptions specially
            if CIRCUIT_BREAKER_CONFIG['enable_circuit_breaker']:
                from src.utils.circuit_breaker import CircuitOpenException
                if isinstance(e, CircuitOpenException):
                    log_error(f"üî¥ Circuit Breaker –æ—Ç–∫—Ä—ã—Ç –≤–æ –≤—Ä–µ–º—è mandatory –∞–Ω–∞–ª–∏–∑–∞: {e}")
                    return False  # Don't fallback when circuit is open
            
            log_error(f"‚ùå Async mandatory analysis failed: {e}")
            if ASYNC_GPT_CONFIG['fallback_to_sync']:
                log_info("üîÑ Falling back to sync mandatory analysis...")
                sync_result = check_mandatory_criteria(company_info, audience, mandatory_df, session_id, use_deep_analysis)
                
                # Create detailed results from sync function results
                audience_mandatory_df = mandatory_df[mandatory_df['Target Audience'] == audience].copy()
                detailed_mandatory_results = []
                
                for _, criterion_row in audience_mandatory_df.iterrows():
                    crit_text = criterion_row.get("Criteria", "Unknown")
                    # Try to get result from sync function
                    sync_key = f"Mandatory_{audience}_{crit_text}"
                    sync_value = company_info.get(sync_key, "Unknown")
                    
                    criterion_info = {
                        "criteria_text": crit_text,
                        "result": "Pass" if sync_value == "Passed" else "Fail" if sync_value == "Not Passed" else "ND" if sync_value == "ND" else "Unknown"
                    }
                    
                    # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ individual –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
                    
                    detailed_mandatory_results.append(criterion_info)
                
                # Store detailed results
                company_info[f"Mandatory_Detailed_{audience}"] = detailed_mandatory_results
                return sync_result
            return False
    else:
        # Use original sync function
        sync_result = check_mandatory_criteria(company_info, audience, mandatory_df, session_id, use_deep_analysis)
        
        # Create detailed results from sync function results
        audience_mandatory_df = mandatory_df[mandatory_df['Target Audience'] == audience].copy()
        detailed_mandatory_results = []
        
        for _, criterion_row in audience_mandatory_df.iterrows():
            crit_text = criterion_row.get("Criteria", "Unknown")
            # Try to get result from sync function
            sync_key = f"Mandatory_{audience}_{crit_text}"
            sync_value = company_info.get(sync_key, "Unknown")
            
            criterion_info = {
                "criteria_text": crit_text,
                "result": "Pass" if sync_value == "Passed" else "Fail" if sync_value == "Not Passed" else "ND" if sync_value == "ND" else "Unknown"
            }
            
            # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ individual –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
            
            detailed_mandatory_results.append(criterion_info)
        
        # Store detailed results
        company_info[f"Mandatory_Detailed_{audience}"] = detailed_mandatory_results
        return sync_result


def check_nth_criteria_batch(company_info, audience, nth_df, session_id=None, use_deep_analysis=False):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ NTH –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Å –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    # Check for Circuit Breaker exceptions  
    if CIRCUIT_BREAKER_CONFIG['enable_circuit_breaker']:
        from src.utils.circuit_breaker import CircuitOpenException
        try:
            pass  # Circuit breaker check happens in openai_client
        except CircuitOpenException as e:
            log_error(f"üî¥ Circuit Breaker –±–ª–æ–∫–∏—Ä—É–µ—Ç NTH –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è {audience}: {e}")
            # For NTH, set default values instead of failing
            company_info[f"NTH_Score_{audience}"] = 0
            company_info[f"NTH_Total_{audience}"] = 0
            company_info[f"NTH_Passed_{audience}"] = 0
            company_info[f"NTH_ND_{audience}"] = 0
            return
    
    # –ü–æ–ª—É—á–∞–µ–º state_manager –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    state_manager = None
    if session_id:
        try:
            from src.utils.state_manager import ProcessingStateManager
            state_manager = ProcessingStateManager(session_id)
        except Exception as e:
            log_error(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å StateManager: {e}")
    
    if ASYNC_GPT_CONFIG['enable_async_gpt'] and not nth_df.empty:
        log_info(f"ü§ñ Using async GPT for NTH criteria: {audience}")
        try:
            # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
            audience_nth_df = nth_df[nth_df['Target Audience'] == audience].copy()
            
            if audience_nth_df.empty:
                log_info(f"‚ö†Ô∏è No NTH criteria found for audience: {audience}")
                company_info[f"NTH_Score_{audience}"] = 0
                company_info[f"NTH_Total_{audience}"] = 0
                company_info[f"NTH_Passed_{audience}"] = 0
                company_info[f"NTH_ND_{audience}"] = 0
                # Store empty detailed results when no criteria
                company_info[f"NTH_Detailed_{audience}"] = []
                return
            
            log_info(f"üìä Filtering NTH criteria: {len(nth_df)} total ‚Üí {len(audience_nth_df)} for {audience}")
            
            # Build context for async GPT analysis
            company_name = company_info.get("Company_Name", "Unknown")
            description = company_info.get("Description", "")
            website = company_info.get("Official_Website", "")
            
            context = f"Company: {company_name}\nDescription: {description}\nWebsite: {website}"
            
            # Use async GPT analysis for FILTERED NTH criteria
            result = run_async_gpt_analysis_sync(
                context, audience_nth_df, session_id, website,
                max_concurrent=ASYNC_GPT_CONFIG['max_concurrent_gpt_requests']
            )
            
            # Extract detailed NTH results and update company_info
            qualified_count = 0
            total_criteria = len(audience_nth_df)
            detailed_criteria_results = []
            
            # Process each criterion in detail
            for idx, (_, criterion_row) in enumerate(audience_nth_df.iterrows()):
                criterion_info = {
                    "criteria_text": criterion_row.get("Criteria", "Unknown"),
                    "result": "Unknown"
                }
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ GPT
                criterion_text = criterion_info["criteria_text"]
                found_match = False
                
                for key, value in result.items():
                    if key.startswith("Qualified_") and value == "Yes":
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∏–Ω–¥–µ–∫—Å—É –∫—Ä–∏—Ç–µ—Ä–∏—è
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏–∑ –∫–ª—é—á–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä "Qualified_1" -> 1)
                        try:
                            key_index = int(key.split("_")[-1]) - 1  # GPT —Å—á–∏—Ç–∞–µ—Ç —Å 1, –º—ã —Å 0
                            if key_index == idx:
                                criterion_info["result"] = "Pass"
                                qualified_count += 1
                                found_match = True
                                                    # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ individual –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
                                break
                        except (ValueError, IndexError):
                            # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É
                            continue
                
                if not found_match:
                    criterion_info["result"] = "Fail"
                    # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ individual –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
                
                detailed_criteria_results.append(criterion_info)
            
            # Calculate NTH score (ensure valid float)
            if total_criteria > 0:
                nth_score = qualified_count / total_criteria
                # Ensure the score is a valid float
                if not isinstance(nth_score, (int, float)) or not (-1e308 <= nth_score <= 1e308):
                    nth_score = 0.0
            else:
                nth_score = 0.0
            
            # Update company_info with both summary and detailed results
            company_info[f"NTH_Score_{audience}"] = nth_score
            company_info[f"NTH_Total_{audience}"] = total_criteria
            company_info[f"NTH_Passed_{audience}"] = qualified_count
            company_info[f"NTH_ND_{audience}"] = total_criteria - qualified_count
            company_info[f"NTH_Detailed_{audience}"] = detailed_criteria_results
            
            log_info(f"‚úÖ NTH results for {audience}: {qualified_count}/{total_criteria} passed (Score: {nth_score:.3f})")
            
        except Exception as e:
            # Handle Circuit Breaker exceptions specially
            if CIRCUIT_BREAKER_CONFIG['enable_circuit_breaker']:
                from src.utils.circuit_breaker import CircuitOpenException
                if isinstance(e, CircuitOpenException):
                    log_error(f"üî¥ Circuit Breaker –æ—Ç–∫—Ä—ã—Ç –≤–æ –≤—Ä–µ–º—è NTH –∞–Ω–∞–ª–∏–∑–∞: {e}")
                    # Set default values when circuit is open
                    company_info[f"NTH_Score_{audience}"] = 0
                    company_info[f"NTH_Total_{audience}"] = 0
                    company_info[f"NTH_Passed_{audience}"] = 0
                    company_info[f"NTH_ND_{audience}"] = 0
                    return
            
            log_error(f"‚ùå Async NTH analysis failed: {e}")
            if ASYNC_GPT_CONFIG['fallback_to_sync']:
                log_info("üîÑ Falling back to sync NTH analysis...")
                check_nth_criteria(company_info, audience, nth_df, session_id, use_deep_analysis)
                
                # Create detailed results from sync function results
                audience_nth_df = nth_df[nth_df['Target Audience'] == audience].copy()
                detailed_criteria_results = []
                qualified_count = 0
                total_criteria = len(audience_nth_df)
                
                for _, criterion_row in audience_nth_df.iterrows():
                    crit_text = criterion_row.get("Criteria", "Unknown")
                    # Try to get result from sync function
                    sync_key = f"NTH_{audience}_{crit_text}"
                    sync_value = company_info.get(sync_key, "Unknown")
                    
                    criterion_info = {
                        "criteria_text": crit_text,
                        "result": "Pass" if sync_value == "Passed" else "Fail" if sync_value == "Not Passed" else "ND" if sync_value == "ND" else "Unknown"
                    }
                    
                    if sync_value == "Passed":
                        qualified_count += 1
                    
                    # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ individual –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
                    
                    detailed_criteria_results.append(criterion_info)
                
                # Store detailed results if not already set by sync function
                if f"NTH_Detailed_{audience}" not in company_info:
                    company_info[f"NTH_Detailed_{audience}"] = detailed_criteria_results
    else:
        # Use original sync function
        check_nth_criteria(company_info, audience, nth_df, session_id, use_deep_analysis)
        
        # Create detailed results from sync function results
        audience_nth_df = nth_df[nth_df['Target Audience'] == audience].copy()
        detailed_criteria_results = []
        qualified_count = 0
        total_criteria = len(audience_nth_df)
        
        for _, criterion_row in audience_nth_df.iterrows():
            crit_text = criterion_row.get("Criteria", "Unknown")
            # Try to get result from sync function
            sync_key = f"NTH_{audience}_{crit_text}"
            sync_value = company_info.get(sync_key, "Unknown")
            
            criterion_info = {
                "criteria_text": crit_text,
                "result": "Pass" if sync_value == "Passed" else "Fail" if sync_value == "Not Passed" else "ND" if sync_value == "ND" else "Unknown"
            }
            
            if sync_value == "Passed":
                qualified_count += 1
            
            # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ individual –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
            
            detailed_criteria_results.append(criterion_info)
        
        # Store detailed results if not already set by sync function
        if f"NTH_Detailed_{audience}" not in company_info:
            company_info[f"NTH_Detailed_{audience}"] = detailed_criteria_results


def run_parallel_analysis(companies_file=None, load_all_companies=False, session_id=None, use_deep_analysis=False, max_concurrent_companies=12, selected_products=None):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–û–†–Ø–î–û–ö - –∫–∞–∂–¥–∞—è –∫–æ–º–ø–∞–Ω–∏—è —á–µ—Ä–µ–∑ –≤—Å–µ –ø—Ä–æ–¥—É–∫—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    """
    try:
        # Load all data
        log_info("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
        data_dict = load_data(
            companies_file=companies_file,
            load_all_companies=load_all_companies,
            selected_products=selected_products
        )
        
        companies_df = data_dict["companies"]
        products = data_dict["products"]
        products_data = data_dict["products_data"]
        general_criteria = data_dict["general_criteria"]
        
        log_info(f"üè¢ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ü–û–†–Ø–î–û–ö: –ö–∞–∂–¥–∞—è –∫–æ–º–ø–∞–Ω–∏—è —á–µ—Ä–µ–∑ –≤—Å–µ –ø—Ä–æ–¥—É–∫—Ç—ã")
        log_info(f"üìä –ö–æ–º–ø–∞–Ω–∏–π: {len(companies_df)}")
        log_info(f"üì¶ –ü—Ä–æ–¥—É–∫—Ç—ã: {', '.join(products)}")
        log_info(f"üéØ –û–∂–∏–¥–∞–µ–º –∑–∞–ø–∏—Å–µ–π: {len(companies_df)} √ó {len(products)} = {len(companies_df) * len(products)}")
        
        # Initialize state manager for this session  
        state_manager = None
        if session_id:
            try:
                from src.utils.state_manager import ProcessingStateManager
                state_manager = ProcessingStateManager(session_id)
                state_manager.update_totals(len(products), len(companies_df))
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
                state_manager.initialize_criteria_totals(products_data, len(companies_df), general_criteria)
            except Exception as e:
                log_error(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å StateManager: {e}")
        
        # 1. Check General Criteria ONCE for all companies
        log_info(f"\nüåê –≠—Ç–∞–ø 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –í–°–ï–• –∫–æ–º–ø–∞–Ω–∏–π...")
        general_status = {}
        
        for index, company_row in companies_df.iterrows():
            company_data = company_row.to_dict()
            company_name = company_data.get("Company_Name", "Unknown")
            description = company_data.get("Description", "")
            
            log_info(f"üåê General –¥–ª—è: {company_name}")
            
            try:
                temp_general_info = {}
                general_passed = check_general_criteria(description, temp_general_info, general_criteria)
                general_status[company_name] = general_passed
                
                # Store detailed general criteria information
                general_status[f"{company_name}_detailed"] = temp_general_info
                
                # –ù–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º individual –∫—Ä–∏—Ç–µ—Ä–∏–∏, —Ç–æ–ª—å–∫–æ –∫–æ–º–ø–∞–Ω–∏–∏
                
                if general_passed:
                    log_info("‚úÖ General –ø—Ä–æ–π–¥–µ–Ω—ã")
                else:
                    log_info("‚ùå General –ù–ï –ø—Ä–æ–π–¥–µ–Ω—ã")
                
                # Save progress for general criteria
                if state_manager:
                    state_manager.save_progress(0, index + 1, stage="general_criteria")
                    
            except Exception as e:
                log_error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ general –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è {company_name}: {e}")
                general_status[company_name] = False
                # –£–±–∏—Ä–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ individual –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
                if state_manager:
                    state_manager.save_progress(0, index + 1, stage="general_criteria")
        
        # 2. –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–û–†–Ø–î–û–ö: Process each COMPANY through all PRODUCTS
        all_results = []

        def process_single_company_all_products(company_args):
            """
            –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –û–î–ù–£ –∫–æ–º–ø–∞–Ω–∏—é —á–µ—Ä–µ–∑ –í–°–ï –ø—Ä–æ–¥—É–∫—Ç—ã.
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –û–î–ù–£ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—É—é –∑–∞–ø–∏—Å—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ –≤—Å–µ–º –ø—Ä–æ–¥—É–∫—Ç–∞–º.
            """
            company_row, products_data, general_status, session_id, use_deep_analysis = company_args
            
            company_data = company_row.to_dict()
            company_name = company_data.get("Company_Name", "Unknown")
            
            log_info(f"üè¢ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–º–ø–∞–Ω–∏—é: {company_name} —á–µ—Ä–µ–∑ –í–°–ï –ø—Ä–æ–¥—É–∫—Ç—ã: {', '.join(products)}")
            
            # Create ONE consolidated record for this company
            consolidated_record = {
                **company_data,  # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
                "All_Results": {},  # JSON —Å–æ –í–°–ï–ú–ò –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                "Qualified_Products": ""  # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –ø—Ä–æ–¥—É–∫—Ç–∞–º
            }
            
            all_products_results = {}
            qualified_products_text = []
            
            # Process this company through ALL products
            for product in products:
                try:
                    log_info(f"  üì¶ {company_name} ‚Üí {product}")
                    
                    # Use the existing function for this company-product combination
                    args = (company_row, product, products_data[product], general_status, session_id, use_deep_analysis)
                    product_results = process_single_company_for_product(args)
                    
                    # Extract the product results from the returned list
                    if product_results and len(product_results) > 0:
                        # Get the All_Results from the first result (they should all be the same for this product)
                        product_result = product_results[0]
                        product_all_results = product_result.get("All_Results", {})
                        product_qualified_text = product_result.get("Qualified_Products", "")
                        
                        # Store results for this product
                        all_products_results[product] = product_all_results
                        
                        # Add to qualified products text
                        if product_qualified_text and product_qualified_text != "NOT QUALIFIED":
                            qualified_products_text.append(f"=== {product.upper()} ===\n{product_qualified_text}")
                        else:
                            qualified_products_text.append(f"=== {product.upper()} ===\nNOT QUALIFIED")
                    
                except Exception as e:
                    log_error(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {company_name} –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ {product}: {e}")
                    all_products_results[product] = {"error": str(e)}
                    qualified_products_text.append(f"=== {product.upper()} ===\nERROR: {str(e)}")
            
            # Consolidate all results
            consolidated_record["All_Results"] = all_products_results
            consolidated_record["Qualified_Products"] = "\n\n".join(qualified_products_text) if qualified_products_text else "NOT QUALIFIED"
            
            log_info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–ø–∞–Ω–∏–∏ {company_name}: –û–î–ù–ê –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å —Å {len(products)} –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏")
            return [consolidated_record]  # Return as list for consistency
        
        # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –∫–æ–º–ø–∞–Ω–∏–π (–∫–∞–∂–¥–∞—è –∫–æ–º–ø–∞–Ω–∏—è —á–µ—Ä–µ–∑ –í–°–ï –ø—Ä–æ–¥—É–∫—Ç—ã)
        log_info(f"\nüöÄ –≠—Ç–∞–ø 2: –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–û–†–Ø–î–û–ö - –∫–∞–∂–¥–∞—è –∫–æ–º–ø–∞–Ω–∏—è —á–µ—Ä–µ–∑ –≤—Å–µ –ø—Ä–æ–¥—É–∫—Ç—ã")
        log_info(f"‚ö° –ö–æ–º–ø–∞–Ω–∏–∏: {len(companies_df)}")
        log_info(f"üì¶ –ü—Ä–æ–¥—É–∫—Ç—ã: {', '.join(products)}")
        log_info(f"üìä –û–∂–∏–¥–∞–µ–º –∑–∞–ø–∏—Å–µ–π: {len(companies_df)} (–ø–æ –æ–¥–Ω–æ–π –Ω–∞ –∫–æ–º–ø–∞–Ω–∏—é —Å –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏)")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        company_args = []
        for index, company_row in companies_df.iterrows():
            args = (company_row, products_data, general_status, session_id, use_deep_analysis)
            company_args.append(args)
        
        # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –∫–æ–º–ø–∞–Ω–∏–π —Å Circuit Breaker
        circuit_breaker_triggered = False
        
        try:
            with ThreadPoolExecutor(max_workers=max_concurrent_companies) as executor:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ –í–°–ï –ø—Ä–æ–¥—É–∫—Ç—ã
                future_to_company = {
                    executor.submit(process_single_company_all_products, args): args[0].get("Company_Name", f"Company_{i}")
                    for i, args in enumerate(company_args)
                }
                
                # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                for future in as_completed(future_to_company):
                    company_name = future_to_company[future]
                    try:
                        company_results = future.result()
                        all_results.extend(company_results)
                        log_info(f"üéâ –ö–æ–º–ø–∞–Ω–∏—è {company_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(company_results)} –∑–∞–ø–∏—Å–µ–π")
                        
                        # Mark company as completed in state manager (–¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤)
                        if state_manager:
                            for product in products:
                                state_manager.mark_company_completed(company_name, product, success=True)
                                
                    except Exception as e:
                        # Handle Circuit Breaker exceptions
                        if CIRCUIT_BREAKER_CONFIG['enable_circuit_breaker']:
                            from src.utils.circuit_breaker import CircuitOpenException
                            if isinstance(e, CircuitOpenException):
                                log_error(f"üî¥ Circuit Breaker —Å—Ä–∞–±–æ—Ç–∞–ª –¥–ª—è {company_name}: {e}")
                                circuit_breaker_triggered = True
                                if state_manager:
                                    state_manager.record_circuit_breaker_event("triggered_during_processing", {
                                        "company": company_name,
                                        "error": str(e)
                                    })
                                break  # Stop processing
                        
                        log_error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏ {company_name}: {e}")
                        if state_manager:
                            for product in products:
                                state_manager.mark_company_completed(company_name, product, success=False)
            
            # Save partial results
            if state_manager and all_results:
                state_manager.save_partial_results(all_results)
                
        except Exception as e:
            log_error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            if state_manager:
                state_manager.record_circuit_breaker_event("critical_error", {
                    "error": str(e),
                    "stage": "parallel_processing"
                })
        
        # Count qualified companies
        qualified_count = sum(1 for result in all_results if result["Qualified_Products"] != "NOT QUALIFIED")
        
        # Save results
        log_info("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
        json_path, csv_path = save_results(all_results, "PARALLEL_BY_COMPANIES", session_id=session_id)
        
        log_info(f"""
üéâ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (–ö–û–ù–°–û–õ–ò–î–ò–†–û–í–ê–ù–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´):
   üè¢ –ö–æ–º–ø–∞–Ω–∏–∏: {len(companies_df)}
   üì¶ –ü—Ä–æ–¥—É–∫—Ç—ã: {', '.join(products)}
   üìä –ó–∞–ø–∏—Å–µ–π (–æ–¥–Ω–∞ –Ω–∞ –∫–æ–º–ø–∞–Ω–∏—é): {len(all_results)}
   ‚úÖ –ö–æ–º–ø–∞–Ω–∏–π —Å –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–µ–π: {qualified_count}
   üìÑ JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {json_path}
   üìã CSV —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {csv_path}""")
        
        # Mark session as completed
        if state_manager:
            state_manager.mark_completed("completed")
        
        return all_results
        
    except Exception as e:
        log_error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        raise 