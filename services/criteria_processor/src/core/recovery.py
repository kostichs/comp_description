"""
Recovery –º–æ–¥—É–ª—å –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
"""

import json
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from src.utils.logging import log_info, log_error, log_debug
from src.utils.state_manager import ProcessingStateManager
from src.core.parallel_processor import run_parallel_analysis
from src.data.search_data_saver import initialize_search_data_saver, finalize_search_data_saving


def resume_processing(session_id: str, 
                     companies_file: str = None,
                     load_all_companies: bool = False,
                     use_deep_analysis: bool = False,
                     max_concurrent_companies: int = 12) -> Tuple[bool, str, Optional[List[Dict[str, Any]]]]:
    """
    –í–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å –ø—Ä–µ—Ä–≤–∞–Ω–Ω—É—é —Å–µ—Å—Å–∏—é –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
    
    Args:
        session_id: ID —Å–µ—Å—Å–∏–∏ –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        companies_file: –§–∞–π–ª –∫–æ–º–ø–∞–Ω–∏–π (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω –¥–ª—è –Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
        load_all_companies: –ó–∞–≥—Ä—É–∂–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã
        use_deep_analysis: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
        max_concurrent_companies: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π
        
    Returns:
        (success, message, results) - —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    """
    try:
        log_info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–µ—Å—Å–∏–∏: {session_id}")
        
        # Initialize state manager for the session
        state_manager = ProcessingStateManager(session_id)
        
        # Check if session can be resumed
        can_resume, reason = state_manager.can_resume()
        if not can_resume:
            return False, f"Cannot resume session: {reason}", None
        
        # Load saved progress
        progress = state_manager.load_progress()
        if not progress:
            return False, "No progress data found", None
        
        # Load existing results
        existing_results = state_manager.load_partial_results()
        log_info(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(existing_results)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        # Get session state summary
        state_summary = state_manager.get_state_summary()
        log_info(f"üìä –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏:")
        log_info(f"   –°—Ç–∞—Ç—É—Å: {state_summary.get('status', 'unknown')}")
        log_info(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {state_summary.get('progress', {})}")
        log_info(f"   –¢–µ–∫—É—â–∏–π: {state_summary.get('current', {})}")
        
        # Validate partial results
        valid_results, validation_report = validate_partial_results(existing_results)
        if validation_report['errors']:
            log_error(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö:")
            for error in validation_report['errors']:
                log_error(f"   - {error}")
        
        log_info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {validation_report['valid_count']}/{validation_report['total_count']} –≤–∞–ª–∏–¥–Ω—ã—Ö")
        
        # Mark session as resuming
        state_manager.save_progress(
            progress.get('current_product_index', 0),
            progress.get('current_company_index', 0),
            progress.get('current_product'),
            progress.get('current_company'),
            progress.get('current_audience'),
            "resuming"
        )
        
        # Record resume event
        state_manager.record_circuit_breaker_event("session_resumed", {
            "previous_status": progress.get('status'),
            "existing_results_count": len(valid_results),
            "resume_reason": reason
        })
        
        log_info(f"üöÄ –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –º–µ—Å—Ç–∞...")
        
        # Initialize search data saver for resumed session
        initialize_search_data_saver(session_id)
        log_info(f"üîß Initialized search data saver for resumed session: {session_id}")
        
        # Resume actual processing
        # Note: The run_parallel_analysis function will automatically load 
        # existing results through state_manager.load_partial_results()
        new_results = run_parallel_analysis(
            companies_file=companies_file,
            load_all_companies=load_all_companies,
            session_id=session_id,
            use_deep_analysis=use_deep_analysis,
            max_concurrent_companies=max_concurrent_companies
        )
        
        log_info(f"üéâ –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        log_info(f"üìä –ò—Ç–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(new_results)} (–≤–∫–ª—é—á–∞—è —Ä–∞–Ω–µ–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)")
        
        return True, "Session resumed successfully", new_results
        
    except Exception as e:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–µ—Å—Å–∏–∏ {session_id}: {e}")
        return False, f"Resume failed: {str(e)}", None


def validate_partial_results(results: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Args:
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        
    Returns:
        (valid_results, validation_report) - –≤–∞–ª–∏–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ—Ç—á–µ—Ç
    """
    valid_results = []
    errors = []
    warnings = []
    
    required_fields = ['Company_Name', 'Product']
    
    for i, result in enumerate(results):
        try:
            # Check required fields
            missing_fields = [field for field in required_fields if field not in result or not result[field]]
            if missing_fields:
                errors.append(f"Result {i}: Missing required fields: {missing_fields}")
                continue
            
            # Check if All_Results is valid JSON structure
            if 'All_Results' in result:
                all_results = result['All_Results']
                if isinstance(all_results, str):
                    try:
                        json.loads(all_results)
                    except json.JSONDecodeError:
                        warnings.append(f"Result {i}: Invalid JSON in All_Results for {result.get('Company_Name', 'Unknown')}")
                elif not isinstance(all_results, dict):
                    warnings.append(f"Result {i}: All_Results is not dict or JSON string for {result.get('Company_Name', 'Unknown')}")
            
            # Check qualification status
            qualified_products = result.get('Qualified_Products', '')
            if not qualified_products:
                warnings.append(f"Result {i}: Empty Qualified_Products for {result.get('Company_Name', 'Unknown')}")
            
            valid_results.append(result)
            
        except Exception as e:
            errors.append(f"Result {i}: Validation error: {str(e)}")
    
    validation_report = {
        'total_count': len(results),
        'valid_count': len(valid_results),
        'error_count': len(errors),
        'warning_count': len(warnings),
        'errors': errors,
        'warnings': warnings,
        'success_rate': len(valid_results) / len(results) if results else 0
    }
    
    return valid_results, validation_report


def get_resumable_sessions(base_output_dir: str = None) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–µ—Å—Å–∏–π –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å
    
    Args:
        base_output_dir: –ë–∞–∑–æ–≤–∞—è –ø–∞–ø–∫–∞ output
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º—ã—Ö —Å–µ—Å—Å–∏—è—Ö
    """
    resumable_sessions = []
    
    try:
        if base_output_dir is None:
            from src.utils.config import OUTPUT_DIR
            base_output_dir = OUTPUT_DIR
        
        output_path = Path(base_output_dir)
        
        if not output_path.exists():
            return resumable_sessions
        
        # –ò—â–µ–º –ø–∞–ø–∫–∏ —Å–µ—Å—Å–∏–π
        for session_dir in output_path.iterdir():
            if not session_dir.is_dir():
                continue
                
            session_id = session_dir.name
            
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª—ã —Å–æ—Å—Ç–æ—è–Ω–∏—è
                progress_file = session_dir / f"{session_id}_progress.json"
                
                if progress_file.exists():
                    state_manager = ProcessingStateManager(session_id, base_output_dir)
                    can_resume, reason = state_manager.can_resume()
                    
                    if can_resume:
                        state_summary = state_manager.get_state_summary()
                        resumable_sessions.append({
                            'session_id': session_id,
                            'status': state_summary.get('status', 'unknown'),
                            'progress': state_summary.get('progress', {}),
                            'current': state_summary.get('current', {}),
                            'results_count': state_summary.get('results_count', 0),
                            'last_updated': state_summary.get('last_updated'),
                            'can_resume': True,
                            'resume_reason': reason
                        })
                    else:
                        # Add info about non-resumable sessions too
                        resumable_sessions.append({
                            'session_id': session_id,
                            'can_resume': False,
                            'resume_reason': reason
                        })
                        
            except Exception as e:
                log_debug(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ—Å—Å–∏–∏ {session_id}: {e}")
                continue
    
    except Exception as e:
        log_error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º—ã—Ö —Å–µ—Å—Å–∏–π: {e}")
    
    return resumable_sessions


def cleanup_failed_sessions(base_output_dir: str = None, days_old: int = 7) -> Dict[str, Any]:
    """
    –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –Ω–µ—É—Å–ø–µ—à–Ω—ã—Ö —Å–µ—Å—Å–∏–π
    
    Args:
        base_output_dir: –ë–∞–∑–æ–≤–∞—è –ø–∞–ø–∫–∞ output
        days_old: –£–¥–∞–ª—è—Ç—å —Å–µ—Å—Å–∏–∏ —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π
        
    Returns:
        –û—Ç—á–µ—Ç –æ–± –æ—á–∏—Å—Ç–∫–µ
    """
    import time
    from datetime import datetime, timedelta
    
    cleanup_report = {
        'sessions_checked': 0,
        'sessions_deleted': 0,
        'freed_space': 0,
        'errors': []
    }
    
    try:
        if base_output_dir is None:
            from src.utils.config import OUTPUT_DIR
            base_output_dir = OUTPUT_DIR
        
        output_path = Path(base_output_dir)
        cutoff_time = datetime.now() - timedelta(days=days_old)
        
        for session_dir in output_path.iterdir():
            if not session_dir.is_dir():
                continue
                
            cleanup_report['sessions_checked'] += 1
            
            try:
                # Check session age
                modified_time = datetime.fromtimestamp(session_dir.stat().st_mtime)
                
                if modified_time < cutoff_time:
                    # Check if session is failed/cancelled
                    session_id = session_dir.name
                    state_manager = ProcessingStateManager(session_id, base_output_dir)
                    progress = state_manager.load_progress()
                    
                    if progress and progress.get('status') in ['failed', 'cancelled']:
                        # Calculate size before deletion
                        size_before = sum(f.stat().st_size for f in session_dir.rglob('*') if f.is_file())
                        
                        # Delete session directory
                        import shutil
                        shutil.rmtree(session_dir)
                        
                        cleanup_report['sessions_deleted'] += 1
                        cleanup_report['freed_space'] += size_before
                        
                        log_info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è —Å–µ—Å—Å–∏—è: {session_id}")
                        
            except Exception as e:
                cleanup_report['errors'].append(f"Error cleaning session {session_dir.name}: {str(e)}")
    
    except Exception as e:
        cleanup_report['errors'].append(f"Error during cleanup: {str(e)}")
    
    return cleanup_report 