"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤
"""

import os
import glob
import pandas as pd
from src.utils.config import DATA_DIR, COMPANIES_LIMIT, CRITERIA_DIR, CRITERIA_TYPE, INDUSTRY_MAPPING
from src.utils.logging import log_info, log_error, log_debug
from src.utils.encoding_handler import (
    read_csv_with_encoding, 
    read_excel_with_encoding, 
    normalize_text_encoding,
    get_file_info
)

def load_file_smart(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è—è —Ç–∏–ø (CSV –∏–ª–∏ Excel) —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∏ –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ, –≤–∫–ª—é—á–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫—É
    file_info = get_file_info(file_path)
    log_info(f"üìÅ –§–∞–π–ª: {os.path.basename(file_path)} ({file_info.get('size_mb', 0)} MB, {file_info.get('detected_encoding', 'unknown')})")
    
    try:
        if file_ext in ['.csv']:
            log_debug(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º CSV —Ñ–∞–π–ª: {os.path.basename(file_path)}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π encoding handler –¥–ª—è CSV
            df, used_encoding = read_csv_with_encoding(file_path, quoting=1, on_bad_lines='skip')
            log_info(f"‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {used_encoding}")
            
            # –£–î–ê–õ–ï–ù–ò–ï –ù–ï–ñ–ï–õ–ê–¢–ï–õ–¨–ù–´–• –ö–û–õ–û–ù–û–ö: —É–±–∏—Ä–∞–µ–º validation –∫–æ–ª–æ–Ω–∫–∏
            columns_to_remove = ['validation_status', 'validation_warning']
            columns_removed = []
            for col in columns_to_remove:
                if col in df.columns:
                    df = df.drop(columns=[col])
                    columns_removed.append(col)
            
            if columns_removed:
                log_info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {', '.join(columns_removed)}")
            
            # –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê: –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—è—Ö
            text_columns = ['Company_Name', 'Description', 'Official_Website', 'LinkedIn_URL']
            normalized_columns = []
            for col in text_columns:
                if col in df.columns:
                    df[col] = df[col].apply(lambda x: normalize_text_encoding(str(x)) if pd.notna(x) else x)
                    normalized_columns.append(col)
            
            if normalized_columns:
                log_info(f"üßπ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(normalized_columns)}")
            
            # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–£–°–¢–´–• –°–¢–†–û–ö: —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—É—Å—Ç—ã–µ
            main_columns = ['Company_Name', 'Description']  # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            existing_columns = [col for col in main_columns if col in df.columns]
            
            if existing_columns:
                # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –í–°–ï –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—É—Å—Ç—ã–µ (NaN, None, –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞)
                df_before = len(df)
                df = df.dropna(subset=existing_columns, how='all')  # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –í–°–ï –∫–æ–ª–æ–Ω–∫–∏ NaN
                df = df[df[existing_columns].ne('').any(axis=1)]   # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –í–°–ï –∫–æ–ª–æ–Ω–∫–∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                df_after = len(df)
                
                filtered_count = df_before - df_after
                if filtered_count > 0:
                    log_info(f"üßπ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {filtered_count} –∏–∑ {df_before}")
                
            return df
        elif file_ext in ['.xlsx', '.xls']:
            log_debug(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º Excel —Ñ–∞–π–ª: {os.path.basename(file_path)}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π encoding handler –¥–ª—è Excel
            df, used_encoding = read_excel_with_encoding(file_path)
            log_info(f"‚úÖ Excel –∑–∞–≥—Ä—É–∂–µ–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {used_encoding}")
            
            # –£–î–ê–õ–ï–ù–ò–ï –ù–ï–ñ–ï–õ–ê–¢–ï–õ–¨–ù–´–• –ö–û–õ–û–ù–û–ö: —É–±–∏—Ä–∞–µ–º validation –∫–æ–ª–æ–Ω–∫–∏
            columns_to_remove = ['validation_status', 'validation_warning']
            columns_removed = []
            for col in columns_to_remove:
                if col in df.columns:
                    df = df.drop(columns=[col])
                    columns_removed.append(col)
            
            if columns_removed:
                log_info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {', '.join(columns_removed)}")
            
            # –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê –¥–ª—è Excel: –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—è—Ö
            text_columns = ['Company_Name', 'Description', 'Official_Website', 'LinkedIn_URL']
            normalized_columns = []
            for col in text_columns:
                if col in df.columns:
                    df[col] = df[col].apply(lambda x: normalize_text_encoding(str(x)) if pd.notna(x) else x)
                    normalized_columns.append(col)
            
            if normalized_columns:
                log_info(f"üßπ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(normalized_columns)}")
            
            # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è Excel
            main_columns = ['Company_Name', 'Description']
            existing_columns = [col for col in main_columns if col in df.columns]
            
            if existing_columns:
                df_before = len(df)
                df = df.dropna(subset=existing_columns, how='all')
                df = df[df[existing_columns].ne('').any(axis=1)]
                df_after = len(df)
                
                filtered_count = df_before - df_after
                if filtered_count > 0:
                    log_info(f"üßπ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {filtered_count} –∏–∑ {df_before}")
            
            return df
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_ext}")
    except Exception as e:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        raise

def load_companies_data(file_path=None):
    """Load companies data - either from specific file or automatically find first CSV"""
    if file_path:
        log_info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {file_path}")
        companies_df = load_file_smart(file_path)
    else:
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π CSV —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ data
        csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]
        if not csv_files:
            raise FileNotFoundError(f"‚ùå CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {DATA_DIR}")
        
        first_csv = os.path.join(DATA_DIR, csv_files[0])
        log_info(f"üìä –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω —Ñ–∞–π–ª: {first_csv}")
        companies_df = load_file_smart(first_csv)
    
    if COMPANIES_LIMIT > 0:
        log_info(f"‚ö†Ô∏è  –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {COMPANIES_LIMIT} –∫–æ–º–ø–∞–Ω–∏–π")
        companies_df = companies_df.head(COMPANIES_LIMIT)
    
    log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(companies_df)}")
    return companies_df

def load_all_companies_from_data_folder():
    """Load and combine all CSV and Excel company files from data/ directory"""
    data_dir = DATA_DIR  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–ø–∫—É data –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"‚ùå –ü–∞–ø–∫–∞ data –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_dir}")
    
    # Find all CSV and Excel files in data directory
    company_files = []
    company_files.extend(glob.glob(os.path.join(data_dir, "*.csv")))
    company_files.extend(glob.glob(os.path.join(data_dir, "*.xlsx")))
    company_files.extend(glob.glob(os.path.join(data_dir, "*.xls")))
    
    if not company_files:
        raise FileNotFoundError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ CSV/Excel —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ: {data_dir}")
    
    log_info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π: {len(company_files)}")
    for file_path in company_files:
        file_ext = os.path.splitext(file_path)[1].lower()
        log_info(f"   - {os.path.basename(file_path)} ({file_ext.upper()})")
    
    all_companies = []
    
    for file_path in company_files:
        filename = os.path.basename(file_path)
        file_ext = os.path.splitext(file_path)[1].lower()
        log_info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑: {filename} ({file_ext.upper()})")
        
        try:
            df = load_file_smart(file_path)
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {filename}: {len(df)} –∫–æ–º–ø–∞–Ω–∏–π")
            all_companies.append(df)
            
        except Exception as e:
            log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
            continue
    
    if not all_companies:
        raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∫–æ–º–ø–∞–Ω–∏–π")
    
    # Combine all companies into single DataFrame
    combined_companies = pd.concat(all_companies, ignore_index=True)
    
    if COMPANIES_LIMIT > 0:
        log_info(f"‚ö†Ô∏è  –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {COMPANIES_LIMIT} –∫–æ–º–ø–∞–Ω–∏–π")
        combined_companies = combined_companies.head(COMPANIES_LIMIT)
    
    log_info(f"üéØ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(combined_companies)}")
    return combined_companies

def load_all_criteria_files():
    """Load and combine all criteria files from criteria/ directory"""
    criteria_dir = CRITERIA_DIR
    
    if not os.path.exists(criteria_dir):
        raise FileNotFoundError(f"‚ùå –ü–∞–ø–∫–∞ criteria –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {criteria_dir}")
    
    # Find all CSV files in criteria directory
    criteria_files = glob.glob(os.path.join(criteria_dir, "*.csv"))
    
    if not criteria_files:
        raise FileNotFoundError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –≤ –ø–∞–ø–∫–µ: {criteria_dir}")
    
    log_info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {len(criteria_files)}")
    
    all_criteria = []
    
    for file_path in criteria_files:
        filename = os.path.basename(file_path)
        log_info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑: {filename}")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º encoding handler –¥–ª—è —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
            file_info = get_file_info(file_path)
            log_debug(f"üìÅ –ö—Ä–∏—Ç–µ—Ä–∏–∏: {filename} ({file_info.get('detected_encoding', 'unknown')})")
            
            df, used_encoding = read_csv_with_encoding(file_path)
            log_debug(f"‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {used_encoding}")
            
            # –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê –≤ –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö
            text_columns = ['Product', 'Target Audience', 'Criteria Type', 'Criteria', 'Place', 'Search Query', 'Signals']
            normalized_columns = []
            for col in text_columns:
                if col in df.columns:
                    df[col] = df[col].apply(lambda x: normalize_text_encoding(str(x)) if pd.notna(x) else x)
                    normalized_columns.append(col)
            
            if normalized_columns:
                log_debug(f"üßπ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∫—Ä–∏—Ç–µ—Ä–∏–∏: {', '.join(normalized_columns)}")
            
            # Validate required columns
            required_columns = ['Product', 'Target Audience', 'Criteria Type', 'Criteria']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                log_error(f"‚ùå –í —Ñ–∞–π–ª–µ {filename} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
                continue
            
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {filename}: {len(df)} –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤")
            all_criteria.append(df)
            
        except Exception as e:
            log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
            continue
    
    if not all_criteria:
        raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤")
    
    # Combine all criteria into single DataFrame
    combined_criteria = pd.concat(all_criteria, ignore_index=True)
    
    log_info(f"üéØ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {len(combined_criteria)}")
    
    # Show products and types summary
    products = combined_criteria['Product'].unique()
    criteria_types = combined_criteria['Criteria Type'].unique()
    
    log_info(f"üìä –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–¥—É–∫—Ç—ã: {', '.join(products)}")
    log_info(f"üìä –¢–∏–ø—ã –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {', '.join(criteria_types)}")
    
    return combined_criteria

def load_data(companies_file=None, load_all_companies=False, selected_products=None):
    """Load all data files - updated for ALL PRODUCTS processing
    
    Args:
        companies_file: –ø—É—Ç—å –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ñ–∞–π–ª—É –∫–æ–º–ø–∞–Ω–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        load_all_companies: –µ—Å–ª–∏ True, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ CSV —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ data/
        selected_products: —Å–ø–∏—Å–æ–∫ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–µ—Å–ª–∏ None - –≤—Å–µ –ø—Ä–æ–¥—É–∫—Ç—ã)
    """
    log_info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –í–°–ï–• –ø—Ä–æ–¥—É–∫—Ç–æ–≤")
    
    try:
        # Load companies data based on parameters
        if load_all_companies:
            log_info("üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï —Ñ–∞–π–ª—ã –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ –ø–∞–ø–∫–∏ data/")
            companies_df = load_all_companies_from_data_folder()
        elif companies_file:
            log_info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {companies_file}")
            companies_df = load_companies_data(companies_file)
        else:
            log_info(f"üìä –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º CSV —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ: {DATA_DIR}")
            companies_df = load_companies_data()
        
        log_info(f"‚úÖ –ò—Ç–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(companies_df)}")
        
        # Load all criteria files automatically
        df_criteria = load_all_criteria_files()
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        log_info("üåê –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤...")
        all_general_raw = df_criteria[df_criteria["Criteria Type"] == "General"]["Criteria"].dropna().tolist()
        
        # –£–ú–ù–ê–Ø –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø General –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        def deduplicate_general_criteria(criteria_list):
            """Remove duplicate and similar criteria"""
            import re
            
            deduplicated = []
            seen_patterns = []
            
            for criteria in criteria_list:
                criteria_lower = criteria.lower()
                
                # Skip WAAP-specific criteria for non-WAAP analysis
                if "waap" in criteria_lower or "special protocols" in criteria_lower:
                    log_debug(f"   ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º WAAP-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π: {criteria[:50]}...")
                    continue
                
                # Check for HQ/headquarters duplicates
                is_hq_criteria = ("headquarter" in criteria_lower or "hq" in criteria_lower) and any(country in criteria_lower for country in ["china", "iran", "russia"])
                
                if is_hq_criteria:
                    # Check if we already have a similar HQ criteria
                    has_similar_hq = any("hq_criteria" in pattern for pattern in seen_patterns)
                    if has_similar_hq:
                        log_debug(f"   üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–π HQ –∫—Ä–∏—Ç–µ—Ä–∏–π: {criteria[:50]}...")
                        continue
                    else:
                        seen_patterns.append("hq_criteria")
                        deduplicated.append(criteria)
                        log_debug(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω HQ –∫—Ä–∏—Ç–µ—Ä–∏–π: {criteria[:50]}...")
                else:
                    # For non-HQ criteria, check for exact duplicates
                    if criteria not in deduplicated:
                        deduplicated.append(criteria)
                        log_debug(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –∫—Ä–∏—Ç–µ—Ä–∏–π: {criteria[:50]}...")
                    else:
                        log_debug(f"   üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç: {criteria[:50]}...")
            
            return deduplicated
        
        all_general_criteria = deduplicate_general_criteria(all_general_raw)
        log_info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö General –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {len(all_general_criteria)} (–±—ã–ª–æ {len(all_general_raw)})")
        for i, criteria in enumerate(all_general_criteria, 1):
            log_info(f"   {i}. {criteria}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –≤—ã–±—Ä–∞–Ω–Ω—ã–µ
        all_available_products = df_criteria['Product'].unique()
        
        if selected_products:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã
            products = [p for p in all_available_products if p in selected_products]
            if not products:
                raise ValueError(f"‚ùå –í—ã–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã {selected_products} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ä–µ–¥–∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö: {list(all_available_products)}")
            log_info(f"üéØ –ë—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¢–û–õ–¨–ö–û –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã: {', '.join(products)} (–∏–∑ {len(all_available_products)} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö)")
        else:
            products = all_available_products
            log_info(f"üè≠ –ë—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –í–°–ï–• –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {', '.join(products)}")
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        all_products_data = {}
        
        for product in products:
            product_criteria = df_criteria[df_criteria["Product"] == product]
            log_info(f"üìã –ù–∞–π–¥–µ–Ω–æ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ {product}: {len(product_criteria)}")
            
            # Extract qualification questions FOR THIS PRODUCT
            qualification_df = product_criteria[product_criteria["Criteria Type"] == "Qualification"].dropna(subset=["Criteria", "Target Audience"])
            qualification_questions = {
                row["Target Audience"]: row["Criteria"]
                for _, row in qualification_df.iterrows()
            }
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è {product}: {len(qualification_questions)}")
            
            # Load mandatory criteria FOR THIS PRODUCT
            mandatory_df = product_criteria[product_criteria["Criteria Type"] == "Mandatory"].dropna(subset=["Criteria", "Target Audience"])
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è {product}: {len(mandatory_df)}")
            
            # Load NTH criteria FOR THIS PRODUCT
            nth_df = product_criteria[product_criteria["Criteria Type"] == "NTH"].dropna(subset=["Criteria", "Target Audience"])
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ NTH –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è {product}: {len(nth_df)}")
            
            all_products_data[product] = {
                "qualification_questions": qualification_questions,
                "mandatory_df": mandatory_df,
                "nth_df": nth_df
            }
        
        log_info("üöÄ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        log_info(f"General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –±—É–¥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫–æ –≤—Å–µ–º –∫–æ–º–ø–∞–Ω–∏—è–º")
        log_info(f"üéØ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {', '.join(products)}")
        
        return {
            "companies": companies_df,
            "general_criteria": all_general_criteria,  # –í–°–ï General –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
            "products_data": all_products_data,  # –î–∞–Ω–Ω—ã–µ –¥–ª—è –í–°–ï–• –ø—Ä–æ–¥—É–∫—Ç–æ–≤
            "products": list(products)  # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        }
        
    except FileNotFoundError as e:
        log_error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
        raise
    except Exception as e:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise 