"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–æ–≤
"""

import os
import glob
import pandas as pd
from src.utils.config import DATA_DIR, COMPANIES_LIMIT, CRITERIA_DIR, CRITERIA_TYPE, INDUSTRY_MAPPING
from src.utils.logging import log_info, log_error, log_debug
from src.data.encodings import load_csv_with_encoding

def load_file_smart(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è—è —Ç–∏–ø (CSV –∏–ª–∏ Excel) —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_ext in ['.csv']:
            log_debug(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º CSV —Ñ–∞–π–ª: {os.path.basename(file_path)}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è CSV —Å –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
            df = pd.read_csv(file_path, quoting=1, encoding='utf-8', on_bad_lines='skip')
            
            # –£–î–ê–õ–ï–ù–ò–ï –ù–ï–ñ–ï–õ–ê–¢–ï–õ–¨–ù–´–• –ö–û–õ–û–ù–û–ö: —É–±–∏—Ä–∞–µ–º validation –∫–æ–ª–æ–Ω–∫–∏
            columns_to_remove = ['validation_status', 'validation_warning']
            columns_removed = []
            for col in columns_to_remove:
                if col in df.columns:
                    df = df.drop(columns=[col])
                    columns_removed.append(col)
            
            if columns_removed:
                log_info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {', '.join(columns_removed)}")
            
            # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–£–°–¢–´–• –°–¢–†–û–ö: —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—É—Å—Ç—ã–µ
            main_columns = ['Company_Name', 'Description']  # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            existing_columns = [col for col in main_columns if col in df.columns]
            
            if existing_columns:
                # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –í–°–ï –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—É—Å—Ç—ã–µ (NaN, None, –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞)
                df_before = len(df)
                df = df.dropna(subset=existing_columns, how='all')  # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –í–°–ï –∫–æ–ª–æ–Ω–∫–∏ NaN
                df = df[df[existing_columns].ne('').any(axis=1)]   # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –í–°–ï –∫–æ–ª–æ–Ω–∫–∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                df_after = len(df)
                
                filtered_count = df_before - df_after
                if filtered_count > 0:
                    log_info(f"üßπ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {filtered_count} –∏–∑ {df_before}")
                
            return df
        elif file_ext in ['.xlsx', '.xls']:
            log_debug(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º Excel —Ñ–∞–π–ª: {os.path.basename(file_path)}")
            df = pd.read_excel(file_path)
            
            # –£–î–ê–õ–ï–ù–ò–ï –ù–ï–ñ–ï–õ–ê–¢–ï–õ–¨–ù–´–• –ö–û–õ–û–ù–û–ö: —É–±–∏—Ä–∞–µ–º validation –∫–æ–ª–æ–Ω–∫–∏
            columns_to_remove = ['validation_status', 'validation_warning']
            columns_removed = []
            for col in columns_to_remove:
                if col in df.columns:
                    df = df.drop(columns=[col])
                    columns_removed.append(col)
            
            if columns_removed:
                log_info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏: {', '.join(columns_removed)}")
            
            # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è Excel
            main_columns = ['Company_Name', 'Description']
            existing_columns = [col for col in main_columns if col in df.columns]
            
            if existing_columns:
                df_before = len(df)
                df = df.dropna(subset=existing_columns, how='all')
                df = df[df[existing_columns].ne('').any(axis=1)]
                df_after = len(df)
                
                filtered_count = df_before - df_after
                if filtered_count > 0:
                    log_info(f"üßπ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {filtered_count} –∏–∑ {df_before}")
            
            return df
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_ext}")
    except Exception as e:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        raise

def load_companies_data(file_path=None):
    """Load companies data - either from specific file or automatically find first CSV"""
    if file_path:
        log_info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {file_path}")
        companies_df = load_file_smart(file_path)
    else:
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π CSV —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ data
        csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]
        if not csv_files:
            raise FileNotFoundError(f"‚ùå CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {DATA_DIR}")
        
        first_csv = os.path.join(DATA_DIR, csv_files[0])
        log_info(f"üìä –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω —Ñ–∞–π–ª: {first_csv}")
        companies_df = load_file_smart(first_csv)
    
    if COMPANIES_LIMIT > 0:
        log_info(f"‚ö†Ô∏è  –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {COMPANIES_LIMIT} –∫–æ–º–ø–∞–Ω–∏–π")
        companies_df = companies_df.head(COMPANIES_LIMIT)
    
    log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(companies_df)}")
    return companies_df

def load_all_companies_from_data_folder():
    """Load and combine all CSV and Excel company files from data/ directory"""
    data_dir = DATA_DIR  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–ø–∫—É data –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"‚ùå –ü–∞–ø–∫–∞ data –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_dir}")
    
    # Find all CSV and Excel files in data directory
    company_files = []
    company_files.extend(glob.glob(os.path.join(data_dir, "*.csv")))
    company_files.extend(glob.glob(os.path.join(data_dir, "*.xlsx")))
    company_files.extend(glob.glob(os.path.join(data_dir, "*.xls")))
    
    if not company_files:
        raise FileNotFoundError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ CSV/Excel —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ: {data_dir}")
    
    log_info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π: {len(company_files)}")
    for file_path in company_files:
        file_ext = os.path.splitext(file_path)[1].lower()
        log_info(f"   - {os.path.basename(file_path)} ({file_ext.upper()})")
    
    all_companies = []
    
    for file_path in company_files:
        filename = os.path.basename(file_path)
        file_ext = os.path.splitext(file_path)[1].lower()
        log_info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑: {filename} ({file_ext.upper()})")
        
        try:
            df = load_file_smart(file_path)
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {filename}: {len(df)} –∫–æ–º–ø–∞–Ω–∏–π")
            all_companies.append(df)
            
        except Exception as e:
            log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
            continue
    
    if not all_companies:
        raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∫–æ–º–ø–∞–Ω–∏–π")
    
    # Combine all companies into single DataFrame
    combined_companies = pd.concat(all_companies, ignore_index=True)
    
    if COMPANIES_LIMIT > 0:
        log_info(f"‚ö†Ô∏è  –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {COMPANIES_LIMIT} –∫–æ–º–ø–∞–Ω–∏–π")
        combined_companies = combined_companies.head(COMPANIES_LIMIT)
    
    log_info(f"üéØ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(combined_companies)}")
    return combined_companies

def load_all_criteria_files():
    """Load and combine all criteria files from criteria/ directory"""
    criteria_dir = CRITERIA_DIR
    
    if not os.path.exists(criteria_dir):
        raise FileNotFoundError(f"‚ùå –ü–∞–ø–∫–∞ criteria –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {criteria_dir}")
    
    # Find all CSV files in criteria directory
    criteria_files = glob.glob(os.path.join(criteria_dir, "*.csv"))
    
    if not criteria_files:
        raise FileNotFoundError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –≤ –ø–∞–ø–∫–µ: {criteria_dir}")
    
    log_info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {len(criteria_files)}")
    
    all_criteria = []
    
    for file_path in criteria_files:
        filename = os.path.basename(file_path)
        log_info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑: {filename}")
        
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
            
            # Validate required columns
            required_columns = ['Product', 'Target Audience', 'Criteria Type', 'Criteria']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                log_error(f"‚ùå –í —Ñ–∞–π–ª–µ {filename} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
                continue
            
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {filename}: {len(df)} –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤")
            all_criteria.append(df)
            
        except Exception as e:
            log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
            continue
    
    if not all_criteria:
        raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤")
    
    # Combine all criteria into single DataFrame
    combined_criteria = pd.concat(all_criteria, ignore_index=True)
    
    log_info(f"üéØ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {len(combined_criteria)}")
    
    # Show products and types summary
    products = combined_criteria['Product'].unique()
    criteria_types = combined_criteria['Criteria Type'].unique()
    
    log_info(f"üìä –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–¥—É–∫—Ç—ã: {', '.join(products)}")
    log_info(f"üìä –¢–∏–ø—ã –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {', '.join(criteria_types)}")
    
    return combined_criteria

def load_data(companies_file=None, load_all_companies=False):
    """Load all data files - updated for ALL PRODUCTS processing
    
    Args:
        companies_file: –ø—É—Ç—å –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ñ–∞–π–ª—É –∫–æ–º–ø–∞–Ω–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        load_all_companies: –µ—Å–ª–∏ True, –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ CSV —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ data/
    """
    log_info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –í–°–ï–• –ø—Ä–æ–¥—É–∫—Ç–æ–≤")
    
    try:
        # Load companies data based on parameters
        if load_all_companies:
            log_info("üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï —Ñ–∞–π–ª—ã –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ –ø–∞–ø–∫–∏ data/")
            companies_df = load_all_companies_from_data_folder()
        elif companies_file:
            log_info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {companies_file}")
            companies_df = load_companies_data(companies_file)
        else:
            log_info(f"üìä –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏–º CSV —Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ: {DATA_DIR}")
            companies_df = load_companies_data()
        
        log_info(f"‚úÖ –ò—Ç–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(companies_df)}")
        
        # Load all criteria files automatically
        df_criteria = load_all_criteria_files()
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        log_info("üåê –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤...")
        all_general_criteria = df_criteria[df_criteria["Criteria Type"] == "General"]["Criteria"].dropna().unique().tolist()
        log_info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö General –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {len(all_general_criteria)}")
        for i, criteria in enumerate(all_general_criteria, 1):
            log_info(f"   {i}. {criteria}")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –í–°–ï–• –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        products = df_criteria['Product'].unique()
        log_info(f"üè≠ –ë—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –í–°–ï–• –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {', '.join(products)}")
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        all_products_data = {}
        
        for product in products:
            product_criteria = df_criteria[df_criteria["Product"] == product]
            log_info(f"üìã –ù–∞–π–¥–µ–Ω–æ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ {product}: {len(product_criteria)}")
            
            # Extract qualification questions FOR THIS PRODUCT
            qualification_df = product_criteria[product_criteria["Criteria Type"] == "Qualification"].dropna(subset=["Criteria", "Target Audience"])
            qualification_questions = {
                row["Target Audience"]: row["Criteria"]
                for _, row in qualification_df.iterrows()
            }
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è {product}: {len(qualification_questions)}")
            
            # Load mandatory criteria FOR THIS PRODUCT
            mandatory_df = product_criteria[product_criteria["Criteria Type"] == "Mandatory"].dropna(subset=["Criteria", "Target Audience"])
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è {product}: {len(mandatory_df)}")
            
            # Load NTH criteria FOR THIS PRODUCT
            nth_df = product_criteria[product_criteria["Criteria Type"] == "NTH"].dropna(subset=["Criteria", "Target Audience"])
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ NTH –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è {product}: {len(nth_df)}")
            
            all_products_data[product] = {
                "qualification_questions": qualification_questions,
                "mandatory_df": mandatory_df,
                "nth_df": nth_df
            }
        
        log_info("üöÄ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        log_info(f"General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –±—É–¥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫–æ –≤—Å–µ–º –∫–æ–º–ø–∞–Ω–∏—è–º")
        log_info(f"üéØ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {', '.join(products)}")
        
        return {
            "companies": companies_df,
            "general_criteria": all_general_criteria,  # –í–°–ï General –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
            "products_data": all_products_data,  # –î–∞–Ω–Ω—ã–µ –¥–ª—è –í–°–ï–• –ø—Ä–æ–¥—É–∫—Ç–æ–≤
            "products": list(products)  # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        }
        
    except FileNotFoundError as e:
        log_error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
        raise
    except Exception as e:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise 