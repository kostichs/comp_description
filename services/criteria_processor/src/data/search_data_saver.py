"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ markdown —Ñ–∞–π–ª—ã
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ Serper –∏ ScrapingBee –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π markdown —Ñ–∞–π–ª
"""

import os
import json
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
from src.utils.config import OUTPUT_DIR
from src.utils.logging import log_info, log_error, log_debug


class SearchDataSaver:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ markdown —Ñ–∞–π–ª—ã"""
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.search_data_dir = os.path.join(OUTPUT_DIR, session_id, "search_data")
        os.makedirs(self.search_data_dir, exist_ok=True)
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º
        self.company_search_data = {}
    
    def add_serper_data(self, company_name: str, query: str, serper_results: Dict[str, Any]):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Serper –ø–æ–∏—Å–∫–∞ –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏
        
        Args:
            company_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            serper_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –æ—Ç Serper API
        """
        if not company_name or not serper_results:
            return
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if company_name not in self.company_search_data:
            self.company_search_data[company_name] = {
                "company_name": company_name,
                "serper_searches": [],
                "scraped_pages": [],
                "timestamp": datetime.now().isoformat()
            }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ Serper –ø–æ–∏—Å–∫–∞
        serper_entry = {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "results_count": len(serper_results.get("organic", [])),
            "organic_results": serper_results.get("organic", []),
            "knowledge_graph": serper_results.get("knowledgeGraph", {}),
            "answer_box": serper_results.get("answerBox", {}),
            "people_also_ask": serper_results.get("peopleAlsoAsk", []),
            "related_searches": serper_results.get("relatedSearches", [])
        }
        
        self.company_search_data[company_name]["serper_searches"].append(serper_entry)
        log_debug(f"üìä Added Serper data for {company_name}: query='{query}', results={serper_entry['results_count']}")
    
    def add_scrapingbee_data(self, company_name: str, url: str, scraped_content: str, 
                           serper_query: str = "", status_code: int = 200, error: str = None):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ScrapingBee —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏
        
        Args:
            company_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            scraped_content: –°–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            serper_query: –ò—Å—Ö–æ–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            status_code: HTTP —Å—Ç–∞—Ç—É—Å –∫–æ–¥
            error: –û—à–∏–±–∫–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        """
        if not company_name:
            return
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if company_name not in self.company_search_data:
            self.company_search_data[company_name] = {
                "company_name": company_name,
                "serper_searches": [],
                "scraped_pages": [],
                "timestamp": datetime.now().isoformat()
            }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        scraped_entry = {
            "url": url,
            "serper_query": serper_query,
            "timestamp": datetime.now().isoformat(),
            "status_code": status_code,
            "content_length": len(scraped_content) if scraped_content else 0,
            "content": scraped_content[:10000] if scraped_content else "",  # –ü–µ—Ä–≤—ã–µ 10K —Å–∏–º–≤–æ–ª–æ–≤
            "content_preview": scraped_content[:500] if scraped_content else "",  # –ü—Ä–µ–≤—å—é 500 —Å–∏–º–≤–æ–ª–æ–≤
            "error": error
        }
        
        self.company_search_data[company_name]["scraped_pages"].append(scraped_entry)
        log_debug(f"üêù Added ScrapingBee data for {company_name}: url='{url}', content_length={scraped_entry['content_length']}")
    
    def save_company_markdown(self, company_name: str) -> Optional[str]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –≤ markdown —Ñ–∞–π–ª
        
        Args:
            company_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
            
        Returns:
            str: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
        """
        if company_name not in self.company_search_data:
            log_debug(f"No search data found for company: {company_name}")
            return None
        
        try:
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            safe_company_name = re.sub(r'[^a-zA-Z0-9_-]', '_', company_name)
            filename = f"{safe_company_name}_search_data.md"
            filepath = os.path.join(self.search_data_dir, filename)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º markdown –∫–æ–Ω—Ç–µ–Ω—Ç
            markdown_content = self._generate_markdown_content(self.company_search_data[company_name])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            log_info(f"üíæ Saved search data for {company_name} to {filepath}")
            return filepath
            
        except Exception as e:
            log_error(f"‚ùå Error saving search data for {company_name}: {e}")
            return None
    
    def save_all_companies_markdown(self) -> List[str]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö –∫–æ–º–ø–∞–Ω–∏–π –≤ markdown —Ñ–∞–π–ª—ã
        
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
        """
        saved_files = []
        
        for company_name in self.company_search_data.keys():
            filepath = self.save_company_markdown(company_name)
            if filepath:
                saved_files.append(filepath)
        
        log_info(f"üíæ Saved search data for {len(saved_files)} companies to markdown files")
        return saved_files
    
    def _generate_markdown_content(self, company_data: Dict[str, Any]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç markdown –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏
        
        Args:
            company_data: –î–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
            
        Returns:
            str: Markdown –∫–æ–Ω—Ç–µ–Ω—Ç
        """
        company_name = company_data["company_name"]
        timestamp = company_data["timestamp"]
        serper_searches = company_data.get("serper_searches", [])
        scraped_pages = company_data.get("scraped_pages", [])
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞
        content = [f"# Search Data Report for {company_name}"]
        content.append(f"**Generated:** {timestamp}")
        content.append(f"**Session ID:** {self.session_id}")
        content.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–¥–∫—É
        content.append("## Summary")
        content.append(f"* **Total Serper Searches:** {len(serper_searches)}")
        content.append(f"* **Total Scraped Pages:** {len(scraped_pages)}")
        content.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ Serper –ø–æ–∏—Å–∫–æ–≤
        if serper_searches:
            content.append("## Serper Search Results")
            content.append("")
            
            for i, search in enumerate(serper_searches, 1):
                content.append(f"### Search {i}: {search['query']}")
                content.append(f"**Timestamp:** {search['timestamp']}")
                content.append(f"**Results Found:** {search['results_count']}")
                content.append("")
                
                # Organic results
                if search.get("organic_results"):
                    content.append("#### Organic Search Results")
                    for j, result in enumerate(search["organic_results"][:10], 1):  # –ü–µ—Ä–≤—ã–µ 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        title = result.get("title", "No title")
                        link = result.get("link", "No link")
                        snippet = result.get("snippet", "No snippet")
                        
                        content.append(f"**{j}. {title}**")
                        content.append(f"* **URL:** {link}")
                        content.append(f"* **Snippet:** {snippet}")
                        content.append("")
                
                # Knowledge Graph
                if search.get("knowledge_graph"):
                    kg = search["knowledge_graph"]
                    content.append("#### Knowledge Graph")
                    if kg.get("title"):
                        content.append(f"* **Title:** {kg['title']}")
                    if kg.get("type"):
                        content.append(f"* **Type:** {kg['type']}")
                    if kg.get("description"):
                        content.append(f"* **Description:** {kg['description']}")
                    if kg.get("website"):
                        content.append(f"* **Website:** {kg['website']}")
                    content.append("")
                
                # Answer Box
                if search.get("answer_box"):
                    ab = search["answer_box"]
                    content.append("#### Answer Box")
                    if ab.get("answer"):
                        content.append(f"* **Answer:** {ab['answer']}")
                    if ab.get("title"):
                        content.append(f"* **Title:** {ab['title']}")
                    if ab.get("link"):
                        content.append(f"* **Source:** {ab['link']}")
                    content.append("")
                
                # People Also Ask
                if search.get("people_also_ask"):
                    content.append("#### People Also Ask")
                    for paa in search["people_also_ask"][:5]:  # –ü–µ—Ä–≤—ã–µ 5 –≤–æ–ø—Ä–æ—Å–æ–≤
                        if paa.get("question"):
                            content.append(f"* {paa['question']}")
                    content.append("")
                
                # Related Searches
                if search.get("related_searches"):
                    content.append("#### Related Searches")
                    for rs in search["related_searches"][:5]:  # –ü–µ—Ä–≤—ã–µ 5 —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤
                        if rs.get("query"):
                            content.append(f"* {rs['query']}")
                    content.append("")
                
                content.append("---")
                content.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        if scraped_pages:
            content.append("## Scraped Pages Content")
            content.append("")
            
            for i, page in enumerate(scraped_pages, 1):
                content.append(f"### Scraped Page {i}")
                content.append(f"**URL:** {page['url']}")
                content.append(f"**Timestamp:** {page['timestamp']}")
                content.append(f"**Status Code:** {page['status_code']}")
                content.append(f"**Content Length:** {page['content_length']} characters")
                
                if page.get("serper_query"):
                    content.append(f"**Related Search Query:** {page['serper_query']}")
                
                if page.get("error"):
                    content.append(f"**Error:** {page['error']}")
                
                content.append("")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                if page.get("content_preview"):
                    content.append("#### Content Preview (first 500 characters)")
                    content.append("```")
                    content.append(page["content_preview"])
                    content.append("```")
                    content.append("")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–ø–µ—Ä–≤—ã–µ 10K —Å–∏–º–≤–æ–ª–æ–≤)
                if page.get("content") and len(page["content"]) > 500:
                    content.append("#### Full Content (first 10,000 characters)")
                    content.append("```")
                    content.append(page["content"])
                    content.append("```")
                    content.append("")
                
                content.append("---")
                content.append("")
        
        # –î–æ–±–∞–≤–ª—è–µ–º footer
        content.append("---")
        content.append(f"*Report generated by Criteria Processor - Session {self.session_id}*")
        content.append(f"*Generated at: {datetime.now().isoformat()}*")
        
        return "\n".join(content)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
_current_saver: Optional[SearchDataSaver] = None


def initialize_search_data_saver(session_id: str) -> SearchDataSaver:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä SearchDataSaver –¥–ª—è —Å–µ—Å—Å–∏–∏
    
    Args:
        session_id: ID —Å–µ—Å—Å–∏–∏
        
    Returns:
        SearchDataSaver: –≠–∫–∑–µ–º–ø–ª—è—Ä —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—è
    """
    global _current_saver
    _current_saver = SearchDataSaver(session_id)
    log_info(f"üîß Initialized SearchDataSaver for session: {session_id}")
    return _current_saver


def get_current_search_data_saver() -> Optional[SearchDataSaver]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —ç–∫–∑–µ–º–ø–ª—è—Ä SearchDataSaver
    
    Returns:
        SearchDataSaver –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
    """
    return _current_saver


def save_serper_search_data(company_name: str, query: str, serper_results: Dict[str, Any]):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ Serper –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å
    
    Args:
        company_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        serper_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
    """
    if _current_saver:
        _current_saver.add_serper_data(company_name, query, serper_results)


def save_scrapingbee_data(company_name: str, url: str, scraped_content: str, 
                         serper_query: str = "", status_code: int = 200, error: str = None):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ ScrapingBee —á–µ—Ä–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å
    
    Args:
        company_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        scraped_content: –°–∫—Ä–∞–ø–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        serper_query: –ò—Å—Ö–æ–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        status_code: HTTP —Å—Ç–∞—Ç—É—Å –∫–æ–¥
        error: –û—à–∏–±–∫–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
    """
    if _current_saver:
        _current_saver.add_scrapingbee_data(company_name, url, scraped_content, serper_query, status_code, error)


def finalize_search_data_saving() -> List[str]:
    """
    –ó–∞–≤–µ—Ä—à–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ markdown —Ñ–∞–π–ª—ã
    
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
    """
    if _current_saver:
        saved_files = _current_saver.save_all_companies_markdown()
        log_info(f"üéâ Finalized search data saving: {len(saved_files)} files saved")
        return saved_files
    return [] 