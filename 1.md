# План разработки нового пайплайна обработки информации о компаниях

Этот документ описывает пошаговый план создания нового пайплайна, начиная с интеграции существующего `CompanyHomepageFinder` и постепенно добавляя функциональность для поиска LinkedIn профилей, скрейпинга данных и извлечения информации с помощью LLM.

## Общая архитектура

Проект будет состоять из следующих основных частей:
1.  **Изолированные компоненты (классы):** Каждый класс отвечает за конкретную задачу (поиск URL, скрейпинг, взаимодействие с LLM).
2.  **Оркестратор (основной скрипт):** Управляет потоком данных, координирует вызовы компонентов, обрабатывает пакеты компаний асинхронно и сохраняет результаты.
3.  **Конфигурация:** Внешние файлы для настроек (API ключи, параметры LLM, пути).
4.  **Вспомогательные модули:** Для операций ввода/вывода, специфических утилит.

## Этапы разработки

### Часть 1: Основа Оркестратора и Интеграция `CompanyHomepageFinder`

-   [x] **1.1. Создание базовой структуры проекта и Оркестратора**
    -   [x] 1.1.1. Создать новый файл `pipeline_orchestrator.py` (или аналогичный) в директории `src`.
    -   [x] 1.1.2. Определить класс `PipelineOrchestrator`.
        -   [x] 1.1.2.1. Конструктор `__init__` для загрузки конфигураций (пути, API ключи через `dotenv` или файл конфигурации) и инициализации `CompanyHomepageFinder`.
        -   [x] 1.1.2.2. Базовый асинхронный метод `run(company_list_path, output_path)` для оркестрации.
        -   [x] 1.1.2.3. Внутренний асинхронный метод `_process_single_company(company_name, aiohttp_session)` для обработки одной компании (пока только поиск homepage).
    -   [x] 1.1.3. Создать базовую функцию `main()` в `pipeline_orchestrator.py` для запуска оркестратора.
    -   [x] 1.1.4. Реализовать загрузку списка компаний (можно использовать `load_company_names` из `company_homepage_finder.py` или создать новый `data_io.py`).
    -   [x] 1.1.5. Реализовать базовое сохранение результатов (список словарей: `{'name': company_name, 'homepage_url': url, 'homepage_method': method}`) в CSV.
    -   [x] 1.1.6. Настроить базовое асинхронное выполнение с `asyncio.gather` для обработки небольшого тестового набора компаний. Использовать `aiohttp.ClientSession` управляемый оркестратором.
    -   *Ожидаемый результат: Оркестратор может прочитать список компаний, для каждой найти URL домашней страницы с помощью `CompanyHomepageFinder` и сохранить результаты в CSV.*

-   [x] **1.2. Улучшение Оркестратора: Пакетная обработка и логирование**
    -   [x] 1.2.1. Добавить в `PipelineOrchestrator` логику разделения списка компаний на пакеты (батчи).
    -   [x] 1.2.2. Обрабатывать компании пакет за пакетом асинхронно.
    -   [x] 1.2.3. Внедрить базовое логирование (`logging`) для отслеживания процесса, ошибок и статистики по каждому пакету и в целом.
    -   [x] 1.2.4. Добавить инкрементальное сохранение результатов в CSV после обработки каждого пакета.
    -   *Ожидаемый результат: Оркестратор эффективно обрабатывает большой список компаний пакетами, логирует процесс и инкрементально сохраняет результаты.*

### Часть 2: Интеграция Поиска LinkedIn Профилей

-   [ ] **2.1. Создание компонента `LinkedInFinder`**
    -   [ ] 2.1.1. Создать новый файл `linkedin_finder.py`.
    -   [ ] 2.1.2. Определить класс `LinkedInFinder`.
        -   [ ] 2.1.2.1. Конструктор для API ключей (Serper, OpenAI, если нужны).
        -   [ ] 2.1.2.2. Асинхронный метод `find_linkedin_url(company_name, homepage_url=None, context_text=None)` для поиска URL LinkedIn.
            -   Логика может быть похожа на `find_urls_with_serper_async` из старого `pipeline.py` или использовать специализированные запросы.
            -   Рассмотреть возможность использования LLM для валидации/выбора лучшей ссылки LinkedIn.
    -   *Ожидаемый результат: Класс `LinkedInFinder` способен находить URL профиля компании в LinkedIn.*

-   [ ] **2.2. Интеграция `LinkedInFinder` в Оркестратор**
    -   [ ] 2.2.1. Инициализировать `LinkedInFinder` в конструкторе `PipelineOrchestrator`.
    -   [ ] 2.2.2. В методе `_process_single_company` после нахождения homepage, вызывать `linkedin_finder.find_linkedin_url()`.
    -   [ ] 2.2.3. Добавлять найденный URL LinkedIn и метод его обнаружения в результаты.
    -   [ ] 2.2.4. Обновить сохранение в CSV для включения полей LinkedIn.
    -   *Ожидаемый результат: Оркестратор находит URL домашней страницы и URL LinkedIn, сохраняет оба.*

### Часть 3: Скрейпинг Данных

-   [ ] **3.1. Создание компонента `CompanyInfoScraper`**
    -   [ ] 3.1.1. Создать новый файл `info_scraper.py`.
    -   [ ] 3.1.2. Определить класс `CompanyInfoScraper`.
        -   [ ] 3.1.2.1. Конструктор (может принимать ScrapingBee API ключ).
        -   [ ] 3.1.2.2. Асинхронный метод `scrape_text_from_url(url, aiohttp_session, use_scrapingbee=False)` для получения основного текстового контента страницы.
            -   Использовать `aiohttp` для прямых запросов.
            -   Использовать `BeautifulSoup` для извлечения текста (например, из `<body>`, отфильтровывая `<script>`, `<style>`, `<nav>`, `<footer>`).
            -   Предусмотреть возможность использования ScrapingBee для сложных сайтов.
        -   [ ] 3.1.2.3. (Опционально) Асинхронный метод `find_and_scrape_about_page(homepage_url, homepage_html_content, aiohttp_session, ...)` для поиска и скрейпинга страницы "О нас".
        -   [ ] 3.1.2.4. (Опционально) Асинхронный метод для скрейпинга LinkedIn (текст из секции "About").
        -   [ ] 3.1.2.5. (Опционально) Метод для получения сводки из Wikipedia (можно адаптировать из старого `pipeline.py`).
    -   *Ожидаемый результат: Класс `CompanyInfoScraper` способен извлекать текстовый контент с указанных URL.*

-   [ ] **3.2. Интеграция `CompanyInfoScraper` в Оркестратор**
    -   [ ] 3.2.1. Инициализировать `CompanyInfoScraper` в конструкторе `PipelineOrchestrator`.
    -   [ ] 3.2.2. В методе `_process_single_company`, после нахождения URL-ов, вызывать методы скрейпера для получения текстов с домашней страницы, (опционально) LinkedIn, "О нас", Wikipedia.
    -   [ ] 3.2.3. Собирать все полученные тексты для дальнейшей обработки. (Пока не сохраняем их в CSV, только передаем дальше).
    -   *Ожидаемый результат: Оркестратор собирает URL-ы и текстовый контент с этих страниц.*

### Часть 4: Извлечение Данных и Генерация Описаний с LLM

-   [ ] **4.1. Создание/Обновление компонента `DataExtractorLLM`**
    -   [ ] 4.1.1. Создать/адаптировать файл `llm_extractor.py`.
    -   [ ] 4.1.2. Определить класс `DataExtractorLLM`.
        -   [ ] 4.1.2.1. Конструктор для OpenAI API ключа и конфигураций LLM (схемы JSON, параметры моделей).
        -   [ ] 4.1.2.2. Асинхронный метод `extract_structured_data(text_content, company_name, schema_name, sub_schema_dict)` для извлечения данных по предоставленной JSON схеме (адаптировать `extract_data_with_schema` из старого `pipeline.py`).
        -   [ ] 4.1.2.3. Асинхронный метод `generate_text_summary(structured_data, company_name)` для генерации текстового описания на основе извлеченных структурированных данных (адаптировать `generate_text_summary_from_json_async`).
        -   [ ] 4.1.2.4. (Опционально) Асинхронный метод `run_deep_search_queries(company_name, context_text, specific_queries_list)` для выполнения "глубоких" запросов к LLM (адаптировать `query_llm_for_deep_info`).
    -   *Ожидаемый результат: Класс `DataExtractorLLM` способен извлекать структурированные данные из текста и генерировать на их основе описания.*

-   [ ] **4.2. Интеграция `DataExtractorLLM` в Оркестратор**
    -   [ ] 4.2.1. Инициализировать `DataExtractorLLM` в конструкторе `PipelineOrchestrator`.
    -   [ ] 4.2.2. В методе `_process_single_company`, после сбора всех текстов:
        -   [ ] 4.2.2.1. (Опционально) Выполнить "глубокий поиск" и добавить его результаты к общему тексту.
        -   [ ] 4.2.2.2. Итеративно вызывать `extract_structured_data` для различных схем (Basic, Product, Market и т.д.), объединяя результаты в один JSON-объект.
        -   [ ] 4.2.2.3. Вызывать `generate_text_summary` для создания итогового текстового описания.
    -   [ ] 4.2.3. Добавлять финальное текстовое описание (или полный JSON, если описание не удалось) в результаты.
    -   [ ] 4.2.4. Обновить сохранение в CSV для включения поля описания.
    -   [ ] 4.2.5. Реализовать сохранение полного извлеченного JSON для каждой компании в отдельный `.json` файл (аналогично старому пайплайну).
    -   *Ожидаемый результат: Оркестратор собирает информацию, извлекает структурированные данные, генерирует описание и сохраняет все релевантные результаты.*

### Часть 5: Финализация и Улучшения

-   [ ] **5.1. Конфигурация и Ввод/Вывод**
    -   [ ] 5.1.1. Создать/обновить `config.py` для управления всеми конфигурациями (API ключи, параметры LLM, пути, настройки батчей и таймаутов). Загружать API ключи из `.env`.
    -   [ ] 5.1.2. Создать/обновить `data_io.py` для всех операций чтения/записи файлов (загрузка компаний из разных форматов, сохранение CSV, JSON).
    -   [ ] 5.1.3. Использовать эти модули в Оркестраторе и компонентах.

-   [ ] **5.2. Обработка ошибок и отказоустойчивость**
    -   [ ] 5.2.1. Улучшить обработку ошибок на уровне каждого компонента и в Оркестраторе.
    -   [ ] 5.2.2. Добавить логику повторных попыток (retries) для сетевых запросов и вызовов API с использованием экспоненциальной задержки.
    -   [ ] 5.2.3. Обеспечить, чтобы ошибка при обработке одной компании не останавливала весь пайплайн.

-   [ ] **5.3. Оптимизация и Тестирование**
    -   [ ] 5.3.1. Провести тестирование на различных наборах данных.
    -   [ ] 5.3.2. Проанализировать производительность, выявить узкие места.
    -   [ ] 5.3.3. (Опционально) Добавить юнит-тесты для ключевых компонентов.

-   [ ] **5.4. Документация**
    -   [ ] 5.4.1. Написать комментарии и docstrings для всех классов и методов.
    -   [ ] 5.4.2. Обновить `README.md` с инструкциями по настройке и запуску нового пайплайна.

Этот план является гибким и может корректироваться по мере продвижения. Главная цель - создать надежный, модульный и эффективный пайплайн. 