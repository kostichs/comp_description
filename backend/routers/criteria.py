"""
–†–æ—É—Ç–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∫–æ–º–ø–∞–Ω–∏–π (–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å)
"""

import sys
import os
import time
import logging
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import asyncio
import aiofiles
import pandas as pd
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, BackgroundTasks
from starlette.background import BackgroundTask
from fastapi.responses import FileResponse, JSONResponse

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ criteria_processor –≤ sys.path –°–†–ê–ó–£
CRITERIA_PROCESSOR_PATH = Path(__file__).parent.parent.parent / "services" / "criteria_processor"
if str(CRITERIA_PROCESSOR_PATH) not in sys.path:
    sys.path.insert(0, str(CRITERIA_PROCESSOR_PATH))

# –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –ø–∞–ø–∫–µ criteria_processor
CRITERIA_SRC_PATH = CRITERIA_PROCESSOR_PATH / "src"
if str(CRITERIA_SRC_PATH) not in sys.path:
    sys.path.insert(0, str(CRITERIA_SRC_PATH))

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π –ø—É—Ç—å criteria_processor –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
if str(CRITERIA_PROCESSOR_PATH) not in sys.path:
    sys.path.insert(0, str(CRITERIA_PROCESSOR_PATH))

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏ –∏–∑ data_io.py
from src.data_io import SESSIONS_DIR, SESSIONS_METADATA_FILE

def run_criteria_processor(input_file_path: str, load_all_companies: bool = False, session_id: str = None, use_deep_analysis: bool = False, use_parallel: bool = True, max_concurrent: int = 12, selected_products: List[str] = None, selected_criteria_files: List[str] = None, write_to_hubspot_criteria: bool = False):
    """–ó–∞–ø—É—Å–∫–∞–µ–º criteria_processor –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å"""
    import subprocess
    import shutil
    import os
    
    try:
        if load_all_companies:
            cmd = [
                "python", 
                str(CRITERIA_PROCESSOR_PATH / "main.py"),
                "--all-files"
            ]
            if session_id:
                cmd.extend(["--session-id", session_id])
        else:
            # –û—á–∏—â–∞–µ–º –ø–∞–ø–∫—É data –æ—Ç —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø–µ—Ä–µ–¥ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–≥–æ
            data_dir = CRITERIA_PROCESSOR_PATH / "data"
            data_dir.mkdir(exist_ok=True)
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ CSV —Ñ–∞–π–ª—ã –∏–∑ data –ø–∞–ø–∫–∏
            for old_file in data_dir.glob("*.csv"):
                old_file.unlink()
                logger.info(f"Removed old file: {old_file}")
            
            # –ö–æ–ø–∏—Ä—É–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª –≤ data –ø–∞–ø–∫—É criteria_processor
            source_path = Path(input_file_path)
            target_path = data_dir / source_path.name
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—É—Ç–∏
            logger.info(f"Copying file: {source_path} -> {target_path}")
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª
            shutil.copy2(source_path, target_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª —Å–∫–æ–ø–∏—Ä–æ–≤–∞–ª—Å—è
            if target_path.exists():
                logger.info(f"File copied successfully: {target_path}")
            else:
                logger.error(f"File copy failed: {target_path}")
                return {"status": "error", "error": "Failed to copy file to data directory"}
            
            cmd = [
                "python", 
                str(CRITERIA_PROCESSOR_PATH / "main.py"),
                "--file", f"data/{target_path.name}",  # –ü—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ criteria_processor
                "--session-id", session_id  # –ü–µ—Ä–µ–¥–∞–µ–º session_id –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –ø–∞–ø–∫–∏
            ]
        
        if use_deep_analysis:
            cmd.append("--deep-analysis")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if use_parallel:
            cmd.append("--parallel")
            cmd.extend(["--max-concurrent", str(max_concurrent)])
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        if selected_criteria_files:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –≤ –ø—Ä–æ–¥—É–∫—Ç—ã
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã —Ç–æ–ª—å–∫–æ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            criteria_dir = CRITERIA_PROCESSOR_PATH / "criteria"
            selected_products_from_files = []
            
            for filename in selected_criteria_files:
                file_path = criteria_dir / filename
                if file_path.exists():
                    try:
                        if file_path.suffix.lower() == '.csv':
                            import pandas as pd
                            df = pd.read_csv(file_path)
                            if 'Product' in df.columns:
                                file_products = df['Product'].unique().tolist()
                                file_products = [p for p in file_products if pd.notna(p) and str(p).strip()]
                                selected_products_from_files.extend(file_products)
                                logger.info(f"From file {filename} extracted products: {file_products}")
                    except Exception as e:
                        logger.error(f"Error reading criteria file {filename}: {e}")
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            selected_products_from_files = list(set(selected_products_from_files))
            logger.info(f"Final products from selected files: {selected_products_from_files}")
            
            if selected_products_from_files:
                cmd.append("--selected-products")
                cmd.append(",".join(selected_products_from_files))
                logger.info(f"Adding products from selected files to command: {selected_products_from_files}")
            
        # Fallback –∫ —Å—Ç–∞—Ä–æ–π –ª–æ–≥–∏–∫–µ —Å selected_products
        elif selected_products:
            cmd.append("--selected-products")
            cmd.append(",".join(selected_products))
            logger.info(f"Adding selected products to command: {selected_products}")
        else:
            logger.info("No selected products or files specified - will process all products")
        
        # Add Circuit Breaker support (enabled by default, can be disabled via env var)
        import os
        if os.getenv('DISABLE_CIRCUIT_BREAKER', 'false').lower() == 'true':
            cmd.append("--disable-circuit-breaker")
            logger.info("Circuit Breaker –æ—Ç–∫–ª—é—á–µ–Ω —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        
        # Add HubSpot integration flag
        logger.info(f"üîç HUBSPOT –ü–ê–†–ê–ú–ï–¢–† –ü–†–û–í–ï–†–ö–ê:")
        logger.info(f"   üîó write_to_hubspot_criteria = {write_to_hubspot_criteria}")
        logger.info(f"   üìù –¢–∏–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {type(write_to_hubspot_criteria)}")
        
        if write_to_hubspot_criteria:
            cmd.append("--write-to-hubspot-criteria")
            logger.info("‚úÖ HubSpot –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞ - –¥–æ–±–∞–≤–ª–µ–Ω —Ñ–ª–∞–≥ --write-to-hubspot-criteria")
        else:
            logger.info("üìù HubSpot –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ - —Ñ–ª–∞–≥ –ù–ï –¥–æ–±–∞–≤–ª–µ–Ω")
        
        # Log the full command for debugging –ü–û–°–õ–ï –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö —Ñ–ª–∞–≥–æ–≤
        logger.info(f"üöÄ –ò–¢–û–ì–û–í–ê–Ø –ö–û–ú–ê–ù–î–ê: {' '.join(cmd)}")
        
        # –ú–µ–Ω—è–µ–º —Ä–∞–±–æ—á—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –Ω–∞ criteria_processor
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è Windows
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['PYTHONLEGACYWINDOWSSTDIO'] = '0'
        
        result = subprocess.run(
            cmd, 
            cwd=str(CRITERIA_PROCESSOR_PATH),
            capture_output=True, 
            text=True, 
            encoding='utf-8',
            env=env,
            timeout=None  # –ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π - –ø—É—Å—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ
        )
        
        if result.returncode == 0:
            logger.info(f"Criteria processor completed successfully")
            return {"status": "success", "output": result.stdout}
        else:
            logger.error(f"Criteria processor failed: {result.stderr}")
            return {"status": "error", "error": result.stderr}
            
    except subprocess.TimeoutExpired:
        logger.error("Criteria processor timed out")
        return {"status": "error", "error": "Process timed out"}
    except Exception as e:
        logger.error(f"Error running criteria processor: {e}")
        return {"status": "error", "error": str(e)}

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/criteria", tags=["Criteria Analysis"])

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
# –§–æ—Ä–º–∞—Ç: {"session_id": {"status": "processing|completed|failed", "result_path": "...", ...}}
criteria_sessions: Dict[str, Dict[str, Any]] = {}

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
criteria_tasks: Dict[str, asyncio.Task] = {}

def generate_criteria_session_id() -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID —Å–µ—Å—Å–∏–∏ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º crit_"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"crit_{timestamp}"

# –§—É–Ω–∫—Ü–∏—è cleanup_old_backups —É–¥–∞–ª–µ–Ω–∞ - backup —Ñ–∞–π–ª—ã –±–æ–ª—å—à–µ –Ω–µ —Å–æ–∑–¥–∞—é—Ç—Å—è

async def run_criteria_analysis_task(
    session_id: str,
    input_file_path: Path,
    load_all_companies: bool = False,
    use_deep_analysis: bool = False,
    use_parallel: bool = True,
    max_concurrent: int = 12,
    selected_products: List[str] = None,
    selected_criteria_files: List[str] = None,
    write_to_hubspot_criteria: bool = False
):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    log_info = None
    log_error = None
    
    try:
        logger.info(f"Starting criteria analysis for session {session_id}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        criteria_sessions[session_id]["status"] = "processing"
        criteria_sessions[session_id]["start_time"] = datetime.now().isoformat()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None,
            run_criteria_processor,
            str(input_file_path),
            load_all_companies,
            session_id,
            use_deep_analysis,
            use_parallel,
            max_concurrent,
            selected_products,
            selected_criteria_files,
            write_to_hubspot_criteria
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
        if result["status"] == "success":
            criteria_sessions[session_id].update({
                "status": "completed",
                "end_time": datetime.now().isoformat(),
                "output": result["output"]
            })
            logger.info(f"Completed criteria analysis for session {session_id}")
        else:
            criteria_sessions[session_id].update({
                "status": "failed",
                "error": result["error"],
                "end_time": datetime.now().isoformat()
            })
            logger.error(f"Criteria analysis failed for session {session_id}: {result['error']}")
        
    except Exception as e:
        error_msg = f"Error in criteria analysis for session {session_id}: {e}"
        logger.error(error_msg)
        
        criteria_sessions[session_id].update({
            "status": "failed",
            "error": str(e),
            "end_time": datetime.now().isoformat()
        })
    finally:
        # –£–¥–∞–ª—è–µ–º –∑–∞–¥–∞—á—É –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö
        if session_id in criteria_tasks:
            del criteria_tasks[session_id]

@router.post("/analyze")
async def create_criteria_analysis(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    load_all_companies: bool = Form(False),
    use_deep_analysis: bool = Form(False),
    use_parallel: bool = Form(True),
    max_concurrent: int = Form(12),
    selected_products: str = Form("[]"),  # Backward compatibility
    selected_criteria_files: str = Form("[]"),  # NEW: JSON string of selected criteria files
    write_to_hubspot_criteria: bool = Form(False)  # NEW: HubSpot integration flag
):
    """
    –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
    
    - **file**: CSV —Ñ–∞–π–ª —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –∫–æ–º–ø–∞–Ω–∏–π 
    - **load_all_companies**: –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª–∏ –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ data
    - **use_deep_analysis**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
    - **use_parallel**: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–ø–∞–Ω–∏–π (–≤–∫–ª—é—á–µ–Ω–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    - **max_concurrent**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 12)
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
        if not file.filename.endswith(('.csv', '.xlsx', '.xls')):
            raise HTTPException(
                status_code=400, 
                detail="Unsupported file format. Use CSV or Excel files."
            )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID —Å–µ—Å—Å–∏–∏
        session_id = generate_criteria_session_id()
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è —ç—Ç–æ–π —Å–µ—Å—Å–∏–∏
        session_dir = Path("temp") / "criteria_analysis" / session_id
        session_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏
        input_file_path = session_dir / file.filename
        async with aiofiles.open(input_file_path, 'wb') as f:
            content = await file.read()
            await f.write(content)
        
        # Log basic file info
        logger.info(f"Uploaded file: {file.filename} ({len(content)} bytes)")
        
        # Parse selected criteria files or fallback to selected products
        import json
        try:
            selected_criteria_files_list = json.loads(selected_criteria_files) if selected_criteria_files != "[]" else []
        except json.JSONDecodeError:
            selected_criteria_files_list = []
        
        # Fallback to selected_products for backward compatibility
        if not selected_criteria_files_list:
            try:
                selected_products_list = json.loads(selected_products) if selected_products else []
            except json.JSONDecodeError:
                selected_products_list = []
        else:
            selected_products_list = []
        
        logger.info(f"Selected criteria files for session analysis: {selected_criteria_files_list}")
        logger.info(f"Selected products (fallback) for session analysis: {selected_products_list}")

        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–µ—Å—Å–∏–∏
        criteria_sessions[session_id] = {
            "session_id": session_id,
            "status": "created",
            "created_time": datetime.now().isoformat(),
            "filename": file.filename,
            "file_size": len(content),
            "input_file_path": str(input_file_path),
            "load_all_companies": load_all_companies,
            "use_deep_analysis": use_deep_analysis,
            "use_parallel": use_parallel,
            "max_concurrent": max_concurrent,
            "selected_products": selected_products_list,
            "selected_criteria_files": selected_criteria_files_list
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ
        task = asyncio.create_task(
            run_criteria_analysis_task(
                session_id, 
                input_file_path, 
                load_all_companies,
                use_deep_analysis,
                use_parallel,
                max_concurrent,
                selected_products_list,
                selected_criteria_files_list,
                write_to_hubspot_criteria
            )
        )
        criteria_tasks[session_id] = task
        
        logger.info(f"Created criteria analysis session: {session_id}")
        
        return {
            "session_id": session_id,
            "status": "created",
            "message": "Criteria analysis session created and started"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating criteria analysis session: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create analysis session: {str(e)}")

@router.post("/analyze_from_session")
async def create_criteria_analysis_from_session(
    background_tasks: BackgroundTasks,
    session_id: str = Form(...),
    load_all_companies: bool = Form(False),
    use_deep_analysis: bool = Form(False),
    use_parallel: bool = Form(True),
    max_concurrent: int = Form(12),
    selected_products: str = Form("[]"),  # Backward compatibility
    selected_criteria_files: str = Form("[]"),  # NEW: JSON string of selected criteria files
    write_to_hubspot_criteria: bool = Form(False)  # NEW: HubSpot integration flag
):
    """
    –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–µ—Å—Å–∏–∏
    
    - **session_id**: ID —Å–µ—Å—Å–∏–∏ –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –≤–∑—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    - **load_all_companies**: –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª–∏ –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ data
    - **use_deep_analysis**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
    - **use_parallel**: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–ø–∞–Ω–∏–π (–≤–∫–ª—é—á–µ–Ω–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    - **max_concurrent**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 12)
    - **selected_products**: JSON string –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    """
    try:
        # Parse selected criteria files or fallback to selected products
        import json
        try:
            selected_criteria_files_list = json.loads(selected_criteria_files) if selected_criteria_files != "[]" else []
        except json.JSONDecodeError:
            selected_criteria_files_list = []
        
        # Fallback to selected_products for backward compatibility
        if not selected_criteria_files_list:
            try:
                selected_products_list = json.loads(selected_products) if selected_products else []
            except json.JSONDecodeError:
                selected_products_list = []
        else:
            selected_products_list = []
        
        logger.info(f"Selected criteria files for session analysis: {selected_criteria_files_list}")
        logger.info(f"Selected products (fallback) for session analysis: {selected_products_list}")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–µ—Å—Å–∏–∏
        import json
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–µ—Å—Å–∏–π —á–µ—Ä–µ–∑ data_io.py (—Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –ª–æ–∫–∞–ª—å–Ω–æ –∏ –≤ Docker)
        metadata = []
        if SESSIONS_METADATA_FILE.exists():
            try:
                with open(SESSIONS_METADATA_FILE, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    if not isinstance(metadata, list):
                        metadata = []
            except Exception as e:
                logger.error(f"Error loading session metadata: {e}")
                metadata = []
        
        source_session = next((m for m in metadata if m.get("session_id") == session_id), None)
        
        if not source_session:
            raise HTTPException(status_code=404, detail=f"Source session {session_id} not found")
        
        if source_session.get("status") != "completed":
            raise HTTPException(status_code=400, detail=f"Source session {session_id} is not completed")
        
        # –ò—â–µ–º CSV —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤ –ø–∞–ø–∫–µ —Å–µ—Å—Å–∏–∏
        import glob
        session_dir = SESSIONS_DIR / session_id  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å
        
        if not session_dir.exists():
            raise HTTPException(status_code=404, detail=f"Session directory not found for session {session_id}")
        
        # –ò—â–µ–º CSV —Ñ–∞–π–ª—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        csv_files = list(session_dir.glob("*results*.csv"))
        if not csv_files:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ª—é–±–æ–π CSV —Ñ–∞–π–ª
            csv_files = list(session_dir.glob("*.csv"))
        
        if not csv_files:
            raise HTTPException(status_code=404, detail=f"No CSV results file found for session {session_id}")
        
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Ñ–∞–π–ª
        source_file_path = max(csv_files, key=lambda p: p.stat().st_ctime)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        new_session_id = generate_criteria_session_id()
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏
        session_dir = Path("temp") / "criteria_analysis" / new_session_id
        session_dir.mkdir(parents=True, exist_ok=True)
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–∞–ø–∫—É –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏
        input_file_path = session_dir / "source_results.csv"
        shutil.copy2(source_file_path, input_file_path)
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏
        criteria_sessions[new_session_id] = {
            "session_id": new_session_id,
            "status": "created",
            "created_time": datetime.now().isoformat(),
            "filename": f"Results from session {session_id}",
            "source_session_id": session_id,
            "file_size": os.path.getsize(input_file_path),
            "input_file_path": str(input_file_path),
            "load_all_companies": load_all_companies,
            "use_deep_analysis": use_deep_analysis,
            "use_parallel": use_parallel,
            "max_concurrent": max_concurrent,
            "selected_products": selected_products_list,
            "selected_criteria_files": selected_criteria_files_list
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–µ
        task = asyncio.create_task(
            run_criteria_analysis_task(
                new_session_id, 
                input_file_path, 
                load_all_companies,
                use_deep_analysis,
                use_parallel,
                max_concurrent,
                selected_products_list,
                selected_criteria_files_list,
                write_to_hubspot_criteria
            )
        )
        criteria_tasks[new_session_id] = task
        
        logger.info(f"Created criteria analysis session from existing session: {new_session_id} (source: {session_id})")
        
        return {
            "session_id": new_session_id,
            "status": "created",
            "source_session_id": session_id,
            "message": f"Criteria analysis session created using results from session {session_id}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating criteria analysis session from existing session: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create analysis session: {str(e)}")

@router.get("/sessions")
async def get_criteria_sessions():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–µ—Å—Å–∏–π –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    return list(criteria_sessions.values())

@router.get("/sessions/{session_id}")
async def get_criteria_session(session_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å–µ—Å—Å–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    if session_id not in criteria_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    
    return criteria_sessions[session_id]

@router.get("/sessions/{session_id}/status")
async def get_criteria_session_status(session_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—É—Å —Å–µ—Å—Å–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    if session_id not in criteria_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session_data = criteria_sessions[session_id]
    return {
        "session_id": session_id,
        "status": session_data["status"],
        "progress": "In progress..." if session_data["status"] == "processing" else "Complete"
    }

@router.get("/sessions/{session_id}/results")
async def get_criteria_session_results(session_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    if session_id not in criteria_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session_data = criteria_sessions[session_id]
    
    if session_data["status"] != "completed":
        raise HTTPException(
            status_code=400, 
            detail=f"Analysis not completed. Current status: {session_data['status']}"
        )
    
    # –ò—â–µ–º —Ñ–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–∞–ø–∫–µ —Å–µ—Å—Å–∏–∏
    session_results_dir = CRITERIA_PROCESSOR_PATH / "output" / session_id
    
    logger.info(f"Looking for results in: {session_results_dir}")
    
    if not session_results_dir.exists():
        logger.error(f"Session results directory not found: {session_results_dir}")
        return {
            "session_id": session_id, 
            "status": "completed",
            "results_count": 0,
            "results": [],
            "message": f"Session results directory not found: {session_results_dir}"
        }
    
    result_files = []
    
    # –ü–æ–∏—Å–∫ CSV –∏ JSON —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤ –ø–∞–ø–∫–µ —Å–µ—Å—Å–∏–∏
    for pattern in ["*.csv", "*.json"]:
        found_files = list(session_results_dir.glob(pattern))
        result_files.extend(found_files)
        logger.info(f"Found {len(found_files)} files with pattern {pattern}: {[f.name for f in found_files]}")
    
    logger.info(f"Total result files found: {len(result_files)}")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ —Ñ–∞–π–ª—ã, –æ—Ç–¥–∞–≤–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–∞–π–ª–∞–º —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    if result_files:
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–∞–π–ª–∞–º —Å "results" –∏–ª–∏ "analysis" –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        results_files = [f for f in result_files if any(keyword in f.name.lower() for keyword in ['results', 'analysis'])]
        
        if results_files:
            # –ò–∑ —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ—Ä–µ–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π
            latest_file = max(results_files, key=lambda f: f.stat().st_mtime)
            logger.info(f"Selected results file: {latest_file.name} from {len(results_files)} result files")
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏, –±–µ—Ä–µ–º —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π –∏–∑ –≤—Å–µ—Ö
            latest_file = max(result_files, key=lambda f: f.stat().st_mtime)
            logger.info(f"No dedicated results files found, selected: {latest_file.name}")
        
        try:
            if latest_file.suffix == '.csv':
                # –ß–∏—Ç–∞–µ–º CSV –∫–∞–∫ DataFrame –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON
                df = pd.read_csv(latest_file)
                # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                import numpy as np
                df = df.replace([float('inf'), float('-inf'), np.inf, -np.inf], None)
                df = df.where(pd.notnull(df), None)
                
                # –ü–∞—Ä—Å–∏–º JSON –≤ –∫–æ–ª–æ–Ω–∫–µ All_Results
                if 'All_Results' in df.columns:
                    import json
                    def parse_json_column(value):
                        if pd.isna(value) or value is None:
                            return None
                        try:
                            if isinstance(value, str):
                                return json.loads(value)
                            return value
                        except (json.JSONDecodeError, TypeError):
                            return value
                    
                    df['All_Results'] = df['All_Results'].apply(parse_json_column)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ records –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞–µ–º
                records = df.to_dict('records')
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏
                def clean_data_recursive(obj):
                    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ—á–∏—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö float –∑–Ω–∞—á–µ–Ω–∏–π"""
                    if isinstance(obj, dict):
                        return {k: clean_data_recursive(v) for k, v in obj.items()}
                    elif isinstance(obj, list):
                        return [clean_data_recursive(item) for item in obj]
                    elif isinstance(obj, float):
                        if np.isnan(obj) or np.isinf(obj):
                            return None
                        return obj
                    elif pd.isna(obj):
                        return None
                    else:
                        return obj
                
                cleaned_records = []
                for record in records:
                    cleaned_record = clean_data_recursive(record)
                    cleaned_records.append(cleaned_record)
                
                results = cleaned_records
            else:
                # –ß–∏—Ç–∞–µ–º JSON —Ñ–∞–π–ª
                import json
                with open(latest_file, 'r', encoding='utf-8') as f:
                    results = json.load(f)
            
            logger.info(f"Results loaded successfully: {len(results) if isinstance(results, list) else 1} records from {latest_file.name}")
            logger.info(f"Sample data (first record): {results[0] if isinstance(results, list) and len(results) > 0 else 'No records'}")
            
            return {
                "session_id": session_id,
                "status": "completed",
                "results_count": len(results) if isinstance(results, list) else 1,
                "results": results,
                "result_file": str(latest_file)
            }
            
        except Exception as e:
            logger.error(f"Error reading results file {latest_file}: {e}")
            logger.error(f"File details: {latest_file.stat()}")
            raise HTTPException(status_code=500, detail=f"Failed to read results: {str(e)}")
    
    else:
        logger.error(f"No result files found in {session_results_dir}")
        logger.error(f"Directory contents: {list(session_results_dir.iterdir()) if session_results_dir.exists() else 'Directory does not exist'}")
        return {
            "session_id": session_id, 
            "status": "completed",
            "results_count": 0,
            "results": [],
            "message": "No result files found"
        }

@router.get("/sessions/{session_id}/download")
async def download_criteria_results(session_id: str):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç CSV —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å–µ—Å—Å–∏–∏."""
    if session_id not in criteria_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
        
    session_info = criteria_sessions[session_id]
    if session_info["status"] != "completed":
        raise HTTPException(status_code=400, detail="Analysis is not completed yet")
        
    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–Ω—É—Ç—Ä–∏ –ø–∞–ø–∫–∏ output –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞
    # –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º session_id, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–∞–π–ª
    output_dir = CRITERIA_PROCESSOR_PATH / "output" / session_id
    result_files = list(output_dir.glob("*.csv"))
    
    if not result_files:
        raise HTTPException(status_code=404, detail="Result file not found in session directory")
        
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π CSV —Ñ–∞–π–ª
    result_file_path = result_files[0]
    
    return FileResponse(
        path=result_file_path,
        filename=f"criteria_analysis_{session_id}.csv",
        media_type="text/csv"
    )

@router.get("/sessions/{session_id}/scrapingbee_logs")
async def download_scrapingbee_logs(session_id: str):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –µ–¥–∏–Ω—ã–π, —á–∏—Ç–∞–µ–º—ã–π —Ñ–∞–π–ª .log —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ ScrapingBee."""
    if session_id not in criteria_sessions:
        raise HTTPException(status_code=404, detail="Session not found")

    session_info = criteria_sessions[session_id]
    if session_info["status"] != "completed":
        raise HTTPException(status_code=400, detail="Analysis is not completed yet")

    log_file_path = CRITERIA_PROCESSOR_PATH / "output" / session_id / "scrapingbee_logs" / "scrapingbee_session.log"

    if not log_file_path.is_file():
        raise HTTPException(status_code=404, detail="No ScrapingBee logs found for this session.")

    return FileResponse(
        path=log_file_path,
        filename=f"scrapingbee_logs_{session_id}.log",
        media_type="text/plain"
    )

@router.post("/sessions/{session_id}/cancel")
async def cancel_criteria_analysis(session_id: str):
    """–û—Ç–º–µ–Ω—è–µ—Ç —Ç–µ–∫—É—â—É—é –∑–∞–¥–∞—á—É –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤."""
    if session_id not in criteria_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    
    # –û—Ç–º–µ–Ω—è–µ–º –∑–∞–¥–∞—á—É –µ—Å–ª–∏ –æ–Ω–∞ –∞–∫—Ç–∏–≤–Ω–∞
    if session_id in criteria_tasks:
        criteria_tasks[session_id].cancel()
        del criteria_tasks[session_id]
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
    criteria_sessions[session_id].update({
        "status": "cancelled",
        "end_time": datetime.now().isoformat()
    })
    
    return {
        "session_id": session_id,
        "status": "cancelled",
        "message": "Analysis cancelled successfully"
    }

@router.get("/files")
async def get_criteria_files():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö"""
    criteria_dir = CRITERIA_PROCESSOR_PATH / "criteria"
    
    if not criteria_dir.exists():
        return {"files": [], "products": []}
    
    files = []
    all_products = set()
    
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∫–∞–∫ CSV, —Ç–∞–∫ –∏ XLSX —Ñ–∞–π–ª—ã
    for pattern in ["*.csv", "*.xlsx"]:
        for file_path in criteria_dir.glob(pattern):
            # –ò—Å–∫–ª—é—á–∞–µ–º backup —Ñ–∞–π–ª—ã –∏–∑ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –æ–Ω–∏ –æ—Å—Ç–∞–ª–∏—Å—å –æ—Ç —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π)
            if ".backup_" in file_path.name or ".deleted_" in file_path.name:
                continue
            try:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                if file_path.suffix.lower() == '.csv':
                    df = pd.read_csv(file_path)
                elif file_path.suffix.lower() == '.xlsx':
                    df = pd.read_excel(file_path)
                else:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã
                
                # –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–£–°–¢–´–• –°–¢–†–û–ö –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞
                # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
                main_columns = ['Product', 'Criteria', 'Target Audience']
                existing_columns = [col for col in main_columns if col in df.columns]
                
                if existing_columns:
                    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –í–°–ï –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—É—Å—Ç—ã–µ
                    df_filtered = df.dropna(subset=existing_columns, how='all')
                    df_filtered = df_filtered[df_filtered[existing_columns].ne('').any(axis=1)]
                    actual_rows = len(df_filtered)
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫, —Å—á–∏—Ç–∞–µ–º –≤—Å–µ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                    actual_rows = len(df.dropna(how='all'))
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞
                file_products = []
                if 'Product' in df.columns:
                    file_products = df['Product'].unique().tolist()
                    # –£–±–∏—Ä–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
                    file_products = [p for p in file_products if pd.notna(p) and str(p).strip()]
                    all_products.update(file_products)
                
                files.append({
                    "filename": file_path.name,
                    "full_path": str(file_path),
                    "size": file_path.stat().st_size,
                    "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                    "rows_preview": min(5, actual_rows),  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 5 —Å—Ç—Ä–æ–∫ –¥–ª—è preview
                    "total_rows": actual_rows,  # –†–ï–ê–õ–¨–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ —Å –¥–∞–Ω–Ω—ã–º–∏
                    "columns": list(df.columns) if not df.empty else [],
                    "products": file_products  # –ü—Ä–æ–¥—É–∫—Ç—ã –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ
                })
            except Exception as e:
                logger.error(f"Error reading criteria file {file_path}: {e}")
                files.append({
                    "filename": file_path.name,
                    "full_path": str(file_path),
                    "size": file_path.stat().st_size,
                    "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                    "error": str(e),
                    "products": []
                })
    
    return {
        "files": files,
        "products": sorted(list(all_products))  # –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã
    }

@router.get("/files/{filename}")
async def get_criteria_file_content(filename: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    criteria_dir = CRITERIA_PROCESSOR_PATH / "criteria"
    file_path = criteria_dir / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Criteria file not found")
    
    if not file_path.suffix.lower() == '.csv':
        raise HTTPException(status_code=400, detail="Only CSV files are supported")
    
    try:
        df = pd.read_csv(file_path)
        
        # –ó–∞–º–µ–Ω—è–µ–º NaN –∏ Infinity –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ None/null –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        df = df.replace([float('inf'), float('-inf')], None)
        df = df.where(pd.notnull(df), None)
        
        return {
            "filename": filename,
            "columns": list(df.columns),
            "data": df.to_dict('records'),
            "total_rows": len(df),
            "file_info": {
                "size": file_path.stat().st_size,
                "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
            }
        }
    except Exception as e:
        logger.error(f"Error reading criteria file {filename}: {e}")
        raise HTTPException(status_code=500, detail=f"Error reading file: {str(e)}")

@router.put("/files/{filename}")
async def update_criteria_file(filename: str, file_data: dict):
    """–û–±–Ω–æ–≤–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    criteria_dir = CRITERIA_PROCESSOR_PATH / "criteria"
    file_path = criteria_dir / filename
    
    if not filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="Only CSV files are supported")
    
    try:
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        if "data" not in file_data or "columns" not in file_data:
            raise HTTPException(status_code=400, detail="Invalid data format. Expected 'data' and 'columns' fields")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df = pd.DataFrame(file_data["data"], columns=file_data["columns"])
        
        # –ù–ï —Å–æ–∑–¥–∞–µ–º backup —Ñ–∞–π–ª—ã - –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏—Ö –≤—Å–µ —Ä–∞–≤–Ω–æ –Ω–µ —Å–º–æ–≥—É—Ç –¥–æ—Å—Ç–∞—Ç—å
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª
        df.to_csv(file_path, index=False)
        
        logger.info(f"Updated criteria file: {filename}")
        
        return {
            "message": "File updated successfully",
            "filename": filename,
            "rows_saved": len(df),
            "timestamp": datetime.now().isoformat()
        }
        
    except pd.errors.EmptyDataError:
        raise HTTPException(status_code=400, detail="Empty data provided")
    except Exception as e:
        logger.error(f"Error updating criteria file {filename}: {e}")
        raise HTTPException(status_code=500, detail=f"Error updating file: {str(e)}")

@router.post("/files")
async def create_criteria_file(file_data: dict):
    """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Ñ–∞–π–ª –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    criteria_dir = CRITERIA_PROCESSOR_PATH / "criteria"
    
    if "filename" not in file_data:
        raise HTTPException(status_code=400, detail="Filename is required")
    
    filename = file_data["filename"]
    if not filename.endswith('.csv'):
        filename += '.csv'
    
    file_path = criteria_dir / filename
    
    if file_path.exists():
        raise HTTPException(status_code=400, detail="File already exists")
    
    try:
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ –ø—É—Å—Ç–æ–π —à–∞–±–ª–æ–Ω
        if "data" in file_data and "columns" in file_data:
            df = pd.DataFrame(file_data["data"], columns=file_data["columns"])
        else:
            # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —à–∞–±–ª–æ–Ω –¥–ª—è –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
            df = pd.DataFrame(columns=[
                "Product", "Target Audience", "Criteria Type", "Criteria", 
                "Place", "Search Query", "Signals"
            ])
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω—É—é —Å—Ç—Ä–æ–∫—É
            df.loc[0] = [
                "New Product", "Target Audience", "Qualification", 
                "Sample criteria", "description", "sample query", "sample signals"
            ]
        
        df.to_csv(file_path, index=False)
        
        logger.info(f"Created new criteria file: {filename}")
        
        return {
            "message": "File created successfully",
            "filename": filename,
            "rows_created": len(df),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error creating criteria file {filename}: {e}")
        raise HTTPException(status_code=500, detail=f"Error creating file: {str(e)}")

@router.post("/upload")
async def upload_criteria_file(file: UploadFile = File(...)):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —á–µ—Ä–µ–∑ multipart/form-data"""
    criteria_dir = CRITERIA_PROCESSOR_PATH / "criteria"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
    if not file.filename.endswith(('.csv', '.xlsx', '.xls')):
        raise HTTPException(
            status_code=400, 
            detail="Unsupported file format. Use CSV or Excel files."
        )
    
    file_path = criteria_dir / file.filename
    
    if file_path.exists():
        raise HTTPException(status_code=400, detail="File already exists")
    
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        async with aiofiles.open(file_path, 'wb') as f:
            content = await file.read()
            await f.write(content)
        
        logger.info(f"Uploaded criteria file: {file.filename}")
        
        return {
            "message": "File uploaded successfully",
            "filename": file.filename,
            "size": len(content),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error uploading criteria file {file.filename}: {e}")
        raise HTTPException(status_code=500, detail=f"Error uploading file: {str(e)}")

@router.delete("/files/{filename}")
async def delete_criteria_file(filename: str):
    """–£–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    criteria_dir = CRITERIA_PROCESSOR_PATH / "criteria"
    file_path = criteria_dir / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Criteria file not found")
    
    try:
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –ë–ï–ó —Å–æ–∑–¥–∞–Ω–∏—è backup
        file_path.unlink()
        
        logger.info(f"Deleted criteria file: {filename}")
        
        return {
            "message": "File deleted successfully",
            "filename": filename,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error deleting criteria file {filename}: {e}")
        raise HTTPException(status_code=500, detail=f"Error deleting file: {str(e)}")

# –≠–Ω–¥–ø–æ–∏–Ω—Ç cleanup-backups —É–¥–∞–ª–µ–Ω - backup —Ñ–∞–π–ª—ã –±–æ–ª—å—à–µ –Ω–µ —Å–æ–∑–¥–∞—é—Ç—Å—è

@router.get("/health")
async def criteria_service_health():
    """Health check –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–∞–ø–∫–∞ criteria_processor —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not CRITERIA_PROCESSOR_PATH.exists():
            raise Exception("Criteria processor path not found")
        
        return {
            "service": "criteria_analysis",
            "status": "healthy",
            "active_sessions": len([s for s in criteria_sessions.values() if s["status"] == "processing"]),
            "total_sessions": len(criteria_sessions),
            "criteria_processor_path": str(CRITERIA_PROCESSOR_PATH)
        }
    except Exception as e:
        return {
            "service": "criteria_analysis", 
            "status": "unhealthy",
            "error": str(e)
        }

@router.get("/sessions/{session_id}/progress")
async def get_criteria_session_progress(session_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Å —Å—á–µ—Ç—á–∏–∫–∞–º–∏"""
    if session_id not in criteria_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ output –ø–∞–ø–∫–µ criteria_processor
        progress_file = CRITERIA_PROCESSOR_PATH / "output" / session_id / f"{session_id}_progress.json"
        
        if not progress_file.exists():
            # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä–∏–º –µ—Å—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - –≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ progress —Ñ–∞–π–ª–∞
            session_data = criteria_sessions[session_id]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π CSV —Ñ–∞–π–ª
            output_dir = CRITERIA_PROCESSOR_PATH / "output" / session_id
            result_files = []
            if output_dir.exists():
                result_files = list(output_dir.glob("*.csv"))
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ –Ω–µ—Ç progress —Ñ–∞–π–ª–∞ - –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
            if result_files and session_data["status"] == "processing":
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ –ø–∞–º—è—Ç–∏
                criteria_sessions[session_id]["status"] = "completed"
                criteria_sessions[session_id]["end_time"] = datetime.now().isoformat()
                
                return {
                    "session_id": session_id,
                    "status": "completed",
                    "progress": {
                        "criteria": "N/A",
                        "companies": "N/A", 
                        "processed": len(result_files),
                        "failed": 0
                    },
                    "current": {
                        "product": None,
                        "company": None,
                        "audience": None,
                        "stage": "completed"
                    },
                    "percentage": 100,
                    "message": f"Analysis completed! Found {len(result_files)} result files.",
                    "detailed_progress": False,
                    "note": "Process completed without progress tracking (legacy mode)"
                }
            
            # –ü—Ä–æ–≤–µ—Ä–∏–º –Ω–µ –∑–∞–≤–∏—Å–ª–∞ –ª–∏ –∑–∞–¥–∞—á–∞ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ
            start_time_str = session_data.get("start_time")
            if start_time_str and session_data["status"] == "processing":
                try:
                    start_time = datetime.fromisoformat(start_time_str.replace('Z', '+00:00'))
                    time_elapsed = datetime.now() - start_time.replace(tzinfo=None)
                    
                    # –ï—Å–ª–∏ –ø—Ä–æ—à–ª–æ –±–æ–ª—å—à–µ 30 –º–∏–Ω—É—Ç –±–µ–∑ progress —Ñ–∞–π–ª–∞ - —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–∏—Å
                    if time_elapsed.total_seconds() > 1800:  # 30 –º–∏–Ω—É—Ç
                        criteria_sessions[session_id]["status"] = "failed"
                        criteria_sessions[session_id]["error"] = "Process timeout - no progress detected"
                        criteria_sessions[session_id]["end_time"] = datetime.now().isoformat()
                        
                        return {
                            "session_id": session_id,
                            "status": "failed",
                            "progress": {
                                "criteria": "0/0",
                                "companies": "0/0",
                                "processed": 0,
                                "failed": 0
                            },
                            "current": {
                                "product": None,
                                "company": None,
                                "audience": None,
                                "stage": "timeout"
                            },
                            "percentage": 0,
                            "message": "Analysis timed out - process may have crashed during initialization",
                            "detailed_progress": False,
                            "error": "Process timeout after 30 minutes"
                        }
                except Exception:
                    pass
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            return {
                "session_id": session_id,
                "status": session_data["status"],
                "progress": {
                    "criteria": "0/0",
                    "companies": "0/0",
                    "processed": 0,
                    "failed": 0
                },
                "current": {
                    "product": None,
                    "company": None,
                    "audience": None,
                    "stage": "initialization"
                },
                "percentage": 0,
                "message": "Initializing...",
                "detailed_progress": False
            }
        
        # –ß–∏—Ç–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–∑ —Ñ–∞–π–ª–∞ ProcessingStateManager
        import json
        with open(progress_file, 'r', encoding='utf-8') as f:
            progress_data = json.load(f)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        total_companies = progress_data.get("total_companies", 0)
        processed_companies = progress_data.get("processed_companies", 0)
        total_criteria = progress_data.get("total_criteria", 0)
        processed_criteria = progress_data.get("processed_criteria", 0)
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, –µ—Å–ª–∏ –æ–Ω–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        if total_criteria > 0:
            percentage = min(100, int((processed_criteria / total_criteria) * 100))
        elif total_companies > 0:
            percentage = min(100, int((processed_companies / total_companies) * 100))
        else:
            percentage = 0
        
        # –°–æ–∑–¥–∞–µ–º –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        current_stage = progress_data.get("current_stage", "unknown")
        current_product = progress_data.get("current_product")
        current_company = progress_data.get("current_company")
        
        if current_stage == "general_criteria":
            message = "Checking general criteria..."
        elif current_stage == "product_start":
            message = f"Starting analysis for {current_product}"
        elif current_stage == "processing":
            if current_company:
                message = f"Analyzing {current_company} for {current_product or 'products'}"
            else:
                message = "Processing companies..."
        elif current_stage == "product_completed":
            message = f"Completed {current_product}"
        else:
            message = f"Stage: {current_stage}"
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö
        criteria_breakdown = progress_data.get("criteria_breakdown", {})
        criteria_summary = ""
        if criteria_breakdown:
            for crit_type, stats in criteria_breakdown.items():
                if stats.get("total", 0) > 0:
                    criteria_summary += f"{crit_type.title()}: {stats.get('processed', 0)}/{stats.get('total', 0)} "
        
        return {
            "session_id": session_id,
            "status": progress_data.get("status", "unknown"),
            "progress": {
                "criteria": f"{processed_criteria}/{total_criteria}" if total_criteria > 0 else "0/0",
                "companies": f"{processed_companies}/{total_companies}",
                "processed": processed_companies,
                "failed": progress_data.get("failed_companies", 0)
            },
            "current": {
                "product": current_product,
                "company": current_company,
                "audience": progress_data.get("current_audience"),
                "stage": current_stage
            },
            "percentage": percentage,
            "message": message,
            "criteria_breakdown": criteria_breakdown,
            "criteria_summary": criteria_summary.strip(),
            "detailed_progress": True,
            "last_updated": progress_data.get("updated_at"),
            "circuit_breaker_events": len(progress_data.get("circuit_breaker_events", []))
        }
        
    except Exception as e:
        logger.error(f"Error reading progress for session {session_id}: {e}")
        # Fallback –∫ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        session_data = criteria_sessions[session_id]
        return {
            "session_id": session_id,
            "status": session_data["status"],
            "progress": {
                "criteria": "0/0",
                "companies": "0/0",
                "processed": 0,
                "failed": 0
            },
            "current": {
                "product": None,
                "company": None,
                "audience": None,
                "stage": "error"
            },
            "percentage": 0,
            "message": f"Error reading progress: {str(e)}",
            "detailed_progress": False,
            "error": str(e)
        } 