"""
–û–±—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è Criteria Analysis

–°–æ–¥–µ—Ä–∂–∏—Ç –∏–º–ø–æ—Ä—Ç—ã, —É—Ç–∏–ª–∏—Ç—ã –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤–æ –≤—Å–µ—Ö –º–æ–¥—É–ª—è—Ö
"""

import os
import sys
import json
import shutil
import logging
import asyncio
import aiofiles
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from fastapi import APIRouter, File, Form, UploadFile, HTTPException
from starlette.background import BackgroundTasks
from fastapi.responses import FileResponse, JSONResponse
from src.data_io import load_session_metadata, save_session_metadata, SESSIONS_DIR, SESSIONS_METADATA_FILE

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –∫ criteria_processor
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
CRITERIA_PROCESSOR_PATH = PROJECT_ROOT / "services" / "criteria_processor"
CRITERIA_SRC_PATH = CRITERIA_PROCESSOR_PATH / "src"

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –≤ sys.path –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
for path in [str(CRITERIA_PROCESSOR_PATH), str(CRITERIA_SRC_PATH)]:
    if path not in sys.path:
        sys.path.insert(0, path)

# –õ–æ–≥–≥–µ—Ä
logger = logging.getLogger(__name__)

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
# –§–æ—Ä–º–∞—Ç: {"session_id": {"status": "processing|completed|failed", "result_path": "...", ...}}
criteria_sessions: Dict[str, Dict[str, Any]] = {}

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
criteria_tasks: Dict[str, asyncio.Task] = {}

def load_existing_criteria_sessions():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ—Å—Å–∏–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    global criteria_sessions
    
    output_dir = CRITERIA_PROCESSOR_PATH / "output"
    if not output_dir.exists():
        return
    
    for session_dir in output_dir.iterdir():
        if session_dir.is_dir() and session_dir.name.startswith("crit_"):
            session_id = session_dir.name
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Å–µ—Å—Å–∏—è —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            if session_id in criteria_sessions:
                continue
                
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Å–µ—Å—Å–∏–∏ –ø–æ –Ω–∞–ª–∏—á–∏—é —Ñ–∞–π–ª–æ–≤
            metadata_file = session_dir / f"{session_id}_metadata.json"
            progress_file = session_dir / f"{session_id}_progress.json"
            result_files = list(session_dir.glob("*.csv"))
            
            status = "unknown"
            if result_files:
                status = "completed"
            elif progress_file.exists():
                try:
                    with open(progress_file, 'r', encoding='utf-8') as f:
                        progress_data = json.load(f)
                        status = progress_data.get("status", "processing")
                except:
                    status = "processing"
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Å–µ—Å—Å–∏–∏
            criteria_sessions[session_id] = {
                "session_id": session_id,
                "status": status,
                "created_time": session_dir.stat().st_ctime,
                "result_path": str(session_dir) if result_files else None
            }
            
            logger.info(f"Loaded existing session: {session_id} (status: {status})")

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ—Å—Å–∏–∏ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
load_existing_criteria_sessions()

def generate_criteria_session_id() -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID —Å–µ—Å—Å–∏–∏ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º crit_"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"crit_{timestamp}"

def run_criteria_processor(
    input_file_path: str, 
    load_all_companies: bool = False, 
    session_id: str = None, 
    use_deep_analysis: bool = False, 
    use_parallel: bool = True, 
    max_concurrent: int = 12, 
    selected_products: List[str] = None, 
    selected_criteria_files: List[str] = None, 
    write_to_hubspot_criteria: bool = False
):
    """–ó–∞–ø—É—Å–∫–∞–µ–º criteria_processor –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å"""
    import subprocess
    
    try:
        if load_all_companies:
            cmd = [
                "python", 
                str(CRITERIA_PROCESSOR_PATH / "main.py"),
                "--all-files"
            ]
            if session_id:
                cmd.extend(["--session-id", session_id])
        else:
            # –û—á–∏—â–∞–µ–º –ø–∞–ø–∫—É data –æ—Ç —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø–µ—Ä–µ–¥ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–≥–æ
            data_dir = CRITERIA_PROCESSOR_PATH / "data"
            data_dir.mkdir(exist_ok=True)
            
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ CSV —Ñ–∞–π–ª—ã –∏–∑ data –ø–∞–ø–∫–∏
            for old_file in data_dir.glob("*.csv"):
                old_file.unlink()
                logger.info(f"Removed old file: {old_file}")
            
            # –ö–æ–ø–∏—Ä—É–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª –≤ data –ø–∞–ø–∫—É criteria_processor
            source_path = Path(input_file_path)
            target_path = data_dir / source_path.name
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—É—Ç–∏
            logger.info(f"Copying file: {source_path} -> {target_path}")
            
            # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª
            shutil.copy2(source_path, target_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª —Å–∫–æ–ø–∏—Ä–æ–≤–∞–ª—Å—è
            if target_path.exists():
                logger.info(f"File copied successfully: {target_path}")
            else:
                logger.error(f"File copy failed: {target_path}")
                return {"status": "error", "error": "Failed to copy file to data directory"}
            
            cmd = [
                "python", 
                str(CRITERIA_PROCESSOR_PATH / "main.py"),
                "--file", f"data/{target_path.name}",  # –ü—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ criteria_processor
                "--session-id", session_id  # –ü–µ—Ä–µ–¥–∞–µ–º session_id –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –ø–∞–ø–∫–∏
            ]
        
        if use_deep_analysis:
            cmd.append("--deep-analysis")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if use_parallel:
            cmd.append("--parallel")
            cmd.extend(["--max-concurrent", str(max_concurrent)])
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        if selected_criteria_files:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –≤ –ø—Ä–æ–¥—É–∫—Ç—ã
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã —Ç–æ–ª—å–∫–æ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            criteria_dir = CRITERIA_PROCESSOR_PATH / "criteria"
            selected_products_from_files = []
            
            for filename in selected_criteria_files:
                file_path = criteria_dir / filename
                if file_path.exists():
                    try:
                        if file_path.suffix.lower() == '.csv':
                            df = pd.read_csv(file_path)
                            if 'Product' in df.columns:
                                file_products = df['Product'].unique().tolist()
                                file_products = [str(p).strip() for p in file_products if pd.notna(p) and str(p).strip()]
                                selected_products_from_files.extend(file_products)
                                logger.info(f"From file {filename} extracted products: {file_products}")
                    except Exception as e:
                        logger.error(f"Error reading criteria file {filename}: {e}")
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            selected_products_from_files = list(set(selected_products_from_files))
            logger.info(f"Final products from selected files: {selected_products_from_files}")
            
            if selected_products_from_files:
                cmd.append("--selected-products")
                cmd.append(",".join(selected_products_from_files))
                logger.info(f"Adding products from selected files to command: {selected_products_from_files}")
            
        # Fallback –∫ —Å—Ç–∞—Ä–æ–π –ª–æ–≥–∏–∫–µ —Å selected_products
        elif selected_products:
            cmd.append("--selected-products")
            cmd.append(",".join(selected_products))
            logger.info(f"Adding selected products to command: {selected_products}")
        else:
            logger.info("No selected products or files specified - will process all products")
        
        # Add Circuit Breaker support (enabled by default, can be disabled via env var)
        if os.getenv('DISABLE_CIRCUIT_BREAKER', 'false').lower() == 'true':
            cmd.append("--disable-circuit-breaker")
            logger.info("Circuit Breaker –æ—Ç–∫–ª—é—á–µ–Ω —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        
        # Add HubSpot integration flag
        logger.info(f"üîç HUBSPOT –ü–ê–†–ê–ú–ï–¢–† –ü–†–û–í–ï–†–ö–ê:")
        logger.info(f"   üîó write_to_hubspot_criteria = {write_to_hubspot_criteria}")
        logger.info(f"   üìù –¢–∏–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {type(write_to_hubspot_criteria)}")
        
        if write_to_hubspot_criteria:
            cmd.append("--write-to-hubspot-criteria")
            logger.info("‚úÖ HubSpot –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞ - –¥–æ–±–∞–≤–ª–µ–Ω —Ñ–ª–∞–≥ --write-to-hubspot-criteria")
        else:
            logger.info("üìù HubSpot –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ - —Ñ–ª–∞–≥ –ù–ï –¥–æ–±–∞–≤–ª–µ–Ω")
        
        # Log the full command for debugging –ü–û–°–õ–ï –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ—Ö —Ñ–ª–∞–≥–æ–≤
        logger.info(f"üöÄ –ò–¢–û–ì–û–í–ê–Ø –ö–û–ú–ê–ù–î–ê: {' '.join(cmd)}")
        
        # –ú–µ–Ω—è–µ–º —Ä–∞–±–æ—á—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –Ω–∞ criteria_processor
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è Windows
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'
        env['PYTHONLEGACYWINDOWSSTDIO'] = '0'
        
        result = subprocess.run(
            cmd, 
            cwd=str(CRITERIA_PROCESSOR_PATH),
            capture_output=True, 
            text=True, 
            encoding='utf-8',
            env=env,
            timeout=None  # –ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π - –ø—É—Å—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ
        )
        
        if result.returncode == 0:
            logger.info(f"Criteria processor completed successfully")
            return {"status": "success", "output": result.stdout}
        else:
            logger.error(f"Criteria processor failed: {result.stderr}")
            return {"status": "error", "error": result.stderr}
            
    except subprocess.TimeoutExpired:
        logger.error("Criteria processor timed out")
        return {"status": "error", "error": "Process timed out"}
    except Exception as e:
        logger.error(f"Error running criteria processor: {e}")
        return {"status": "error", "error": str(e)}

def cleanup_old_sessions(max_sessions: int = 10) -> None:
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Å–µ—Å—Å–∏–π –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏ —Å–µ—Å—Å–∏–π –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        output_dir = CRITERIA_PROCESSOR_PATH / "output"
        if not output_dir.exists():
            return
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º crit_
        session_dirs = [d for d in output_dir.iterdir() if d.is_dir() and d.name.startswith("crit_")]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        session_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–µ—Å—Å–∏–∏
        for old_dir in session_dirs[max_sessions:]:
            try:
                shutil.rmtree(old_dir)
                logger.info(f"Removed old criteria session: {old_dir.name}")
            except Exception as e:
                logger.error(f"Error removing old criteria session {old_dir.name}: {e}")
                
    except Exception as e:
        logger.error(f"Error in cleanup_old_sessions: {e}")

def cleanup_temp_sessions(max_sessions: int = 10) -> None:
    """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π –∏–∑ temp/criteria_analysis"""
    try:
        temp_dir = PROJECT_ROOT / "temp" / "criteria_analysis"
        if not temp_dir.exists():
            return
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º crit_
        session_dirs = [d for d in temp_dir.iterdir() if d.is_dir() and d.name.startswith("crit_")]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        session_dirs.sort(key=lambda x: x.stat().st_ctime, reverse=True)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–µ—Å—Å–∏–∏
        for old_dir in session_dirs[max_sessions:]:
            try:
                shutil.rmtree(old_dir)
                logger.info(f"Removed old temp criteria session: {old_dir.name}")
            except Exception as e:
                logger.error(f"Error removing old temp criteria session {old_dir.name}: {e}")
                
    except Exception as e:
        logger.error(f"Error in cleanup_temp_sessions: {e}") 