# Company Information Processor

Веб-приложение для углубленной обработки и анализа информации о компаниях с использованием LLM, веб-скрейпинга и веб-поиска.

## Требования

- Python 3.8+
- pip (менеджер пакетов Python)

## Установка

1. Клонируйте репозиторий:
```bash
git clone <repository-url>
cd company-description
```

2. Создайте виртуальное окружение и активируйте его:
```bash
python -m venv venv
source venv/bin/activate  # для Linux/Mac
venv\Scripts\activate     # для Windows
```

3. Установите зависимости:
```bash
pip install -r requirements.txt
```

4. Создайте файл `.env` в корневой директории проекта и добавьте необходимые переменные окружения:
```
OPENAI_API_KEY=your_openai_api_key
SCRAPINGBEE_API_KEY=your_scrapingbee_api_key
```

## Запуск

1. Запустите сервер (убедитесь, что вы находитесь в корневой директории проекта):
```bash
# Пример для uvicorn, если ваш server.py использует его (например, для FastAPI бэкенда)
# python -m uvicorn backend.main:app --host 127.0.0.1 --port 8000 --reload 
# Или ваша стандартная команда, если server.py - это простой Flask или другой сервер
python server.py 
```

2. Откройте веб-браузер и перейдите по адресу (порт может отличаться в зависимости от вашей конфигурации сервера):
```
http://localhost:8000 
```

## Использование

1.  Нажмите кнопку "Start new session" (если доступна) или сразу перейдите к загрузке файла на главной странице.
2.  **Загрузите входной файл** (CSV или Excel) с названиями компаний в первом столбце. Первая строка файла (заголовок) будет проигнорирована.
3.  **Дополнительный контекст (опционально):** В поле "Additional Context" вы можете ввести текстовую информацию (до 120 символов), которая поможет системе более целенаправленно искать данные о компаниях (например, указать специфический регион, отрасль, или ключевые аспекты для фокуса).
4.  Нажмите кнопку "Run" (или "Run Processing").
5.  Будет создана новая сессия обработки. Система по умолчанию запустит полный пайплайн, включающий как стандартный сбор данных (веб-скрейпинг, Wikipedia), так и углубленный поиск с помощью LLM (`gpt-4o-mini-search-preview`) для обогащения информации.
6.  Дождитесь завершения обработки. Статус можно отслеживать на странице.
7.  Просмотрите результаты в таблице.
8.  **Скачивание результатов и логов:**
    *   **CSV отчет:** Кнопка "Download Results" скачает CSV файл с именем `<ID_сессии>_results.csv`. Колонка `description` в этом файле будет содержать текстовое резюме из трех параграфов, сгенерированное на основе всех собранных данных.
    *   **Детальный JSON:** Для каждой обработанной компании в директории сессии на сервере (`output/sessions/<ID_сессии>/json_details/`) будет сохранен отдельный `<имя_компании>.json` файл. Этот файл содержит полный структурированный JSON-объект со всеми извлеченными данными (согласно предопределенной схеме), который использовался для генерации текстового резюме. Этот JSON может быть полезен для дальнейшего анализа или интеграции.
    *   **Логи:** Кнопка "View Logs" позволяет скачать лог-файл пайплайна для текущей сессии.

## Ключевые особенности обновленного пайплайна

*   **Двухэтапная генерация описания:**
    1.  **Сбор и структурирование данных в JSON:** Система собирает информацию из различных источников (веб-сайты, Wikipedia, углубленный LLM-поиск с `gpt-4o-mini-search-preview`). Затем эта информация структурируется в подробный JSON-объект согласно предопределенной схеме с помощью `gpt-4o-mini` (или модели, указанной в `llm_config.yaml`).
    2.  **Синтез текстового резюме:** На основе этого структурированного JSON генерируется читаемое текстовое резюме из трех параграфов.
*   **Углубленный LLM-поиск:** Использует `gpt-4o-mini-search-preview` для поиска актуальной информации по широкому списку аспектов, включая специфические запросы (год основания, штаб-квартира, финансовые показатели и т.д.). Пользовательский контекст также учитывается на этом этапе.
*   **Единый выходной формат:** Основной результат в CSV всегда содержит стандартизированный набор колонок, где ключевая информация представлена в текстовом резюме в поле `description`.

## Структура проекта

```
company-description/
├── frontend/
│   ├── index.html
│   └── app.js
├── src/
│   ├── pipeline.py
│   ├── data_io.py
│   └── external_apis/
│       ├── serper_client.py
│       └── ...
├── input/
├── output/
│   └── sessions/
├── server.py
├── requirements.txt
└── README.md
```

## Лицензия

MIT
