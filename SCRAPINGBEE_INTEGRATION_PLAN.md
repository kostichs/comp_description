# План по интеграции ScrapingBee для глубокого анализа

**Цель:** Перейти от анализа сниппетов Serper к анализу полного текстового содержимого топ-N ссылок из результатов поиска, используя ScrapingBee для повышения точности и глубины анализа критериев.

---

### Фаза 0: Подготовка и Конфигурация

- [ ] **Обновить файл конфигурации (`services/criteria_processor/src/utils/config.py`):**
    - [ ] Добавить `SCRAPINGBEE_API_KEY` для хранения API-ключа.
    - [ ] Добавить флаг `USE_SCRAPINGBEE_DEEP_ANALYSIS` для включения/выключения новой функции.
    - [ ] Добавить параметр `SCRAPE_TOP_N_RESULTS` для контроля количества анализируемых ссылок (например, `3`).

- [ ] **Обновить файл `.env`:**
    - [ ] Добавить `SCRAPINGBEE_API_KEY=your_api_key_here` в `.env` файл в корне `services/criteria_processor`.

---

### Фаза 1: Создание клиента ScrapingBee

- [ ] **Создать новый файл `services/criteria_processor/src/external/scrapingbee_client.py`:**
    - [ ] Этот модуль будет содержать всю логику для взаимодействия с API ScrapingBee.

- [ ] **Реализовать функцию `scrape_website_text(url: str)` в новом файле:**
    - [ ] Функция должна принимать URL в качестве аргумента.
    - [ ] Делать API-запрос к ScrapingBee для извлечения чистого текста со страницы (используя `extract_rules`).
    - [ ] Реализовать обработку ошибок и таймаутов.
    - [ ] Возвращать либо полный текст страницы, либо `None` в случае неудачи.

---

### Фаза 2: Интеграция в основной алгоритм

- [ ] **Импортировать новую логику:**
    - [ ] В `services/criteria_processor/src/external/serper.py` импортировать `scrape_website_text` из `scrapingbee_client` и новые переменные из `config`.

- [ ] **Модифицировать `get_information_for_criterion` в `services/criteria_processor/src/external/serper.py`:**
    - [ ] Добавить проверку флага `USE_SCRAPINGBEE_DEEP_ANALYSIS`. Если он `False`, выполнять старую логику.
    - [ ] **Если флаг `True`:**
        - [ ] Выполнить поиск в Serper, как и раньше, для получения ссылок.
        - [ ] Взять топ-N ссылок из результатов Serper.
        - [ ] Запустить цикл по этим ссылкам.
        - [ ] Внутри цикла вызывать `scrape_website_text(url)` для каждой ссылки.
        - [ ] Собирать тексты всех успешно загруженных страниц в одну большую текстовую переменную, разделяя их для ясности (например, `\n\n--- CONTENT FROM {url} ---\n\n{scraped_text}`).
        - [ ] Объединить полученный текст со всех сайтов с исходным `description` компании.
        - [ ] Вернуть этот новый, обогащенный текст для дальнейшего анализа языковой моделью.

---

### Фаза 3: Тестирование и Валидация

- [ ] **Провести тестовый запуск:**
    - [ ] Запустить анализ для одной компании с критерием, требующим веб-поиска.
    - [ ] Проверить логи, чтобы убедиться, что ScrapingBee вызывается, и текст со страниц успешно извлекается.
- [ ] **Сравнить результаты:**
    - [ ] Сравнить результат анализа "до" (только сниппеты) и "после" (полный текст) для одного и того же сложного критерия, чтобы убедиться, что качество анализа улучшилось.
- [ ] **Проверить обработку ошибок:**
    - [ ] Убедиться, что алгоритм не падает, если один из сайтов не удалось загрузить, а просто пропускает его и переходит к следующему. 