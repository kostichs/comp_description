import os
import json
import pandas as pd
import csv
import chardet
import glob
from config import INPUT_PATH, COMPANIES_LIMIT, CSV_OUTPUT_PATH, OUTPUT_DIR
from logger_config import log_info, log_error, log_debug

def detect_encoding(file_path):
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)  # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 10KB –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            confidence = result['confidence']
            log_debug(f"üìù –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ {file_path}: {encoding} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            return encoding
    except Exception as e:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è {file_path}: {e}")
        return 'utf-8'

def load_csv_with_encoding(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç CSV —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏"""
    # –°–ø–∏—Å–æ–∫ –∫–æ–¥–∏—Ä–æ–≤–æ–∫ –¥–ª—è –ø–æ–ø—ã—Ç–æ–∫
    encodings_to_try = [
        detect_encoding(file_path),  # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        'utf-8-sig',                 # UTF-8 —Å BOM
        'utf-8',                     # –û–±—ã—á–Ω—ã–π UTF-8
        'windows-1251',              # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ Windows
        'cp1252',                    # Windows Western
        'iso-8859-1',                # Latin-1
        'latin1'                     # Fallback
    ]
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    encodings_to_try = list(dict.fromkeys(encodings_to_try))
    
    for encoding in encodings_to_try:
        if not encoding:
            continue
            
        try:
            log_debug(f"üîÑ –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å {file_path} —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {encoding}")
            df = pd.read_csv(file_path, encoding=encoding)
            log_info(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {encoding}")
            return df
        except (UnicodeDecodeError, UnicodeError) as e:
            log_debug(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}: {e}")
            continue
        except Exception as e:
            log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            raise
    
    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–º–æ–≥–ª–æ
    raise UnicodeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è —Ñ–∞–π–ª–∞: {file_path}")

def load_companies_data():
    """Load only companies data for processing"""
    from config import INPUT_PATH, COMPANIES_LIMIT
    
    log_info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–π: {INPUT_PATH}")
    
    companies_df = load_csv_with_encoding(INPUT_PATH)
    
    if COMPANIES_LIMIT > 0:
        log_info(f"‚ö†Ô∏è  –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {COMPANIES_LIMIT} –∫–æ–º–ø–∞–Ω–∏–π")
        companies_df = companies_df.head(COMPANIES_LIMIT)
    
    log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(companies_df)}")
    return companies_df

def load_all_criteria_files():
    """Load and combine all criteria files from criteria/ directory"""
    criteria_dir = os.path.join(os.path.dirname(__file__), "criteria")
    
    if not os.path.exists(criteria_dir):
        raise FileNotFoundError(f"‚ùå –ü–∞–ø–∫–∞ criteria –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {criteria_dir}")
    
    # Find all CSV files in criteria directory
    criteria_files = glob.glob(os.path.join(criteria_dir, "*.csv"))
    
    if not criteria_files:
        raise FileNotFoundError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –≤ –ø–∞–ø–∫–µ: {criteria_dir}")
    
    log_info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {len(criteria_files)}")
    
    all_criteria = []
    
    for file_path in criteria_files:
        filename = os.path.basename(file_path)
        log_info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑: {filename}")
        
        try:
            df = load_csv_with_encoding(file_path)
            
            # Validate required columns
            required_columns = ['Product', 'Target Audience', 'Criteria Type', 'Criteria']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                log_error(f"‚ùå –í —Ñ–∞–π–ª–µ {filename} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
                continue
            
            log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {filename}: {len(df)} –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤")
            all_criteria.append(df)
            
        except Exception as e:
            log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
            continue
    
    if not all_criteria:
        raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤")
    
    # Combine all criteria into single DataFrame
    combined_criteria = pd.concat(all_criteria, ignore_index=True)
    
    log_info(f"üéØ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {len(combined_criteria)}")
    
    # Show products and types summary
    products = combined_criteria['Product'].unique()
    criteria_types = combined_criteria['Criteria Type'].unique()
    
    log_info(f"üìä –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–¥—É–∫—Ç—ã: {', '.join(products)}")
    log_info(f"üìä –¢–∏–ø—ã –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {', '.join(criteria_types)}")
    
    return combined_criteria

def load_data():
    """Load all data files - updated for automatic criteria loading with global general criteria"""
    from config import CRITERIA_TYPE, INPUT_PATH, COMPANIES_LIMIT, INDUSTRY_MAPPING
    
    log_info(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è: {CRITERIA_TYPE}")
    
    # Get product name for filtering
    product_name = INDUSTRY_MAPPING.get(CRITERIA_TYPE, CRITERIA_TYPE)
    log_info(f"üè≠ –§–∏–ª—å—Ç—Ä—É–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É: {product_name}")
    
    try:
        # Load companies data
        log_info(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏: {INPUT_PATH}")
        companies_df = load_csv_with_encoding(INPUT_PATH)
        
        if COMPANIES_LIMIT > 0:
            log_info(f"‚ö†Ô∏è  –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {COMPANIES_LIMIT} –∫–æ–º–ø–∞–Ω–∏–π")
            companies_df = companies_df.head(COMPANIES_LIMIT)
        
        log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(companies_df)}")
        
        # Load all criteria files automatically
        df_criteria = load_all_criteria_files()
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        log_info("üåê –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤...")
        all_general_criteria = df_criteria[df_criteria["Criteria Type"] == "General"]["Criteria"].dropna().unique().tolist()
        log_info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö General –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {len(all_general_criteria)}")
        for i, criteria in enumerate(all_general_criteria, 1):
            log_info(f"   {i}. {criteria}")
        
        # Filter by product for other criteria types (NOT for General!)
        product_criteria = df_criteria[df_criteria["Product"] == product_name]
        log_info(f"üìã –ù–∞–π–¥–µ–Ω–æ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ {product_name}: {len(product_criteria)}")
        
        if len(product_criteria) == 0:
            log_error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞: {product_name}")
            log_info(f"üí° –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã: {', '.join(df_criteria['Product'].unique())}")
            raise ValueError(f"–ù–µ—Ç –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞: {product_name}")
        
        # Extract qualification questions FOR THIS PRODUCT
        qualification_df = product_criteria[product_criteria["Criteria Type"] == "Qualification"].dropna(subset=["Criteria", "Target Audience"])
        qualification_questions = {
            row["Target Audience"]: row["Criteria"]
            for _, row in qualification_df.iterrows()
        }
        log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è {product_name}: {len(qualification_questions)}")
        for audience in qualification_questions:
            log_info(f"   - {audience}")
        
        # Load mandatory criteria FOR THIS PRODUCT
        mandatory_df = product_criteria[product_criteria["Criteria Type"] == "Mandatory"].dropna(subset=["Criteria", "Target Audience"])
        log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è {product_name}: {len(mandatory_df)}")
        
        # Load NTH criteria FOR THIS PRODUCT
        nth_df = product_criteria[product_criteria["Criteria Type"] == "NTH"].dropna(subset=["Criteria", "Target Audience"])
        log_info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ NTH –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è {product_name}: {len(nth_df)}")
        
        log_info("üöÄ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        log_info(f"üåê General –∫—Ä–∏—Ç–µ—Ä–∏–∏ –±—É–¥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫–æ –≤—Å–µ–º –∫–æ–º–ø–∞–Ω–∏—è–º")
        log_info(f"üéØ –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞: {product_name}")
        
        return {
            "companies": companies_df,
            "general_criteria": all_general_criteria,  # –í–°–ï General –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
            "qualification_questions": qualification_questions,
            "mandatory_df": mandatory_df,
            "nth_df": nth_df,
            "product": CRITERIA_TYPE
        }
        
    except FileNotFoundError as e:
        log_error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
        raise
    except Exception as e:
        log_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def save_results(results, product, timestamp=None):
    """Save results to both JSON and CSV files"""
    if not timestamp:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create output filenames
    json_filename = f"analysis_results_{product}_{timestamp}.json"
    csv_filename = f"analysis_results_{product}_{timestamp}.csv"
    
    json_path = os.path.join(OUTPUT_DIR, json_filename)
    csv_path = os.path.join(OUTPUT_DIR, csv_filename)
    
    # Ensure output directory exists
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Save to JSON with pretty formatting
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    log_info(f"üíæ JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_path}")
    
    # Convert to CSV format
    csv_data = []
    for result in results:
        # Flatten nested structures for CSV
        flat_result = flatten_result_for_csv(result)
        csv_data.append(flat_result)
    
    # Save to CSV
    if csv_data:
        df_csv = pd.DataFrame(csv_data)
        df_csv.to_csv(csv_path, index=False, encoding='utf-8-sig')
        log_info(f"üíæ CSV —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        log_info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(results)}")
        log_info(f"üìã –ö–æ–ª–æ–Ω–æ–∫ –≤ CSV: {len(df_csv.columns)}")
    
    return json_path, csv_path

def flatten_result_for_csv(result):
    """Converts nested JSON result to flat dictionary for CSV"""
    flat = {}
    
    # Basic company info
    flat["Company_Name"] = result.get("Company_Name", "")
    flat["Official_Website"] = result.get("Official_Website", "")
    flat["Description"] = result.get("Description", "")
    
    # Status fields
    flat["Global_Criteria_Status"] = result.get("Global_Criteria_Status", "")
    flat["Final_Status"] = result.get("Final_Status", "")
    flat["Qualified_Audiences"] = ", ".join(result.get("Qualified_Audiences", []))
    
    # Qualification results
    for key, value in result.items():
        if key.startswith("Qualification_"):
            flat[key] = value
    
    # Status for each audience
    for key, value in result.items():
        if key.startswith("Status_"):
            flat[key] = value
    
    # Mandatory results
    for key, value in result.items():
        if key.startswith("Mandatory_"):
            flat[key] = value
    
    # NTH results and scores
    for key, value in result.items():
        if key.startswith("NTH_"):
            flat[key] = value
    
    # General criteria results
    for key, value in result.items():
        if key.startswith("General_"):
            flat[key] = value
    
    # Any other fields
    for key, value in result.items():
        if key not in flat and not isinstance(value, (dict, list)):
            flat[key] = value
        elif isinstance(value, list):
            flat[key] = ", ".join(str(v) for v in value)
    
    return flat 