Шаблон: Интеграция Новой Функции Парсинга
Это руководство описывает шаги по добавлению новой функции для извлечения данных о компаниях из дополнительных источников (например, другие веб-сайты, бизнес-аналитические платформы) в существующий пайплайн обработки.
Цель: Расширить набор собираемых данных для каждой компании, минимизируя риск для существующей функциональности.
Шаг 1: Разработка Новой Функции Парсинга
Определите Источник Данных:
Какой сайт/API вы хотите парсить?
Нужны ли API-ключи или специфические заголовки для доступа?
Какую информацию вы хотите извлечь (например, финансовые показатели, количество сотрудников, специфические технологии)?
Создайте Новую Асинхронную Функцию:
Местоположение: Рекомендуется создать новый файл в src/external_apis/ (например, src/external_apis/my_new_parser_client.py) или добавить в существующий релевантный файл. Если это не внешний API, а, например, сложный парсинг HTML со специфичного типа сайтов, можно создать src/custom_parsers.py.
Сигнатура функции (пример):
Apply to README.md
Обработка ошибок и Retry: Если новая функция делает внешние сетевые запросы, ОБЯЗАТЕЛЬНО реализуйте надежную обработку ошибок и логику повторных попыток (рекомендуется tenacity).
Возвращаемое значение: Функция должна возвращать словарь с извлеченными данными или None, если данные не найдены или произошла неисправимая ошибка.
Тестирование Новой Функции в Изоляции:
Напишите небольшой тестовый скрипт или используйте интерактивную сессию Python для проверки работы вашей новой функции parse_platform_xyz_async с различными входными данными (успешные случаи, случаи с ошибками, отсутствующие данные).
Шаг 2: Интеграция Новой Функции в Пайплайн process_company
Импортируйте Новую Функцию:
В файле src/pipeline.py добавьте импорт вашей новой функции:
Apply to README.md
Определите Место Вызова:
Внутри функции async def process_company(...) решите, на каком этапе логично вызывать вашу новую функцию. Обычно это делается после того, как у вас есть основные идентификаторы компании (имя, возможно, URL сайта).
Вызовите Новую Функцию и Обработайте Результат:
Apply to README.md
Шаг 3: Обновление Структуры Данных result_data и CSV
Определите Новые Поля: Решите, какие новые ключи появятся в result_data для хранения данных из нового источника. Например, platform_x_revenue, platform_x_employees.
Инициализация result_data (в src/pipeline.py, внутри process_company):
Добавьте новые ключи с значениями по умолчанию.
Apply to README.md
Заполнение result_data Новыми Данными (в src/pipeline.py, внутри process_company):
После вызова parse_platform_xyz_async, если platform_xyz_data не None, заполните соответствующие поля в result_data.
Apply to README.md
Обновление expected_csv_fieldnames (КРИТИЧЕСКИ ВАЖНО!):
В файле backend/main.py (внутри функции execute_pipeline_for_session_async или где у вас формируются заголовки для запусков через UI):
Добавьте имена новых колонок в список base_ordered_fields или в additional_llm_fields (если они приходят от LLM на основе этих новых данных). Обычно, если это просто новые сырые данные, они добавляются в base_ordered_fields.
Apply to README.md
В файле src/pipeline.py (в функции run_pipeline, ЕСЛИ вы используете ее для CLI-запусков):
Аналогично обновите base_ordered_fields.
Apply to README.md
Обогащение Описания (Опционально):
Если вы хотите, чтобы LLM использовал эти новые данные для генерации описания, добавьте их в combined_sources_for_llm в process_company:
Apply to README.md
Шаг 4: Тестирование
Перезапустите Сервер FastAPI/Uvicorn.
УДАЛИТЕ СТАРЫЕ ВЫХОДНЫЕ CSV-ФАЙЛЫ, чтобы заголовки были переписаны с новыми колонками.
Протестируйте на небольшом наборе данных (1-5 компаний):
Убедитесь, что новые колонки появляются в CSV.
Проверьте, что данные из нового источника корректно извлекаются и записываются.
Просмотрите логи на предмет ошибок от вашей новой функции парсинга.
Убедитесь, что существующие поля по-прежнему заполняются корректно.
Протестируйте на большем наборе данных, чтобы оценить влияние на общее время обработки.