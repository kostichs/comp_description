# Архитектура приложения Company Information Processor

## Общая структура

Приложение представляет собой асинхронный пайплайн обработки информации о компаниях, состоящий из следующих основных компонентов:
### Пакетная обработка для больших наборов данных

Для повышения стабильности и управляемости при обработке файлов с большим количеством компаний, основной цикл обработки в функции `run_pipeline_for_file` (вызываемой из бэкенда) теперь реализует пакетный подход. Список компаний из входного файла делится на пакеты (по умолчанию по 50 компаний), и каждый такой пакет обрабатывается последовательно. Это означает, что переход к следующему пакету происходит только после завершения обработки всех компаний в текущем пакете. Внутри одного пакета обработка отдельных компаний по-прежнему происходит асинхронно. Такой подход помогает снизить пиковую нагрузку на внешние API и системные ресурсы.

### 1. Основные модули

#### 1.1 Pipeline (`src/pipeline.py`)
Центральный модуль, координирующий весь процесс обработки. Основные функции:

##### 1.1.1 `process_company(company_name: str, aiohttp_session: aiohttp.ClientSession, sb_client: ScrapingBeeClient, llm_config: dict, openai_client: AsyncOpenAI, context_text: str | None, serper_api_key: str | None) -> dict`

**Входные параметры:**
- `company_name`: str - название компании для обработки
- `aiohttp_session`: aiohttp.ClientSession - сессия для HTTP запросов
- `sb_client`: ScrapingBeeClient - клиент для скрапинга
- `llm_config`: dict - конфигурация для LLM
- `openai_client`: AsyncOpenAI - клиент OpenAI
- `context_text`: str | None - дополнительный контекст
- `serper_api_key`: str | None - API ключ для Serper

**Возвращает:**
- dict с полями:
  - "name": str - название компании
  - "homepage": str - URL домашней страницы
  - "linkedin": str - URL LinkedIn
  - "description": str - сгенерированное описание
  - "timestamp": str - дата обработки компании в формате YYYY-MM-DD

**Процесс работы:**
1. Инициализирует словарь `result_data` с базовыми полями
2. Вызывает `find_urls_with_serper_async` для поиска URL
3. Вызывает `scrape_page_data_async` для скрапинга домашней страницы
4. Вызывает `find_and_scrape_about_page_async` для поиска и скрапинга страницы "About Us"
5. Вызывает `get_wikipedia_summary_async` для получения Wikipedia summary
6. Вызывает `extract_text_for_description` для извлечения текста
7. Вызывает `generate_description_openai_async` для генерации описания
8. Сохраняет результаты через `save_results_csv`

##### 1.1.2 `run_pipeline() -> None`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- None

**Процесс работы:**
1. Создает необходимые директории
2. Загружает конфигурации и API ключи через `load_env_vars`
3. Загружает LLM конфигурацию через `load_llm_config`
4. Инициализирует API клиенты
5. Для каждого файла в директории input:
   - Загружает контекст через `load_context_file`
   - Загружает названия компаний через `load_and_prepare_company_names`
   - Создает асинхронные задачи для каждой компании
   - Сохраняет результаты через `save_results_csv`

##### 1.1.3 `run_pipeline_for_file(input_file_path: str | Path, output_csv_path: str | Path, pipeline_log_path: str, scoring_log_path: str, context_text: str | None, company_col_index: int, aiohttp_session: aiohttp.ClientSession, sb_client: ScrapingBeeClient, llm_config: dict, openai_client: AsyncOpenAI, serper_api_key: str, expected_csv_fieldnames: list[str], broadcast_update: callable = None) -> tuple[int, int, list[dict]]`

**Входные параметры:**
- `input_file_path`: str | Path - путь к входному файлу
- `output_csv_path`: str | Path - путь для сохранения результатов
- `pipeline_log_path`: str - путь для логов пайплайна
- `scoring_log_path`: str - путь для логов скоринга
- `context_text`: str | None - контекст
- `company_col_index`: int - индекс колонки с названиями компаний
- `aiohttp_session`: aiohttp.ClientSession - сессия для HTTP запросов
- `sb_client`: ScrapingBeeClient - клиент для скрапинга
- `llm_config`: dict - конфигурация LLM
- `openai_client`: AsyncOpenAI - клиент OpenAI
- `serper_api_key`: str - API ключ Serper
- `expected_csv_fieldnames`: list[str] - ожидаемые заголовки CSV
- `broadcast_update`: callable - функция для отправки обновлений

**Возвращает:**
- tuple[int, int, list[dict]]:
  - Количество успешных обработок
  - Количество ошибок
  - Список результатов

    **Процесс работы:**
    1. Загружает полный список названий компаний из `input_file_path` с помощью `load_and_prepare_company_names`.
    2. Если список компаний не пуст, инициализирует CSV-файл для результатов (`output_csv_path`) с заголовками (если файл новый).
    3. Итерируется по общему списку компаний, разбивая его на пакеты размером `main_batch_size`.
    4. Для каждого пакета компаний:
        a. Создает список асинхронных задач, где каждая задача - это вызов `process_company` для одной компании из текущего пакета.
        b. Асинхронно выполняет задачи текущего пакета, обрабатывая результаты по мере их поступления.
        c. Сохраняет результат обработки каждой компании в `output_csv_path` (в режиме добавления).
        d. При использовании `broadcast_update`, отправляет обновления о прогрессе и результатах.
        e. Логирует завершение обработки пакета.
    5. После обработки всех пакетов логирует общую статистику (количество успехов и неудач).
    6. Возвращает кортеж с общим количеством успешных обработок, общим количеством ошибок и списком всех собранных результатов.
    7. Корректно обрабатывает внешнюю отмену задачи (`asyncio.CancelledError`), отменяя активные подзадачи и возвращая текущие результаты.

##### 1.1.4 `process_companies(file_path: str, session_dir: str, broadcast_update: callable) -> None`

**Входные параметры:**
- `file_path`: str - путь к файлу с компаниями
- `session_dir`: str - директория сессии
- `broadcast_update`: callable - функция для отправки обновлений

**Возвращает:**
- None

**Процесс работы:**
1. Загружает названия компаний через `load_and_prepare_company_names`
2. Инициализирует API клиенты
3. Создает путь для выходного CSV
4. Запускает обработку через `run_pipeline_for_file`
5. Отправляет обновления через `broadcast_update`

##### 1.1.5 `setup_session_logging(pipeline_log_path: str, scoring_log_path: str) -> None`

**Входные параметры:**
- `pipeline_log_path`: str - путь для логов пайплайна
- `scoring_log_path`: str - путь для логов скоринга

**Возвращает:**
- None

**Процесс работы:**
1. Настраивает корневой логгер для пайплайна
2. Настраивает отдельный логгер для скоринга
3. Создает файловые обработчики для логов
4. Настраивает форматирование логов

**Точка входа в приложение:**
Основной точкой входа является функция `run_pipeline()`, которая вызывается в блоке `if __name__ == "__main__":`. Она инициализирует все необходимые компоненты и запускает обработку файлов.

#### 1.2 Data I/O (`src/data_io.py`)
Модуль для работы с данными. Основные функции:

##### 1.2.1 `load_and_prepare_company_names(file_path: str | Path, col_index: int = 0) -> list[str] | None`

**Входные параметры:**
- `file_path`: str | Path - путь к файлу с данными (CSV или Excel)
- `col_index`: int - индекс колонки с названиями компаний (по умолчанию 0)

**Возвращает:**
- list[str] | None - список названий компаний или None в случае ошибки

**Вызывается из:**
- `run_pipeline()` - при обработке входных файлов
- `run_pipeline_for_file()` - при инициализации обработки файла
- `process_companies()` - при загрузке списка компаний

**Процесс работы:**
1. Определяет тип файла (Excel или CSV)
2. Пытается прочитать файл с заголовком
3. Если не удалось, пробует без заголовка
4. Извлекает названия компаний из указанной колонки
5. Фильтрует пустые значения и 'nan'
6. Возвращает список валидных названий

**Отправляет данные в:**
- `process_company()` - для обработки каждой компании
- `save_results_csv()` - для сохранения результатов

##### 1.2.2 `load_context_file(context_file_path: str) -> str | None`

**Входные параметры:**
- `context_file_path`: str - путь к файлу с контекстом

**Возвращает:**
- str | None - текст контекста или None, если файл не найден/пуст

**Вызывается из:**
- `run_pipeline()` - при обработке каждого входного файла
- `run_pipeline_for_file()` - при инициализации обработки

**Процесс работы:**
1. Проверяет существование файла
2. Читает содержимое файла
3. Удаляет лишние пробелы
4. Возвращает текст контекста

**Отправляет данные в:**
- `process_company()` - как дополнительный контекст для обработки

##### 1.2.3 `save_context_file(context_file_path: str, context_text: str) -> bool`

**Входные параметры:**
- `context_file_path`: str - путь для сохранения контекста
- `context_text`: str - текст контекста для сохранения

**Возвращает:**
- bool - True при успешном сохранении, False при ошибке

**Вызывается из:**
- `run_pipeline()` - при сохранении нового контекста
- `run_pipeline_for_file()` - при обновлении контекста

**Процесс работы:**
1. Создает директорию, если не существует
2. Записывает контекст в файл
3. Возвращает статус операции

##### 1.2.4 `load_session_metadata() -> list[dict]`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- list[dict] - список метаданных сессий

**Вызывается из:**
- `main.py` - при получении списка сессий через API
- `run_pipeline()` - при инициализации новой сессии

**Процесс работы:**
1. Проверяет существование файла метаданных
2. Читает JSON данные
3. Валидирует формат данных
4. Возвращает список метаданных

**Отправляет данные в:**
- API endpoints для отображения списка сессий
- `save_session_metadata()` - при обновлении метаданных

##### 1.2.5 `save_session_metadata(metadata: list[dict]) -> None`

**Входные параметры:**
- `metadata`: list[dict] - список метаданных сессий для сохранения

**Возвращает:**
- None

**Вызывается из:**
- `main.py` - при создании/обновлении сессии через API
- `run_pipeline()` - при завершении обработки

**Процесс работы:**
1. Создает директорию для сессий, если не существует
2. Сохраняет метаданные в JSON файл
3. Логирует результат операции

##### 1.2.6 `save_results_csv(results: list[dict], output_file_path: str | Path, append_mode: bool = False, fieldnames: list[str] | None = None) -> None`

**Входные параметры:**
- `results`: list[dict] - список результатов для сохранения
- `output_file_path`: str | Path - путь для сохранения CSV
- `append_mode`: bool - режим добавления (True) или перезаписи (False)
- `fieldnames`: list[str] | None - список заголовков CSV

**Возвращает:**
- None

**Вызывается из:**
- `process_company()` - при сохранении результатов обработки компании
- `run_pipeline_for_file()` - при сохранении результатов файла
- `process_companies()` - при сохранении итоговых результатов

**Процесс работы:**
1. Создает директорию для выходного файла
2. Определяет режим записи (append/write)
3. Записывает заголовки, если необходимо
4. Записывает результаты в CSV
5. Логирует результат операции

**Отправляет данные в:**
- Файловую систему (CSV файл)
- Логирование результатов

#### 1.3 Backend API (`backend/main.py`)
FastAPI приложение, предоставляющее REST API. Основные эндпоинты:

##### 1.3.1 `GET /api/sessions`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- JSON с полями:
  - "sessions": list[dict] - список сессий с метаданными

**Вызывается из:**
- Frontend: `app.js` - при загрузке списка сессий
- Frontend: `index.html` - при инициализации страницы

**Процесс работы:**
1. Вызывает `load_session_metadata()`
2. Форматирует данные для отображения
3. Возвращает список сессий

**Отправляет данные в:**
- Frontend для отображения списка сессий
- WebSocket для real-time обновлений

##### 1.3.2 `GET /api/sessions/{session_id}`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- JSON с полями:
  - "session": dict - метаданные сессии
  - "status": str - статус сессии
  - "results": list[dict] - результаты обработки

**Вызывается из:**
- Frontend: `app.js` - при выборе сессии
- Frontend: `index.html` - при обновлении деталей сессии

**Процесс работы:**
1. Загружает метаданные сессии
2. Проверяет существование сессии
3. Загружает результаты обработки
4. Форматирует данные для отображения

**Отправляет данные в:**
- Frontend для отображения деталей сессии
- WebSocket для обновления статуса

##### 1.3.3 `POST /api/sessions`

**Входные параметры:**
- FormData с полями:
  - "file": File - файл с данными
  - "context": str - контекст обработки (опционально)

**Возвращает:**
- JSON с полями:
  - "session_id": str - идентификатор созданной сессии
  - "status": str - статус создания

**Вызывается из:**
- Frontend: `app.js` - при загрузке файла
- Frontend: `index.html` - при создании новой сессии

**Процесс работы:**
1. Создает новую сессию
2. Сохраняет загруженный файл
3. Сохраняет контекст
4. Обновляет метаданные сессии

**Отправляет данные в:**
- `save_session_metadata()` - для сохранения метаданных
- `save_context_file()` - для сохранения контекста
- Frontend для отображения новой сессии

##### 1.3.4 `POST /api/sessions/{session_id}/start`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- JSON с полями:
  - "status": str - статус запуска
  - "message": str - сообщение о результате

**Вызывается из:**
- Frontend: `app.js` - при запуске обработки
- Frontend: `index.html` - при нажатии кнопки Start

**Процесс работы:**
1. Проверяет статус сессии
2. Инициализирует обработку
3. Запускает фоновую задачу
4. Обновляет статус сессии

**Отправляет данные в:**
- `process_companies()` - для обработки данных
- WebSocket для обновления прогресса
- Frontend для отображения статуса

##### 1.3.5 `GET /api/sessions/{session_id}/results`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- JSON с полями:
  - "results": list[dict] - результаты обработки
  - "status": str - статус обработки

**Вызывается из:**
- Frontend: `app.js` - при запросе результатов
- Frontend: `index.html` - при обновлении таблицы результатов

**Процесс работы:**
1. Загружает результаты сессии
2. Форматирует данные
3. Возвращает результаты

**Отправляет данные в:**
- Frontend для отображения результатов
- WebSocket для обновления данных

##### 1.3.6 `GET /api/sessions/{session_id}/logs/{log_type}`

**Входные параметры:**
- `session_id`: str - идентификатор сессии
- `log_type`: str - тип лога (pipeline/scoring)

**Возвращает:**
- JSON с полями:
  - "logs": list[str] - строки лога
  - "status": str - статус загрузки

**Вызывается из:**
- Frontend: `app.js` - при запросе логов
- Frontend: `index.html` - при отображении логов

**Процесс работы:**
1. Определяет путь к лог-файлу
2. Читает содержимое лога
3. Форматирует данные
4. Возвращает логи

**Отправляет данные в:**
- Frontend для отображения логов
- WebSocket для обновления логов

##### 1.3.7 `WebSocket /ws`

**Входные параметры:**
- WebSocket соединение

**Возвращает:**
- WebSocket сообщения с полями:
  - "type": str - тип сообщения
  - "data": dict - данные обновления

**Вызывается из:**
- Frontend: `app.js` - при подключении к WebSocket
- Frontend: `index.html` - при инициализации real-time обновлений

**Процесс работы:**
1. Устанавливает WebSocket соединение
2. Отправляет обновления о:
   - Прогрессе обработки
   - Статусе сессии
   - Результатах
   - Логах

**Отправляет данные в:**
- Frontend для real-time обновлений UI
- `broadcast_update()` - для рассылки обновлений

#### 1.4 Frontend (`frontend/`)
Веб-интерфейс пользователя. Основные компоненты:

##### 1.4.1 `index.html`

**Основные компоненты:**
- HTML структура с полями:
  - Форма загрузки файла
  - Таблица результатов
  - Элементы управления сессией
  - Индикаторы прогресса
  - Логи

**Вызывается из:**
- Браузер при открытии приложения
- Backend: `main.py` - при запросе главной страницы

**Процесс работы:**
1. Загружает базовую структуру страницы
2. Подключает стили (`style.css`)
3. Подключает скрипты (`app.js`)
4. Инициализирует UI компоненты

**Отправляет данные в:**
- `app.js` - для обработки пользовательских действий
- Backend API - для выполнения операций

##### 1.4.2 `app.js`

###### 1.4.2.1 `initializeApp()`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- None

**Вызывается из:**
- `index.html` - при загрузке страницы
- `window.onload` - при инициализации

**Процесс работы:**
1. Инициализирует WebSocket соединение
2. Загружает список сессий
3. Настраивает обработчики событий
4. Инициализирует UI компоненты

**Отправляет данные в:**
- `loadSessions()` - для загрузки сессий
- `setupWebSocket()` - для настройки WebSocket
- `setupEventListeners()` - для настройки обработчиков

###### 1.4.2.2 `loadSessions()`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- Promise<void>

**Вызывается из:**
- `initializeApp()` - при инициализации
- `refreshSessions()` - при обновлении списка

**Процесс работы:**
1. Запрашивает список сессий через API
2. Обновляет таблицу сессий
3. Обрабатывает ошибки
4. Обновляет UI

**Отправляет данные в:**
- `updateSessionsTable()` - для обновления таблицы
- `handleError()` - при ошибках

###### 1.4.2.3 `uploadFile(file: File, context: string)`

**Входные параметры:**
- `file`: File - загружаемый файл
- `context`: string - контекст обработки

**Возвращает:**
- Promise<{sessionId: string, status: string}>

**Вызывается из:**
- Обработчик формы загрузки
- `handleFileUpload()` - при выборе файла

**Процесс работы:**
1. Создает FormData
2. Отправляет файл через API
3. Обрабатывает ответ
4. Обновляет UI

**Отправляет данные в:**
- Backend API: `POST /api/sessions`
- `updateUI()` - для обновления интерфейса

###### 1.4.2.4 `startProcessing(sessionId: string)`

**Входные параметры:**
- `sessionId`: string - идентификатор сессии

**Возвращает:**
- Promise<void>

**Вызывается из:**
- Обработчик кнопки Start
- `handleStartProcessing()` - при запуске

**Процесс работы:**
1. Отправляет запрос на запуск
2. Обновляет статус сессии
3. Подключается к WebSocket
4. Обновляет UI

**Отправляет данные в:**
- Backend API: `POST /api/sessions/{id}/start`
- `setupWebSocket()` - для обновлений
- `updateProgress()` - для отображения прогресса

###### 1.4.2.5 `setupWebSocket()`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- WebSocket соединение

**Вызывается из:**
- `initializeApp()` - при инициализации
- `startProcessing()` - при запуске обработки

**Процесс работы:**
1. Создает WebSocket соединение
2. Настраивает обработчики событий
3. Обрабатывает сообщения
4. Обновляет UI

**Отправляет данные в:**
- `handleWebSocketMessage()` - для обработки сообщений
- `updateProgress()` - для обновления прогресса
- `updateResults()` - для обновления результатов

###### 1.4.2.6 `updateResults(sessionId: string)`

**Входные параметры:**
- `sessionId`: string - идентификатор сессии

**Возвращает:**
- Promise<void>

**Вызывается из:**
- `setupWebSocket()` - при получении обновлений
- `refreshResults()` - при обновлении результатов

**Процесс работы:**
1. Запрашивает результаты через API
2. Обновляет таблицу результатов
3. Обновляет статистику
4. Обновляет UI

**Отправляет данные в:**
- Backend API: `GET /api/sessions/{id}/results`
- `updateResultsTable()` - для обновления таблицы
- `updateStatistics()` - для обновления статистики

##### 1.4.3 `style.css`

**Основные компоненты:**
- Стили для:
  - Контейнеров и layout
  - Форм и кнопок
  - Таблиц и списков
  - Индикаторов прогресса
  - Адаптивного дизайна

**Вызывается из:**
- `index.html` - при загрузке страницы

**Процесс работы:**
1. Определяет базовые стили
2. Настраивает компоненты
3. Определяет анимации
4. Настраивает медиа-запросы

**Отправляет данные в:**
- Браузер для отображения стилей
- UI компоненты для применения стилей

### 2. Процесс обработки компании

#### 2.1 Инициализация сессии

##### 2.1.1 `load_env_vars() -> dict`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- dict с полями:
  - "SERPER_API_KEY": str
  - "SCRAPINGBEE_API_KEY": str
  - "OPENAI_API_KEY": str
  - "LOG_LEVEL": str
  - "ENVIRONMENT": str

**Вызывается из:**
- `run_pipeline()` - при инициализации пайплайна
- `process_companies()` - при запуске обработки

**Процесс работы:**
1. Загружает переменные из .env файла
2. Проверяет наличие обязательных ключей
3. Валидирует значения
4. Возвращает словарь с конфигурацией

**Отправляет данные в:**
- `run_pipeline()` - для инициализации API клиентов
- `process_companies()` - для настройки обработки

##### 2.1.2 `load_llm_config() -> dict`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- dict с полями:
  - "model": str - модель LLM
  - "temperature": float - температура генерации
  - "max_tokens": int - максимальное количество токенов
  - "system_prompt": str - системный промпт
  - "retry_attempts": int - количество попыток

**Вызывается из:**
- `run_pipeline()` - при инициализации пайплайна
- `process_company()` - при обработке компании

**Процесс работы:**
1. Загружает конфигурацию из config.json
2. Проверяет обязательные поля
3. Валидирует значения
4. Возвращает конфигурацию

**Отправляет данные в:**
- `process_company()` - для настройки LLM
- `generate_description_openai_async()` - для генерации описаний

##### 2.1.3 `setup_logging(log_dir: str, log_level: str) -> None`

**Входные параметры:**
- `log_dir`: str - директория для логов
- `log_level`: str - уровень логирования

**Возвращает:**
- None

**Вызывается из:**
- `run_pipeline()` - при инициализации пайплайна
- `setup_session_logging()` - при настройке логирования сессии

**Процесс работы:**
1. Создает директорию для логов
2. Настраивает форматтер
3. Создает файловые обработчики
4. Настраивает корневой логгер

**Отправляет данные в:**
- Файловую систему (лог-файлы)
- `setup_session_logging()` - для настройки логгеров сессии

##### 2.1.4 `create_directories() -> None`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- None

**Вызывается из:**
- `run_pipeline()` - при инициализации пайплайна
- `process_companies()` - при запуске обработки

**Процесс работы:**
1. Создает директорию input
2. Создает директорию output
3. Создает директорию logs
4. Создает директорию sessions

**Отправляет данные в:**
- Файловую систему (создание директорий)
- `run_pipeline()` - для проверки готовности к работе

##### 2.1.5 `initialize_api_clients(api_keys: dict) -> tuple`

**Входные параметры:**
- `api_keys`: dict с полями:
  - "SERPER_API_KEY": str
  - "SCRAPINGBEE_API_KEY": str
  - "OPENAI_API_KEY": str

**Возвращает:**
- tuple с полями:
  - aiohttp.ClientSession
  - ScrapingBeeClient
  - AsyncOpenAI

**Вызывается из:**
- `run_pipeline()` - при инициализации пайплайна
- `process_companies()` - при запуске обработки

**Процесс работы:**
1. Создает aiohttp сессию
2. Инициализирует ScrapingBee клиент
3. Инициализирует OpenAI клиент
4. Возвращает кортеж с клиентами

**Отправляет данные в:**
- `process_company()` - для обработки компаний
- `run_pipeline_for_file()` - для обработки файлов

##### 2.1.6 `validate_session(session_id: str) -> bool`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- bool - True если сессия валидна, False иначе

**Вызывается из:**
- `run_pipeline()` - при проверке сессии
- `process_companies()` - при запуске обработки

**Процесс работы:**
1. Проверяет существование директории сессии
2. Проверяет наличие метаданных
3. Валидирует статус сессии
4. Возвращает результат проверки

**Отправляет данные в:**
- `run_pipeline()` - для принятия решения о запуске
- `process_companies()` - для проверки возможности обработки

#### 2.2 Запуск обработки

##### 2.2.1 `load_companies_from_file(file_path: str | Path, col_index: int = 0) -> list[str]`

**Входные параметры:**
- `file_path`: str | Path - путь к файлу с данными
- `col_index`: int - индекс колонки с названиями компаний (по умолчанию 0)

**Возвращает:**
- list[str] - список названий компаний

**Вызывается из:**
- `run_pipeline_for_file()` - при инициализации обработки файла
- `process_companies()` - при загрузке списка компаний

**Процесс работы:**
1. Определяет тип файла (CSV/Excel)
2. Читает файл с учетом заголовка
3. Извлекает названия из указанной колонки
4. Фильтрует пустые значения
5. Возвращает список компаний

**Отправляет данные в:**
- `process_company()` - для обработки каждой компании
- `create_processing_tasks()` - для создания задач

##### 2.2.2 `create_processing_tasks(companies: list[str], session: aiohttp.ClientSession, sb_client: ScrapingBeeClient, llm_config: dict, openai_client: AsyncOpenAI, context: str | None, serper_api_key: str) -> list[Task]`

**Входные параметры:**
- `companies`: list[str] - список компаний
- `session`: aiohttp.ClientSession - HTTP сессия
- `sb_client`: ScrapingBeeClient - клиент для скрапинга
- `llm_config`: dict - конфигурация LLM
- `openai_client`: AsyncOpenAI - клиент OpenAI
- `context`: str | None - контекст обработки
- `serper_api_key`: str - API ключ Serper

**Возвращает:**
- list[Task] - список асинхронных задач

**Вызывается из:**
- `run_pipeline_for_file()` - при создании задач обработки
- `process_companies()` - при инициализации обработки

**Процесс работы:**
1. Создает список для хранения задач
2. Для каждой компании:
   - Создает задачу process_company
   - Добавляет параметры
   - Добавляет в список
3. Возвращает список задач

**Отправляет данные в:**
- `asyncio.gather()` - для параллельного выполнения
- `process_company()` - для обработки компаний

##### 2.2.3 `execute_processing_tasks(tasks: list[Task], max_concurrent: int = 5) -> list[dict]`

**Входные параметры:**
- `tasks`: list[Task] - список задач для выполнения
- `max_concurrent`: int - максимальное количество одновременных задач

**Возвращает:**
- list[dict] - список результатов обработки

**Вызывается из:**
- `run_pipeline_for_file()` - при выполнении задач
- `process_companies()` - при запуске обработки

**Процесс работы:**
1. Создает семафор для ограничения параллельных задач
2. Запускает задачи с ограничением
3. Собирает результаты
4. Обрабатывает ошибки
5. Возвращает результаты

**Отправляет данные в:**
- `save_results_csv()` - для сохранения результатов
- `broadcast_update()` - для обновления прогресса

##### 2.2.4 `handle_processing_results(results: list[dict], output_path: str | Path, append_mode: bool = False) -> None`

**Входные параметры:**
- `results`: list[dict] - результаты обработки
- `output_path`: str | Path - путь для сохранения
- `append_mode`: bool - режим добавления (True) или перезаписи (False)

**Возвращает:**
- None

**Вызывается из:**
- `run_pipeline_for_file()` - при сохранении результатов
- `process_companies()` - при завершении обработки

**Процесс работы:**
1. Создает директорию для выходного файла
2. Определяет режим записи
3. Записывает заголовки
4. Сохраняет результаты
5. Логирует операцию

**Отправляет данные в:**
- Файловую систему (CSV файл)
- `broadcast_update()` - для обновления статуса

##### 2.2.5 `update_processing_status(session_id: str, status: str, progress: float, message: str | None = None) -> None`

**Входные параметры:**
- `session_id`: str - идентификатор сессии
- `status`: str - новый статус
- `progress`: float - прогресс обработки (0-100)
- `message`: str | None - дополнительное сообщение

**Возвращает:**
- None

**Вызывается из:**
- `run_pipeline_for_file()` - при обновлении статуса
- `process_companies()` - при изменении прогресса

**Процесс работы:**
1. Загружает метаданные сессии
2. Обновляет статус и прогресс
3. Добавляет сообщение если есть
4. Сохраняет метаданные
5. Отправляет обновление

**Отправляет данные в:**
- `save_session_metadata()` - для сохранения метаданных
- `broadcast_update()` - для обновления UI

##### 2.2.6 `handle_processing_error(error: Exception, company_name: str, session_id: str) -> None`

**Входные параметры:**
- `error`: Exception - возникшая ошибка
- `company_name`: str - название компании
- `session_id`: str - идентификатор сессии

**Возвращает:**
- None

**Вызывается из:**
- `execute_processing_tasks()` - при возникновении ошибки
- `process_company()` - при ошибке обработки

**Процесс работы:**
1. Логирует ошибку
2. Обновляет статистику ошибок
3. Обновляет статус сессии
4. Отправляет уведомление

**Отправляет данные в:**
- Лог-файлы
- `update_processing_status()` - для обновления статуса
- `broadcast_update()` - для уведомления UI

#### 2.3 Обработка одной компании (process_company)

##### 2.3.1 `initialize_company_data(company_name: str) -> dict`

**Входные параметры:**
- `company_name`: str - название компании

**Возвращает:**
- dict с полями:
  - "name": str - название компании
  - "homepage": str - URL домашней страницы
  - "linkedin": str - URL LinkedIn
  - "description": str - описание компании
  - "status": str - статус обработки
  - "error": str | None - сообщение об ошибке

**Вызывается из:**
- `process_company()` - при инициализации обработки
- `handle_processing_error()` - при восстановлении после ошибки

**Процесс работы:**
1. Создает базовую структуру данных
2. Инициализирует поля значениями по умолчанию
3. Устанавливает начальный статус
4. Возвращает словарь с данными

**Отправляет данные в:**
- `process_company()` - для дальнейшей обработки
- `save_results_csv()` - для сохранения результатов

##### 2.3.2 `find_urls_with_serper_async(company_name: str, session: aiohttp.ClientSession, serper_api_key: str) -> dict`

**Входные параметры:**
- `company_name`: str - название компании
- `session`: aiohttp.ClientSession - HTTP сессия
- `serper_api_key`: str - API ключ Serper

**Возвращает:**
- dict с полями:
  - "homepage": str | None - URL домашней страницы
  - "linkedin": str | None - URL LinkedIn
  - "wikipedia": str | None - URL Wikipedia

**Вызывается из:**
- `process_company()` - при поиске URL
- `validate_page()` - при проверке URL

**Процесс работы:**
1. Формирует поисковый запрос
2. Отправляет запрос к Serper API
3. Обрабатывает результаты поиска
4. Извлекает и валидирует URL
5. Возвращает словарь с URL

**Отправляет данные в:**
- `scrape_page_data_async()` - для скрапинга домашней страницы
- `find_and_scrape_about_page_async()` - для поиска About Us
- `get_wikipedia_summary_async()` - для получения Wikipedia данных

##### 2.3.3 `scrape_page_data_async(url: str, sb_client: ScrapingBeeClient) -> dict`

**Входные параметры:**
- `url`: str - URL для скрапинга
- `sb_client`: ScrapingBeeClient - клиент ScrapingBee

**Возвращает:**
- dict с полями:
  - "title": str - заголовок страницы
  - "url": str - корневой URL
  - "content": str - HTML контент
  - "status": str - статус скрапинга

**Вызывается из:**
- `process_company()` - при скрапинге домашней страницы
- `find_and_scrape_about_page_async()` - при скрапинге About Us

**Процесс работы:**
1. Проверяет валидность URL
2. Отправляет запрос к ScrapingBee
3. Обрабатывает ответ
4. Извлекает необходимые данные
5. Возвращает результаты

**Отправляет данные в:**
- `validate_page()` - для проверки данных
- `extract_text_for_description()` - для извлечения текста

##### 2.3.4 `find_and_scrape_about_page_async(homepage_url: str, sb_client: ScrapingBeeClient) -> dict`

**Входные параметры:**
- `homepage_url`: str - URL домашней страницы
- `sb_client`: ScrapingBeeClient - клиент ScrapingBee

**Возвращает:**
- dict с полями:
  - "url": str | None - URL страницы About Us
  - "content": str | None - контент страницы
  - "status": str - статус операции

**Вызывается из:**
- `process_company()` - при поиске About Us
- `validate_page()` - при проверке About Us

**Процесс работы:**
1. Скрапит домашнюю страницу
2. Ищет ссылки на About Us
3. Валидирует найденные ссылки
4. Скрапит найденную страницу
5. Возвращает результаты

**Отправляет данные в:**
- `extract_text_for_description()` - для извлечения текста
- `combine_sources()` - для комбинирования источников

##### 2.3.5 `get_wikipedia_summary_async(company_name: str, session: aiohttp.ClientSession) -> dict`

**Входные параметры:**
- `company_name`: str - название компании
- `session`: aiohttp.ClientSession - HTTP сессия

**Возвращает:**
- dict с полями:
  - "summary": str | None - Wikipedia summary
  - "url": str | None - URL статьи
  - "status": str - статус операции

**Вызывается из:**
- `process_company()` - при получении Wikipedia данных
- `validate_page()` - при проверке Wikipedia

**Процесс работы:**
1. Формирует URL для Wikipedia API
2. Отправляет запрос
3. Обрабатывает ответ
4. Извлекает summary
5. Возвращает результаты

**Отправляет данные в:**
- `combine_sources()` - для комбинирования источников
- `extract_text_for_description()` - для извлечения текста

##### 2.3.6 `validate_page(content: dict, company_name: str) -> bool`

**Входные параметры:**
- `content`: dict - данные страницы
- `company_name`: str - название компании

**Возвращает:**
- bool - True если страница валидна, False иначе

**Вызывается из:**
- `process_company()` - при валидации данных
- `scrape_page_data_async()` - после скрапинга

**Процесс работы:**
1. Проверяет наличие обязательных полей
2. Валидирует контент
3. Проверяет соответствие компании
4. Возвращает результат проверки

**Отправляет данные в:**
- `process_company()` - для принятия решения
- `handle_processing_error()` - при ошибке валидации

##### 2.3.7 `extract_text_for_description(sources: dict) -> str`

**Входные параметры:**
- `sources`: dict с полями:
  - "about_us": str | None
  - "wikipedia": str | None
  - "linkedin": str | None
  - "homepage": str | None

**Возвращает:**
- str - извлеченный текст для описания

**Вызывается из:**
- `process_company()` - при извлечении текста
- `combine_sources()` - при комбинировании источников

**Процесс работы:**
1. Проверяет доступные источники
2. Извлекает текст из каждого источника
3. Очищает и форматирует текст
4. Возвращает подготовленный текст

**Отправляет данные в:**
- `generate_description_openai_async()` - для генерации описания
- `validate_description()` - для проверки результата

##### 2.3.8 `generate_description_openai_async(text: str, context: str | None, llm_config: dict, openai_client: AsyncOpenAI) -> str`

**Входные параметры:**
- `text`: str - исходный текст
- `context`: str | None - дополнительный контекст
- `llm_config`: dict - конфигурация LLM
- `openai_client`: AsyncOpenAI - клиент OpenAI

**Возвращает:**
- str - сгенерированное описание

**Вызывается из:**
- `process_company()` - при генерации описания
- `validate_description()` - при перегенерации

**Процесс работы:**
1. Подготавливает промпт
2. Отправляет запрос к OpenAI
3. Обрабатывает ответ
4. Форматирует описание
5. Возвращает результат

**Отправляет данные в:**
- `validate_description()` - для проверки описания
- `save_results_csv()` - для сохранения результатов

##### 2.3.9 `validate_description(description: str, company_name: str) -> bool`

**Входные параметры:**
- `description`: str - сгенерированное описание
- `company_name`: str - название компании

**Возвращает:**
- bool - True если описание валидно, False иначе

**Вызывается из:**
- `process_company()` - при валидации описания
- `generate_description_openai_async()` - после генерации

**Процесс работы:**
1. Проверяет длину описания
2. Валидирует формат
3. Проверяет соответствие компании
4. Возвращает результат проверки

**Отправляет данные в:**
- `process_company()` - для принятия решения
- `generate_description_openai_async()` - при перегенерации

### 3. Внешние API интеграции

#### 3.1 Serper API

##### 3.1.1 `search_company_async(company_name: str, session: aiohttp.ClientSession, api_key: str) -> dict`

**Входные параметры:**
- `company_name`: str - название компании
- `session`: aiohttp.ClientSession - HTTP сессия
- `api_key`: str - API ключ Serper

**Возвращает:**
- dict с полями:
  - "organic": list[dict] - органические результаты
  - "knowledgeGraph": dict | None - данные из Knowledge Graph
  - "status": str - статус запроса

**Вызывается из:**
- `find_urls_with_serper_async()` - при поиске URL
- `validate_company_info()` - при проверке информации

**Процесс работы:**
1. Формирует поисковый запрос
2. Отправляет запрос к Serper API
3. Обрабатывает ответ
4. Извлекает результаты
5. Возвращает данные

**Отправляет данные в:**
- `extract_company_urls()` - для извлечения URL
- `validate_search_results()` - для валидации результатов

##### 3.1.2 `extract_company_urls(search_results: dict) -> dict`

**Входные параметры:**
- `search_results`: dict - результаты поиска от Serper

**Возвращает:**
- dict с полями:
  - "homepage": str | None - URL домашней страницы
  - "linkedin": str | None - URL LinkedIn
  - "wikipedia": str | None - URL Wikipedia
  - "confidence": float - уверенность в результатах

**Вызывается из:**
- `find_urls_with_serper_async()` - при обработке результатов
- `validate_search_results()` - при валидации URL

**Процесс работы:**
1. Анализирует органические результаты
2. Проверяет Knowledge Graph
3. Извлекает URL из сниппетов
4. Оценивает релевантность
5. Возвращает найденные URL

**Отправляет данные в:**
- `validate_urls()` - для проверки URL
- `process_company()` - для дальнейшей обработки

##### 3.1.3 `validate_urls(urls: dict, company_name: str) -> dict`

**Входные параметры:**
- `urls`: dict - словарь с URL
- `company_name`: str - название компании

**Возвращает:**
- dict с полями:
  - "homepage": str | None - валидный URL домашней страницы
  - "linkedin": str | None - валидный URL LinkedIn
  - "wikipedia": str | None - валидный URL Wikipedia
  - "validation_status": dict - статус валидации каждого URL

**Вызывается из:**
- `extract_company_urls()` - после извлечения URL
- `process_company()` - при проверке URL

**Процесс работы:**
1. Проверяет формат URL
2. Валидирует домены
3. Проверяет доступность
4. Оценивает релевантность
5. Возвращает валидные URL

**Отправляет данные в:**
- `scrape_page_data_async()` - для скрапинга
- `find_and_scrape_about_page_async()` - для поиска About Us

##### 3.1.4 `guess_company_domain(company_name: str) -> list[str]`

**Входные параметры:**
- `company_name`: str - название компании

**Возвращает:**
- list[str] - список возможных доменов

**Вызывается из:**
- `validate_urls()` - при отсутствии прямого совпадения
- `process_company()` - при поиске домена

**Процесс работы:**
1. Очищает название компании
2. Генерирует варианты домена
3. Добавляет популярные TLD
4. Возвращает список доменов

**Отправляет данные в:**
- `check_domain_availability()` - для проверки доменов
- `validate_urls()` - для валидации

##### 3.1.5 `check_domain_availability(domain: str) -> bool`

**Входные параметры:**
- `domain`: str - домен для проверки

**Возвращает:**
- bool - True если домен доступен, False иначе

**Вызывается из:**
- `guess_company_domain()` - при проверке доменов
- `validate_urls()` - при валидации URL

**Процесс работы:**
1. Выполняет DNS запрос
2. Проверяет HTTP доступность
3. Проверяет SSL сертификат
4. Возвращает результат проверки

**Отправляет данные в:**
- `validate_urls()` - для принятия решения
- `process_company()` - для обновления статуса

##### 3.1.6 `select_best_url(urls: list[str], company_name: str, llm_client: AsyncOpenAI) -> str`

**Входные параметры:**
- `urls`: list[str] - список URL
- `company_name`: str - название компании
- `llm_client`: AsyncOpenAI - клиент OpenAI

**Возвращает:**
- str - выбранный URL

**Вызывается из:**
- `validate_urls()` - при выборе URL
- `process_company()` - при определении основного URL

**Процесс работы:**
1. Подготавливает контекст для LLM
2. Отправляет запрос к LLM
3. Анализирует ответ
4. Выбирает лучший URL
5. Возвращает результат

**Отправляет данные в:**
- `process_company()` - для дальнейшей обработки
- `scrape_page_data_async()` - для скрапинга

#### 3.2 ScrapingBee

##### 3.2.1 `initialize_scrapingbee_client(api_key: str) -> ScrapingBeeClient`

**Входные параметры:**
- `api_key`: str - API ключ ScrapingBee

**Возвращает:**
- ScrapingBeeClient - инициализированный клиент

**Вызывается из:**
- `run_pipeline()` - при инициализации пайплайна
- `process_companies()` - при запуске обработки

**Процесс работы:**
1. Создает клиент ScrapingBee
2. Настраивает параметры по умолчанию
3. Проверяет подключение
4. Возвращает клиент

**Отправляет данные в:**
- `scrape_page_data_async()` - для скрапинга
- `find_and_scrape_about_page_async()` - для поиска About Us

##### 3.2.2 `scrape_with_retry(url: str, client: ScrapingBeeClient, max_retries: int = 3) -> dict`

**Входные параметры:**
- `url`: str - URL для скрапинга
- `client`: ScrapingBeeClient - клиент ScrapingBee
- `max_retries`: int - максимальное количество попыток

**Возвращает:**
- dict с полями:
  - "content": str - HTML контент
  - "status_code": int - HTTP статус
  - "headers": dict - HTTP заголовки
  - "retry_count": int - количество попыток

**Вызывается из:**
- `scrape_page_data_async()` - при скрапинге страницы
- `find_and_scrape_about_page_async()` - при скрапинге About Us

**Процесс работы:**
1. Настраивает параметры скрапинга
2. Выполняет запрос
3. Обрабатывает ошибки
4. Повторяет при необходимости
5. Возвращает результаты

**Отправляет данные в:**
- `process_page_content()` - для обработки контента
- `handle_scraping_error()` - при ошибках

##### 3.2.3 `process_page_content(content: str, url: str) -> dict`

**Входные параметры:**
- `content`: str - HTML контент
- `url`: str - URL страницы

**Возвращает:**
- dict с полями:
  - "title": str - заголовок страницы
  - "text": str - извлеченный текст
  - "links": list[str] - найденные ссылки
  - "metadata": dict - метаданные страницы

**Вызывается из:**
- `scrape_with_retry()` - после успешного скрапинга
- `find_and_scrape_about_page_async()` - при обработке About Us

**Процесс работы:**
1. Парсит HTML
2. Извлекает заголовок
3. Очищает текст
4. Находит ссылки
5. Возвращает обработанные данные

**Отправляет данные в:**
- `extract_text_for_description()` - для извлечения текста
- `find_about_page_link()` - для поиска About Us

##### 3.2.4 `find_about_page_link(links: list[str], base_url: str) -> str | None`

**Входные параметры:**
- `links`: list[str] - список ссылок
- `base_url`: str - базовый URL

**Возвращает:**
- str | None - URL страницы About Us или None

**Вызывается из:**
- `process_page_content()` - при поиске About Us
- `find_and_scrape_about_page_async()` - при определении URL

**Процесс работы:**
1. Фильтрует ссылки по ключевым словам
2. Проверяет релевантность
3. Валидирует URL
4. Возвращает найденный URL

**Отправляет данные в:**
- `scrape_with_retry()` - для скрапинга About Us
- `process_company()` - для обновления данных

##### 3.2.5 `handle_scraping_error(error: Exception, url: str) -> dict`

**Входные параметры:**
- `error`: Exception - возникшая ошибка
- `url`: str - URL, при скрапинге которого возникла ошибка

**Возвращает:**
- dict с полями:
  - "error_type": str - тип ошибки
  - "message": str - сообщение об ошибке
  - "url": str - проблемный URL
  - "retry_possible": bool - возможность повтора

**Вызывается из:**
- `scrape_with_retry()` - при возникновении ошибки
- `process_company()` - при обработке ошибок

**Процесс работы:**
1. Определяет тип ошибки
2. Формирует сообщение
3. Проверяет возможность повтора
4. Возвращает информацию об ошибке

**Отправляет данные в:**
- `scrape_with_retry()` - для повторной попытки
- `process_company()` - для обработки ошибки

##### 3.2.6 `extract_metadata_from_page(content: str) -> dict`

**Входные параметры:**
- `content`: str - HTML контент

**Возвращает:**
- dict с полями:
  - "meta_description": str | None
  - "meta_keywords": list[str]
  - "og_tags": dict
  - "schema_org": dict | None

**Вызывается из:**
- `process_page_content()` - при обработке страницы
- `validate_page()` - при валидации данных

**Процесс работы:**
1. Извлекает meta-теги
2. Парсит Open Graph теги
3. Ищет Schema.org разметку
4. Форматирует данные
5. Возвращает метаданные

**Отправляет данные в:**
- `validate_page()` - для проверки данных
- `extract_text_for_description()` - для обогащения контекста

#### 3.3 OpenAI

##### 3.3.1 `initialize_openai_client(api_key: str) -> AsyncOpenAI`

**Входные параметры:**
- `api_key`: str - API ключ OpenAI

**Возвращает:**
- AsyncOpenAI - инициализированный клиент

**Вызывается из:**
- `run_pipeline()` - при инициализации пайплайна
- `process_companies()` - при запуске обработки

**Процесс работы:**
1. Создает асинхронный клиент OpenAI
2. Настраивает параметры по умолчанию
3. Проверяет подключение
4. Возвращает клиент

**Отправляет данные в:**
- `generate_description_openai_async()` - для генерации описаний
- `validate_url_with_llm()` - для валидации URL

##### 3.3.2 `generate_description_openai_async(text: str, context: str | None, llm_config: dict, openai_client: AsyncOpenAI) -> str`

**Входные параметры:**
- `text`: str - исходный текст для генерации
- `context`: str | None - дополнительный контекст
- `llm_config`: dict - конфигурация LLM
- `openai_client`: AsyncOpenAI - клиент OpenAI

**Возвращает:**
- str - сгенерированное описание компании

**Вызывается из:**
- `process_company()` - при генерации описания
- `validate_description()` - при перегенерации

**Процесс работы:**
1. Подготавливает промпт
2. Формирует системное сообщение
3. Отправляет запрос к API
4. Обрабатывает ответ
5. Возвращает сгенерированный текст

**Отправляет данные в:**
- `validate_description()` - для проверки результата
- `save_results_csv()` - для сохранения описания

##### 3.3.3 `validate_url_with_llm(url: str, company_name: str, openai_client: AsyncOpenAI) -> dict`

**Входные параметры:**
- `url`: str - URL для валидации
- `company_name`: str - название компании
- `openai_client`: AsyncOpenAI - клиент OpenAI

**Возвращает:**
- dict с полями:
  - "is_valid": bool - валидность URL
  - "confidence": float - уверенность в результате
  - "explanation": str - объяснение решения

**Вызывается из:**
- `validate_urls()` - при проверке URL
- `process_company()` - при валидации найденных URL

**Процесс работы:**
1. Формирует запрос к LLM
2. Отправляет URL и название компании
3. Анализирует ответ
4. Оценивает уверенность
5. Возвращает результат валидации

**Отправляет данные в:**
- `validate_urls()` - для принятия решения
- `process_company()` - для обновления статуса

##### 3.3.4 `get_embeddings_async(text: str, openai_client: AsyncOpenAI) -> list[float]`

**Входные параметры:**
- `text`: str - текст для получения эмбеддингов
- `openai_client`: AsyncOpenAI - клиент OpenAI

**Возвращает:**
- list[float] - вектор эмбеддингов

**Вызывается из:**
- `process_company()` - при анализе текста
- `validate_description()` - при проверке описания

**Процесс работы:**
1. Подготавливает текст
2. Отправляет запрос к API
3. Обрабатывает ответ
4. Возвращает вектор эмбеддингов

**Отправляет данные в:**
- `analyze_context()` - для анализа контекста
- `validate_description()` - для проверки описания

##### 3.3.5 `analyze_context(text: str, embeddings: list[float], openai_client: AsyncOpenAI) -> dict`

**Входные параметры:**
- `text`: str - анализируемый текст
- `embeddings`: list[float] - эмбеддинги текста
- `openai_client`: AsyncOpenAI - клиент OpenAI

**Возвращает:**
- dict с полями:
  - "relevance": float - релевантность
  - "key_topics": list[str] - ключевые темы
  - "sentiment": str - тональность
  - "summary": str - краткое содержание

**Вызывается из:**
- `process_company()` - при анализе контекста
- `validate_description()` - при проверке описания

**Процесс работы:**
1. Анализирует эмбеддинги
2. Извлекает ключевые темы
3. Определяет тональность
4. Генерирует краткое содержание
5. Возвращает результаты анализа

**Отправляет данные в:**
- `generate_description_openai_async()` - для улучшения генерации
- `validate_description()` - для проверки результата

##### 3.3.6 `validate_results_with_llm(results: dict, company_name: str, openai_client: AsyncOpenAI) -> dict`

**Входные параметры:**
- `results`: dict - результаты обработки
- `company_name`: str - название компании
- `openai_client`: AsyncOpenAI - клиент OpenAI

**Возвращает:**
- dict с полями:
  - "is_valid": bool - валидность результатов
  - "issues": list[str] - найденные проблемы
  - "suggestions": list[str] - предложения по улучшению
  - "confidence": float - уверенность в оценке

**Вызывается из:**
- `process_company()` - при валидации результатов
- `validate_description()` - при проверке описания

**Процесс работы:**
1. Анализирует результаты
2. Проверяет соответствие компании
3. Ищет проблемы
4. Формирует предложения
5. Возвращает результаты валидации

**Отправляет данные в:**
- `process_company()` - для принятия решения
- `handle_processing_error()` - при обнаружении проблем
  

### 4. Логирование

#### 4.1 Основное логирование
- Логирование процесса обработки
- Отслеживание ошибок
- Мониторинг прогресса
- Настройка логгеров для разных компонентов
- Ротация лог-файлов

#### 4.2 Логирование скоринга
- Детальное логирование процесса оценки
- Сохранение метрик
- Логирование промежуточных результатов
- Отслеживание производительности

### 5. Управление сессиями

#### 5.1 Метаданные сессий

##### 5.1.1 `create_session_metadata(file_path: str, context: str | None = None) -> dict`

**Входные параметры:**
- `file_path`: str - путь к файлу с данными
- `context`: str | None - дополнительный контекст (опционально)

**Возвращает:**
- dict с полями:
  - "session_id": str - уникальный идентификатор сессии
  - "created_at": str - время создания
  - "file_path": str - путь к файлу
  - "context": str | None - контекст
  - "status": str - статус сессии
  - "progress": float - прогресс обработки (0-100)

**Вызывается из:**
- `run_pipeline()` - при создании новой сессии
- `process_companies()` - при инициализации обработки

**Процесс работы:**
1. Генерирует уникальный ID сессии
2. Создает базовую структуру метаданных
3. Добавляет информацию о файле
4. Устанавливает начальный статус
5. Возвращает метаданные

**Отправляет данные в:**
- `save_session_metadata()` - для сохранения
- `update_session_status()` - для обновления статуса

##### 5.1.2 `update_session_status(session_id: str, status: str, progress: float = None, message: str = None) -> bool`

**Входные параметры:**
- `session_id`: str - идентификатор сессии
- `status`: str - новый статус
- `progress`: float - прогресс обработки (опционально)
- `message`: str - дополнительное сообщение (опционально)

**Возвращает:**
- bool - True при успешном обновлении, False при ошибке

**Вызывается из:**
- `process_company()` - при обновлении прогресса
- `handle_processing_error()` - при изменении статуса

**Процесс работы:**
1. Загружает текущие метаданные
2. Обновляет статус и прогресс
3. Добавляет сообщение если есть
4. Сохраняет обновленные метаданные
5. Возвращает результат операции

**Отправляет данные в:**
- `save_session_metadata()` - для сохранения
- `broadcast_update()` - для уведомления UI

##### 5.1.3 `get_session_metadata(session_id: str) -> dict | None`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- dict | None - метаданные сессии или None если не найдена

**Вызывается из:**
- `process_company()` - при проверке статуса
- `validate_session()` - при валидации сессии

**Процесс работы:**
1. Проверяет существование сессии
2. Загружает метаданные
3. Валидирует формат данных
4. Возвращает метаданные

**Отправляет данные в:**
- `validate_session()` - для проверки
- `update_session_status()` - для обновления

##### 5.1.4 `list_active_sessions() -> list[dict]`

**Входные параметры:**
- Нет входных параметров

**Возвращает:**
- list[dict] - список активных сессий с метаданными

**Вызывается из:**
- `run_pipeline()` - при проверке активных сессий
- `process_companies()` - при инициализации

**Процесс работы:**
1. Сканирует директорию сессий
2. Загружает метаданные каждой сессии
3. Фильтрует активные сессии
4. Сортирует по времени создания
5. Возвращает список сессий

**Отправляет данные в:**
- `run_pipeline()` - для управления сессиями
- `broadcast_update()` - для обновления UI

##### 5.1.5 `cleanup_session(session_id: str, force: bool = False) -> bool`

**Входные параметры:**
- `session_id`: str - идентификатор сессии
- `force`: bool - принудительная очистка (опционально)

**Возвращает:**
- bool - True при успешной очистке, False при ошибке

**Вызывается из:**
- `run_pipeline()` - при завершении сессии
- `handle_processing_error()` - при ошибках

**Процесс работы:**
1. Проверяет статус сессии
2. Удаляет временные файлы
3. Архивирует результаты
4. Обновляет метаданные
5. Возвращает результат операции

**Отправляет данные в:**
- `update_session_status()` - для обновления статуса
- `broadcast_update()` - для уведомления UI

##### 5.1.6 `validate_session_metadata(metadata: dict) -> tuple[bool, str]`

**Входные параметры:**
- `metadata`: dict - метаданные для валидации

**Возвращает:**
- tuple[bool, str]:
  - bool - результат валидации
  - str - сообщение об ошибке если есть

**Вызывается из:**
- `save_session_metadata()` - перед сохранением
- `get_session_metadata()` - при загрузке

**Процесс работы:**
1. Проверяет обязательные поля
2. Валидирует типы данных
3. Проверяет значения
4. Возвращает результат валидации

**Отправляет данные в:**
- `save_session_metadata()` - для принятия решения
- `handle_processing_error()` - при ошибках валидации

#### 5.2 Файловая структура

##### 5.2.1 `setup_session_directories(session_id: str) -> dict`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- dict с полями:
  - "input_dir": str - путь к директории входных файлов
  - "output_dir": str - путь к директории выходных файлов
  - "logs_dir": str - путь к директории логов
  - "temp_dir": str - путь к директории временных файлов
  - "cache_dir": str - путь к директории кэша

**Вызывается из:**
- `create_session_metadata()` - при создании сессии
- `process_companies()` - при инициализации обработки

**Процесс работы:**
1. Создает базовую директорию сессии
2. Создает поддиректории для разных типов файлов
3. Настраивает права доступа
4. Возвращает пути к директориям

**Отправляет данные в:**
- `create_session_metadata()` - для сохранения путей
- `cleanup_session()` - для очистки директорий

##### 5.2.2 `save_input_file(file_path: str, session_id: str) -> str`

**Входные параметры:**
- `file_path`: str - путь к исходному файлу
- `session_id`: str - идентификатор сессии

**Возвращает:**
- str - путь к сохраненному файлу в директории сессии

**Вызывается из:**
- `run_pipeline()` - при загрузке файла
- `process_companies()` - при инициализации

**Процесс работы:**
1. Определяет тип файла
2. Создает копию в директории сессии
3. Валидирует файл
4. Возвращает новый путь

**Отправляет данные в:**
- `create_session_metadata()` - для обновления метаданных
- `load_companies_from_file()` - для загрузки данных

##### 5.2.3 `save_output_file(data: list[dict], session_id: str, file_type: str = "csv") -> str`

**Входные параметры:**
- `data`: list[dict] - данные для сохранения
- `session_id`: str - идентификатор сессии
- `file_type`: str - тип файла (по умолчанию "csv")

**Возвращает:**
- str - путь к сохраненному файлу

**Вызывается из:**
- `process_company()` - при сохранении результатов
- `handle_processing_results()` - при завершении обработки

**Процесс работы:**
1. Определяет формат сохранения
2. Создает файл в выходной директории
3. Записывает данные
4. Возвращает путь к файлу

**Отправляет данные в:**
- `update_session_metadata()` - для обновления метаданных
- `broadcast_update()` - для уведомления UI

##### 5.2.4 `setup_logging_directories(session_id: str) -> dict`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- dict с полями:
  - "pipeline_log": str - путь к логу пайплайна
  - "scoring_log": str - путь к логу скоринга
  - "error_log": str - путь к логу ошибок

**Вызывается из:**
- `setup_session_directories()` - при создании директорий
- `setup_logging()` - при настройке логирования

**Процесс работы:**
1. Создает директорию для логов
2. Настраивает пути для разных типов логов
3. Инициализирует файлы логов
4. Возвращает пути к логам

**Отправляет данные в:**
- `setup_logging()` - для настройки логгеров
- `update_session_metadata()` - для обновления метаданных

##### 5.2.5 `manage_temp_files(session_id: str, action: str = "clean") -> bool`

**Входные параметры:**
- `session_id`: str - идентификатор сессии
- `action`: str - действие ("clean" или "archive")

**Возвращает:**
- bool - True при успешном выполнении, False при ошибке

**Вызывается из:**
- `cleanup_session()` - при очистке сессии
- `process_company()` - при управлении временными файлами

**Процесс работы:**
1. Определяет действие
2. Находит временные файлы
3. Выполняет действие (очистка/архивация)
4. Возвращает результат

**Отправляет данные в:**
- `cleanup_session()` - для завершения очистки
- `update_session_status()` - для обновления статуса

##### 5.2.6 `setup_cache_directory(session_id: str) -> str`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- str - путь к директории кэша

**Вызывается из:**
- `setup_session_directories()` - при создании директорий
- `process_company()` - при инициализации кэша

**Процесс работы:**
1. Создает директорию кэша
2. Настраивает права доступа
3. Инициализирует структуру кэша
4. Возвращает путь к директории

**Отправляет данные в:**
- `setup_session_directories()` - для обновления путей
- `process_company()` - для использования кэша

### 6. Асинхронная обработка

#### 6.1 Основные асинхронные операции
- HTTP запросы
- Скрапинг
- LLM запросы
- Параллельная обработка компаний
- Управление очередью задач

#### 6.2 Управление сессиями
- aiohttp сессии
- ScrapingBee клиент
- OpenAI клиент
- Пул соединений
- Таймауты и повторные попытки

### 7. Валидация и обработка ошибок

#### 7.1 Валидация данных
- Проверка URL
- Валидация контента
- Проверка форматов
- Валидация входных файлов
- Проверка целостности данных

#### 7.2 Обработка ошибок
- Логирование ошибок
- Восстановление после сбоев
- Сохранение частичных результатов
- Обработка сетевых ошибок
- Восстановление после таймаутов

### 8. Оптимизация производительности

#### 8.1 Асинхронная обработка
- Параллельные запросы
- Эффективное использование ресурсов
- Балансировка нагрузки
- Ограничение одновременных запросов

#### 8.2 Кэширование
- Сохранение промежуточных результатов
- Повторное использование данных
- Кэширование API ответов
- Управление кэшем

### 9. Безопасность

#### 9.1 API ключи
- Безопасное хранение
- Управление доступом
- Ротация ключей
- Валидация ключей

#### 9.2 Валидация входных данных
- Проверка форматов
- Санитизация данных
- Защита от инъекций
- Валидация файлов

### 10. Взаимодействие Frontend и Backend

#### 10.1 Frontend (`frontend/`)
- **Основные компоненты**:
  - `index.html` - основной HTML файл с интерфейсом
  - `app.js` - JavaScript логика приложения
  - `style.css` - стили приложения

- **Функциональность**:
  - Загрузка файлов (CSV/Excel)
  - Управление сессиями
  - Отображение результатов в таблице
  - Индикация прогресса
  - WebSocket для real-time обновлений
  - Скачивание результатов

#### 10.2 Backend API (`backend/`)
- **Основные компоненты**:
  - `main.py` - FastAPI приложение
  - `processing_runner.py` - обработчик фоновых задач

- **API Endpoints**:
  - `GET /` - главная страница
  - `GET /api/sessions` - список сессий
  - `GET /api/sessions/{session_id}` - детали сессии
  - `POST /api/sessions` - создание сессии
  - `POST /api/sessions/{session_id}/start` - запуск обработки
  - `GET /api/sessions/{session_id}/results` - результаты
  - `GET /api/sessions/{session_id}/logs/{log_type}` - логи
  - `WebSocket /ws` - real-time обновления

#### 10.3 Взаимодействие компонентов

##### 10.3.1 `handle_session_creation(file: File, context: str | None) -> dict`

**Входные параметры:**
- `file`: File - загружаемый файл
- `context`: str | None - дополнительный контекст

**Возвращает:**
- dict с полями:
  - "session_id": str - идентификатор сессии
  - "status": str - статус создания
  - "file_path": str - путь к сохраненному файлу
  - "context_path": str | None - путь к контексту

**Вызывается из:**
- `POST /api/sessions` - при создании сессии
- `uploadFile()` - при загрузке файла

**Процесс работы:**
1. Валидирует входной файл
2. Создает новую сессию
3. Сохраняет файл и контекст
4. Обновляет метаданные
5. Возвращает информацию о сессии

**Отправляет данные в:**
- `create_session_metadata()` - для создания метаданных
- `save_input_file()` - для сохранения файла
- Frontend для отображения статуса

##### 10.3.2 `start_processing_task(session_id: str) -> dict`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- dict с полями:
  - "task_id": str - идентификатор задачи
  - "status": str - статус запуска
  - "message": str - сообщение о результате

**Вызывается из:**
- `POST /api/sessions/{id}/start` - при запуске обработки
- `startProcessing()` - при инициализации задачи

**Процесс работы:**
1. Проверяет статус сессии
2. Создает фоновую задачу
3. Инициализирует обработку
4. Возвращает информацию о задаче

**Отправляет данные в:**
- `process_companies()` - для запуска обработки
- `broadcast_update()` - для уведомления UI
- WebSocket для обновления статуса

##### 10.3.3 `monitor_processing_progress(session_id: str) -> dict`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- dict с полями:
  - "progress": float - прогресс обработки
  - "status": str - текущий статус
  - "processed": int - количество обработанных компаний
  - "total": int - общее количество компаний
  - "errors": list[dict] - список ошибок

**Вызывается из:**
- WebSocket handler - при мониторинге прогресса
- `updateProgress()` - при обновлении UI

**Процесс работы:**
1. Получает текущий статус сессии
2. Собирает статистику обработки
3. Форматирует данные для UI
4. Возвращает информацию о прогрессе

**Отправляет данные в:**
- WebSocket для real-time обновлений
- Frontend для отображения прогресса
- `updateProgress()` - для обновления UI

##### 10.3.4 `handle_results_display(session_id: str) -> dict`

**Входные параметры:**
- `session_id`: str - идентификатор сессии

**Возвращает:**
- dict с полями:
  - "results": list[dict] - результаты обработки
  - "metadata": dict - метаданные сессии
  - "statistics": dict - статистика обработки
  - "download_url": str - URL для скачивания

**Вызывается из:**
- `GET /api/sessions/{id}/results` - при запросе результатов
- `updateResults()` - при обновлении таблицы

**Процесс работы:**
1. Загружает результаты сессии
2. Форматирует данные для отображения
3. Генерирует URL для скачивания
4. Возвращает подготовленные данные

**Отправляет данные в:**
- Frontend для отображения результатов
- `updateResultsTable()` - для обновления таблицы
- `updateStatistics()` - для обновления статистики

##### 10.3.5 `handle_error_notification(error: Exception, session_id: str) -> dict`

**Входные параметры:**
- `error`: Exception - возникшая ошибка
- `session_id`: str - идентификатор сессии

**Возвращает:**
- dict с полями:
  - "error_type": str - тип ошибки
  - "message": str - сообщение об ошибке
  - "timestamp": str - время возникновения
  - "recovery_action": str | None - действие для восстановления

**Вызывается из:**
- `handle_processing_error()` - при возникновении ошибки
- `process_company()` - при ошибке обработки

**Процесс работы:**
1. Определяет тип ошибки
2. Формирует сообщение
3. Логирует ошибку
4. Определяет действие для восстановления
5. Возвращает информацию об ошибке

**Отправляет данные в:**
- WebSocket для уведомления UI
- `handleError()` - для обработки ошибки
- `updateErrorDisplay()` - для отображения ошибки

##### 10.3.6 `manage_websocket_connection(session_id: str, websocket: WebSocket) -> None`

**Входные параметры:**
- `session_id`: str - идентификатор сессии
- `websocket`: WebSocket - WebSocket соединение

**Возвращает:**
- None

**Вызывается из:**
- WebSocket endpoint - при подключении
- `setupWebSocket()` - при инициализации соединения

**Процесс работы:**
1. Устанавливает соединение
2. Регистрирует обработчики событий
3. Настраивает периодические обновления
4. Обрабатывает отключение

**Отправляет данные в:**
- `broadcast_update()` - для рассылки обновлений
- `handleWebSocketMessage()` - для обработки сообщений
- Frontend для real-time обновлений

#### 10.4 Управление состоянием

1. **Frontend**:
   - Отслеживание текущей сессии
   - Управление UI состоянием
   - Кэширование результатов
   - WebSocket соединение

2. **Backend**:
   - Хранение метаданных сессий
   - Управление фоновыми задачами
   - Сохранение результатов
   - Управление WebSocket соединениями

### 11. Конфигурация и развертывание

#### 11.1 Конфигурация
- **Управление переменными окружения**
- **Конфигурация для разных сред (dev/test/prod)**
- **Настройка логирования**
- **Управление API ключами**

#### 11.2 Развертывание
- **Docker контейнеризация**
- **Настройка веб-сервера (Nginx)**
- **Мониторинг и метрики**
- **Масштабирование**

### 12. Тестирование

#### 12.1 Модульные тесты
- **Тестирование отдельных компонентов**
- **Моки внешних API**
- **Проверка граничных случаев**

#### 12.2 Интеграционные тесты
- **Тестирование взаимодействия компонентов**
- **Проверка полного пайплайна**
- **Тестирование производительности**

Recent changes:
   В выходной CSV-файл результатов для каждой компании теперь добавляется колонка "timestamp", содержащая дату (в формате ГГГГ-ММ-ДД), когда информация по данной компании была обработана и записана.

      ### Добавление метки времени обработки
   В целях отслеживания актуальности данных, для каждой обработанной компании теперь фиксируется дата ее обработки. Эта информация сохраняется в выходном CSV-файле в новой колонке `timestamp` в формате `YYYY-MM-DD`. Данное изменение было реализовано путем добавления логики получения текущей даты в функцию `process_company` (в `src/pipeline.py`) и соответствующего обновления списка полей (`expected_csv_fieldnames`) при формировании CSV-файла в функциях `run_pipeline` (`src/pipeline.py`) и `execute_pipeline_for_session_async` (`backend/main.py`). Как следствие, выходные CSV-файлы теперь содержат эту дополнительную временную метку для каждой записи.
